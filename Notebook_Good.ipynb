{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdbf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "\n",
    "from src.optimizee import *\n",
    "from src.optimizer import *\n",
    "from src.initializer import *\n",
    "\n",
    "from src.train_lstm import *\n",
    "from src.test_optimizer import *\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dce75",
   "metadata": {},
   "source": [
    "## Quadratic Optimizee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d36f3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a49b99b75f4c8eae54cfb563e2bc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/1000 [00:00<?, ?time step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.4532],\n",
       "        [0.4442],\n",
       "        [1.6159],\n",
       "        [0.6725],\n",
       "        [0.7176],\n",
       "        [0.9404],\n",
       "        [0.0115],\n",
       "        [1.2270],\n",
       "        [0.9543],\n",
       "        [0.7636]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "optimizee = QuadraticOptimizee(W, theta0)\n",
    "optimizee.set_params()\n",
    "optimizer_cls = optim.Adam\n",
    "optimizer_kwargs = {\"lr\":0.01}\n",
    "\n",
    "writer = SummaryWriter(f\"quad_test/Adam\")\n",
    "test_optimzier(optimizer_cls, optimizee,optimizer_kwargs, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6c88dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637a5d7ff6804ac5989744f166a9424a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 204141.9531, LR: 1.000e-03\n",
      "Final parameters: [[-21.200607 -20.023066 -20.559832 -20.607178 -18.638407 -21.253262\n",
      "  -20.952675 -21.072395 -20.955984 -19.61591 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 63267.5039, LR: 1.000e-03\n",
      "Final parameters: [[13.115947 13.976924 15.679885 13.805742 14.063043 15.144784 13.409777\n",
      "  12.409916 12.021747 12.871058]]...\n",
      "Epoch [30/100], Cumulative Loss: 331.5034, LR: 1.000e-03\n",
      "Final parameters: [[2.5255766 2.1779127 2.7346442 1.1947067 1.1588609 1.8090003 1.5469613\n",
      "  0.9976194 3.0362272 1.2220871]]...\n",
      "Epoch [40/100], Cumulative Loss: 33.2028, LR: 1.000e-03\n",
      "Final parameters: [[ 1.0584853   0.47112054  0.09122726  1.3366938   1.8059851  -0.07279566\n",
      "   1.1780066   0.75298226 -0.5075195   1.1647782 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 34.3671, LR: 1.000e-03\n",
      "Final parameters: [[1.073365   0.6386564  1.0903507  0.80861986 0.96155095 0.03038208\n",
      "  0.8623191  0.28999883 0.5310506  0.91675633]]...\n",
      "Epoch [60/100], Cumulative Loss: 49.5908, LR: 1.000e-03\n",
      "Final parameters: [[ 1.5453318   0.68540096  1.235002    0.66266054 -0.00923012  1.08615\n",
      "   1.0606338  -0.20768295  0.97057617  1.0324575 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 49.1139, LR: 1.000e-03\n",
      "Final parameters: [[ 1.6096925   0.04155336  0.49637765  0.6414865  -0.02943031  1.3606329\n",
      "   0.53081346  0.6214523   0.46582943  0.88361996]]...\n",
      "Epoch [80/100], Cumulative Loss: 25.1300, LR: 1.000e-03\n",
      "Final parameters: [[ 1.25557     0.35655928  1.9943143  -0.00353353  0.5574667   0.36002177\n",
      "   0.04732101  0.57606506  1.2152748   0.9438717 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 3.9625, LR: 1.000e-03\n",
      "Final parameters: [[1.1315175  0.81442165 0.2689842  1.1558502  1.1776304  0.79298675\n",
      "  1.5314027  0.5999515  0.23053305 1.2689247 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 18.0421, LR: 1.000e-03\n",
      "Final parameters: [[ 1.1638454   0.73023593 -0.91122675  1.3544812   1.7870902   0.77798975\n",
      "   2.4989662   0.41128504 -0.58366895  1.497375  ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10108955204486847\n",
      "Final parameters: [[ 1.5604236   0.22410159  1.5740684   0.5502297   0.6522894   0.79553705\n",
      "  -0.10118663  1.1148645   0.7872809   0.76234716]]...\n",
      "Training time:  80.34097695350647\n",
      "Testing time:  1.441359281539917\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"quad_train/1_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"quad_test/1_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "t0 = time.time()\n",
    "writer = SummaryWriter(\"quad_train/1_optim\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=0.9, writer=writer)\n",
    "t1 = time.time()\n",
    "writer = SummaryWriter(\"quad_test/1_optim\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Training time: \", t1-t0)\n",
    "print(\"Testing time: \", t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56d749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab6485a3169498c916dbd465fc9aea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 21098.5938, LR: 1.000e-03\n",
      "Final parameters: [[-21.203257 -20.025824 -20.56266  -20.610067 -18.64131  -21.255947\n",
      "  -20.95514  -21.07485  -20.958523 -19.61851 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 6563.7642, LR: 1.000e-03\n",
      "Final parameters: [[13.08989  13.950383 15.653728 13.779223 14.03656  15.118164 13.384108\n",
      "  12.384328 11.995342 12.845782]]...\n",
      "Epoch [30/100], Cumulative Loss: 28.3870, LR: 1.000e-03\n",
      "Final parameters: [[2.4676085 2.0858662 2.6382992 1.0905981 1.0507662 1.8148259 1.5754422\n",
      "  1.0004959 3.014398  1.1786183]]...\n",
      "Epoch [40/100], Cumulative Loss: 3.1654, LR: 1.000e-03\n",
      "Final parameters: [[ 1.0787108   0.46757174  0.07779662  1.3304827   1.7932662  -0.04698625\n",
      "   1.1926962   0.7521064  -0.49394643  1.1703732 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 3.4609, LR: 1.000e-03\n",
      "Final parameters: [[1.0640693  0.61482483 1.068398   0.7898686  0.94438845 0.00926477\n",
      "  0.8425668  0.2714095  0.51407254 0.9004774 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 4.9118, LR: 1.000e-03\n",
      "Final parameters: [[ 1.5386068   0.691622    1.2458138   0.66106814 -0.01724836  1.0919778\n",
      "   1.0617657  -0.2139909   0.9801681   1.0342913 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 4.8793, LR: 1.000e-03\n",
      "Final parameters: [[ 1.609457    0.02609999  0.46861762  0.6414333  -0.04168732  1.3631146\n",
      "   0.5294024   0.6115192   0.4485383   0.88684845]]...\n",
      "Epoch [80/100], Cumulative Loss: 2.5361, LR: 1.000e-03\n",
      "Final parameters: [[ 1.248774    0.34250367  1.9818668  -0.02066323  0.5541969   0.3427772\n",
      "   0.04287578  0.55593824  1.2078644   0.9605826 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 0.4180, LR: 1.000e-03\n",
      "Final parameters: [[1.1501266  0.8005682  0.24940403 1.1639425  1.182949   0.7759344\n",
      "  1.5334415  0.58292764 0.18173753 1.2750615 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 1.8524, LR: 1.000e-03\n",
      "Final parameters: [[ 1.1799221   0.6949263  -0.959918    1.3335705   1.7917353   0.7611104\n",
      "   2.4917934   0.40836132 -0.599562    1.4870582 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.07884270697832108\n",
      "Final parameters: [[ 1.5046476   0.2935331   1.5369682   0.604952    0.6907468   0.82217026\n",
      "  -0.01410171  1.107382    0.7924657   0.7804494 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6637419f893046498e1f2e3b81d994eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 21119.6348, LR: 1.000e-03\n",
      "Final parameters: [[-21.203255 -20.02582  -20.56266  -20.610067 -18.64131  -21.255947\n",
      "  -20.955133 -21.074848 -20.958523 -19.61851 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 6571.8018, LR: 1.000e-03\n",
      "Final parameters: [[13.091261 13.951673 15.654954 13.780412 14.037737 15.11964  13.385599\n",
      "  12.385804 11.996779 12.847163]]...\n",
      "Epoch [30/100], Cumulative Loss: 28.0922, LR: 1.000e-03\n",
      "Final parameters: [[2.4624274 2.0775957 2.6298916 1.0827422 1.0445697 1.8155314 1.5661902\n",
      "  0.9957029 3.0065703 1.1707017]]...\n",
      "Epoch [40/100], Cumulative Loss: 3.2111, LR: 1.000e-03\n",
      "Final parameters: [[ 1.0735769   0.46413773  0.07845154  1.3320149   1.7979472  -0.05035071\n",
      "   1.1903772   0.751243   -0.50089407  1.1690739 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 3.4210, LR: 1.000e-03\n",
      "Final parameters: [[1.0676081  0.620054   1.0742989  0.79549074 0.94846374 0.02000681\n",
      "  0.8501008  0.27936858 0.52117455 0.9094968 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 4.9398, LR: 1.000e-03\n",
      "Final parameters: [[ 1.5404897   0.68287104  1.2410761   0.6543534  -0.02979426  1.1034708\n",
      "   1.0686336  -0.21034707  0.985276    1.0399008 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 4.8801, LR: 1.000e-03\n",
      "Final parameters: [[ 1.6073623   0.03011127  0.47370887  0.634163   -0.04335615  1.3634472\n",
      "   0.53134936  0.61533666  0.4564982   0.894001  ]]...\n",
      "Epoch [80/100], Cumulative Loss: 2.5233, LR: 1.000e-03\n",
      "Final parameters: [[ 1.2303232   0.3591978   2.00731    -0.00406075  0.55322814  0.34200132\n",
      "   0.03851603  0.5569848   1.2213616   0.9471822 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 0.4196, LR: 1.000e-03\n",
      "Final parameters: [[1.1579685  0.80940026 0.25909314 1.1732819  1.1919458  0.78315026\n",
      "  1.5312517  0.57936513 0.18004318 1.2816561 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 1.7678, LR: 1.000e-03\n",
      "Final parameters: [[ 1.1763668   0.7198552  -0.9366045   1.3418707   1.7916067   0.7728255\n",
      "   2.5012789   0.42334658 -0.57448286  1.4972545 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.07870456576347351\n",
      "Final parameters: [[ 1.5635467   0.258425    1.5474435   0.60683894  0.6965016   0.7951354\n",
      "  -0.05007859  1.1085663   0.75220823  0.78850675]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fe5d9a4da041f8b46b1890e8f5c1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 23433.3516, LR: 1.000e-03\n",
      "Final parameters: [[-21.203226 -20.025795 -20.56263  -20.61003  -18.641277 -21.255915\n",
      "  -20.955101 -21.07482  -20.958488 -19.618483]]...\n",
      "Epoch [20/100], Cumulative Loss: 7291.1045, LR: 1.000e-03\n",
      "Final parameters: [[13.091328 13.951825 15.655134 13.780593 14.0379   15.119729 13.385673\n",
      "  12.385836 11.996829 12.847211]]...\n",
      "Epoch [30/100], Cumulative Loss: 31.3381, LR: 1.000e-03\n",
      "Final parameters: [[2.4669251 2.0809813 2.6336682 1.0876105 1.0499085 1.8058467 1.5692897\n",
      "  0.997573  3.0142682 1.1806659]]...\n",
      "Epoch [40/100], Cumulative Loss: 3.5658, LR: 1.000e-03\n",
      "Final parameters: [[ 1.0690924   0.46440542  0.08146286  1.3360641   1.8001242  -0.04949269\n",
      "   1.1966345   0.7466923  -0.50184894  1.174934  ]]...\n",
      "Epoch [50/100], Cumulative Loss: 3.6312, LR: 1.000e-03\n",
      "Final parameters: [[1.0713183  0.63446105 1.0861561  0.8036357  0.95867175 0.02339666\n",
      "  0.8628807  0.29122877 0.52550495 0.9064343 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 5.4375, LR: 1.000e-03\n",
      "Final parameters: [[ 1.5383723   0.68360895  1.2282847   0.65224636 -0.02240382  1.0868413\n",
      "   1.0613296  -0.21247414  0.97849005  1.032563  ]]...\n",
      "Epoch [70/100], Cumulative Loss: 5.2680, LR: 1.000e-03\n",
      "Final parameters: [[ 1.6080388   0.03503494  0.47885472  0.64313996 -0.03123747  1.3667455\n",
      "   0.5340383   0.62198615  0.45816484  0.8924753 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 2.7797, LR: 1.000e-03\n",
      "Final parameters: [[ 1.2348878   0.3693429   2.009096   -0.003279    0.5566828   0.34416068\n",
      "   0.04219426  0.55781186  1.2226408   0.9550691 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 0.4605, LR: 1.000e-03\n",
      "Final parameters: [[1.1479652  0.802998   0.25738618 1.1664952  1.1875983  0.78167313\n",
      "  1.5363678  0.5886086  0.18867853 1.2717656 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 2.0501, LR: 1.000e-03\n",
      "Final parameters: [[ 1.1920985   0.70564723 -0.94918907  1.3344309   1.7897542   0.7596669\n",
      "   2.4915097   0.4111125  -0.600823    1.4866318 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.09215322881937027\n",
      "Final parameters: [[ 1.5483286   0.24765489  1.5502875   0.58670264  0.6901158   0.81366616\n",
      "  -0.06045444  1.1143856   0.7566488   0.76676637]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797ffa65422d405190e7dc3e3dc12a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 204141.9531, LR: 1.000e-03\n",
      "Final parameters: [[-21.200607 -20.023066 -20.559832 -20.607178 -18.638407 -21.253262\n",
      "  -20.952675 -21.072395 -20.955984 -19.61591 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 63267.5039, LR: 1.000e-03\n",
      "Final parameters: [[13.115947 13.976924 15.679885 13.805742 14.063043 15.144784 13.409777\n",
      "  12.409916 12.021747 12.871058]]...\n",
      "Epoch [30/100], Cumulative Loss: 331.5034, LR: 1.000e-03\n",
      "Final parameters: [[2.5255766 2.1779127 2.7346442 1.1947067 1.1588609 1.8090003 1.5469613\n",
      "  0.9976194 3.0362272 1.2220871]]...\n",
      "Epoch [40/100], Cumulative Loss: 33.2028, LR: 1.000e-03\n",
      "Final parameters: [[ 1.0584853   0.47112054  0.09122726  1.3366938   1.8059851  -0.07279566\n",
      "   1.1780066   0.75298226 -0.5075195   1.1647782 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 34.3671, LR: 1.000e-03\n",
      "Final parameters: [[1.073365   0.6386564  1.0903507  0.80861986 0.96155095 0.03038208\n",
      "  0.8623191  0.28999883 0.5310506  0.91675633]]...\n",
      "Epoch [60/100], Cumulative Loss: 49.5908, LR: 1.000e-03\n",
      "Final parameters: [[ 1.5453318   0.68540096  1.235002    0.66266054 -0.00923012  1.08615\n",
      "   1.0606338  -0.20768295  0.97057617  1.0324575 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 49.1139, LR: 1.000e-03\n",
      "Final parameters: [[ 1.6096925   0.04155336  0.49637765  0.6414865  -0.02943031  1.3606329\n",
      "   0.53081346  0.6214523   0.46582943  0.88361996]]...\n",
      "Epoch [80/100], Cumulative Loss: 25.1300, LR: 1.000e-03\n",
      "Final parameters: [[ 1.25557     0.35655928  1.9943143  -0.00353353  0.5574667   0.36002177\n",
      "   0.04732101  0.57606506  1.2152748   0.9438717 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 3.9625, LR: 1.000e-03\n",
      "Final parameters: [[1.1315175  0.81442165 0.2689842  1.1558502  1.1776304  0.79298675\n",
      "  1.5314027  0.5999515  0.23053305 1.2689247 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 18.0421, LR: 1.000e-03\n",
      "Final parameters: [[ 1.1638454   0.73023593 -0.91122675  1.3544812   1.7870902   0.77798975\n",
      "   2.4989662   0.41128504 -0.58366895  1.497375  ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10108955204486847\n",
      "Final parameters: [[ 1.5604236   0.22410159  1.5740684   0.5502297   0.6522894   0.79553705\n",
      "  -0.10118663  1.1148645   0.7872809   0.76234716]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d044762b21d49ee9f9214e97133fb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 3792439.7500, LR: 1.000e-03\n",
      "Final parameters: [[-21.145998 -19.966682 -20.501846 -20.548136 -18.579493 -21.198912\n",
      "  -20.902508 -21.02206  -20.904034 -19.56244 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 1130105.5000, LR: 1.000e-03\n",
      "Final parameters: [[13.715738  14.514662  16.21979   14.349406  14.617306  15.816805\n",
      "  14.054712  13.039429  12.626572  13.4777155]]...\n",
      "Epoch [30/100], Cumulative Loss: 38108.1328, LR: 1.000e-03\n",
      "Final parameters: [[3.2691176 3.559902  4.121642  2.6199927 2.5886042 1.9399824 1.7853973\n",
      "  1.3029677 3.4895191 1.8137199]]...\n",
      "Epoch [40/100], Cumulative Loss: 25490.1133, LR: 1.000e-03\n",
      "Final parameters: [[ 0.9352377   0.51485217  0.14200492  1.3979598   1.8615947  -0.19777319\n",
      "   1.1296288   0.7710861  -0.5311663   1.1731731 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 10127.9844, LR: 1.000e-03\n",
      "Final parameters: [[1.1394066  0.6038906  1.0644623  0.8351197  0.95856875 0.12291773\n",
      "  0.87901956 0.34490484 0.51281554 0.94119203]]...\n",
      "Epoch [60/100], Cumulative Loss: 9570.6533, LR: 1.000e-03\n",
      "Final parameters: [[ 1.586359    0.67219335  1.0981228   0.7057636   0.17469192  1.0028638\n",
      "   1.02672    -0.05261368  0.8486453   0.93838525]]...\n",
      "Epoch [70/100], Cumulative Loss: 6819.4067, LR: 1.000e-03\n",
      "Final parameters: [[1.5314703  0.32740954 1.0883346  0.6652152  0.3095315  1.1523592\n",
      "  0.3891626  0.78616214 0.79095554 0.8368429 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 5902.5327, LR: 1.000e-03\n",
      "Final parameters: [[1.6520947  0.3076092  1.7354019  0.28734478 0.5390781  0.8106454\n",
      "  0.0536904  0.8143512  0.9920001  0.8502677 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 4111.1978, LR: 1.000e-03\n",
      "Final parameters: [[0.9388854  1.0439353  0.71383345 1.0894027  1.0848532  0.930177\n",
      "  1.253815   0.8381563  0.8041204  1.0891184 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 4737.2310, LR: 1.000e-03\n",
      "Final parameters: [[ 0.74964064  1.3110055  -0.02694967  1.3703772   1.3730072   0.92820156\n",
      "   2.192647    0.54157907  0.44445568  1.3809663 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.08338378369808197\n",
      "Final parameters: [[ 1.709754    0.04158174  1.8753238   0.4335748   0.54829514  0.80897653\n",
      "  -0.4896048   1.2138486   0.8020095   0.66465986]]...\n"
     ]
    }
   ],
   "source": [
    "for discount in [0, 1e-3, 0.1, 0.9, 1]:\n",
    "    \n",
    "    shutil.rmtree(f\"quad_train/disc_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"quad_test/disc_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "    initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"quad_train/disc_{discount}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=discount, writer=writer)\n",
    "    writer = SummaryWriter(f\"quad_test/disc_{discount}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6edcdfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7843584162f84b119597de141757b17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 84178197905670144.0000, LR: 1.000e-03\n",
      "Final parameters: [[-4.006369   -2.7218328  -2.849578   -2.5857615  -0.46286067 -2.2528164\n",
      "  -1.1990061  -2.118211   -2.6895108  -2.0435967 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 109254398823104512.0000, LR: 1.000e-03\n",
      "Final parameters: [[2.976119  2.0218222 4.081002  2.8993735 3.56812   4.7710614 3.6241539\n",
      "  2.8254986 2.333707  2.967144 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 89577152185368576.0000, LR: 1.000e-03\n",
      "Final parameters: [[13.183223  11.1836405 11.593998  10.919093  11.54511   11.975571\n",
      "  12.938058  10.868667  13.593101  11.783046 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 170529013607956480.0000, LR: 1.000e-03\n",
      "Final parameters: [[18.529066 18.495052 16.22962  15.942067 19.819832 18.450245 19.14051\n",
      "  16.827236 17.731441 17.343493]]...\n",
      "Epoch [50/100], Cumulative Loss: 69716927055921152.0000, LR: 1.000e-03\n",
      "Final parameters: [[13.249968  16.56107   16.45122   14.882063  15.832477  14.98335\n",
      "  16.553722  14.3855095 15.864959  14.746211 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 67514446351695872.0000, LR: 1.000e-03\n",
      "Final parameters: [[10.475569 12.306573 11.824215 12.493629 10.653654 12.881382 13.403112\n",
      "  10.530703 12.354843 11.462779]]...\n",
      "Epoch [70/100], Cumulative Loss: 54747136373293056.0000, LR: 1.000e-03\n",
      "Final parameters: [[12.171914 11.96694  11.17674  12.937345 10.885819 14.194723 14.327632\n",
      "  15.640838 13.846874 13.09241 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 50260536226480128.0000, LR: 1.000e-03\n",
      "Final parameters: [[12.275572  12.83068   14.428134  11.412311  12.663951  13.3513155\n",
      "  14.865303  13.4374275 13.180902  13.357203 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 67926977960476672.0000, LR: 1.000e-03\n",
      "Final parameters: [[13.36075  14.253509 12.3275   12.623239 12.749614 15.602705 19.021765\n",
      "  16.181479 15.395733 15.007296]]...\n",
      "Epoch [100/100], Cumulative Loss: 69106148346691584.0000, LR: 1.000e-03\n",
      "Final parameters: [[13.858176 15.511603 11.489348 12.153998 13.693388 16.663082 20.351694\n",
      "  18.239836 15.120315 15.374644]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 4349912.5\n",
      "Final parameters: [[280.19336 265.4473  254.10262 247.45967 242.1546  333.5222  409.73807\n",
      "  369.59833 328.29037 300.78372]]...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"quad_train/disc_2\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=50, discount=2, writer=writer)\n",
    "writer = SummaryWriter(f\"quad_test/disc_2\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b34bba52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e096d84cdd0e4b8ea54d7598e611d33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 204141.9531, LR: 1.000e-03\n",
      "Final parameters: [[-21.200607 -20.023066 -20.559832 -20.607178 -18.638407 -21.253262\n",
      "  -20.952675 -21.072395 -20.955984 -19.61591 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 63267.5039, LR: 1.000e-03\n",
      "Final parameters: [[13.115947 13.976924 15.679885 13.805742 14.063043 15.144784 13.409777\n",
      "  12.409916 12.021747 12.871058]]...\n",
      "Epoch [30/100], Cumulative Loss: 331.5034, LR: 1.000e-03\n",
      "Final parameters: [[2.5255766 2.1779127 2.7346442 1.1947067 1.1588609 1.8090003 1.5469613\n",
      "  0.9976194 3.0362272 1.2220871]]...\n",
      "Epoch [40/100], Cumulative Loss: 33.2028, LR: 1.000e-03\n",
      "Final parameters: [[ 1.0584853   0.47112054  0.09122726  1.3366938   1.8059851  -0.07279566\n",
      "   1.1780066   0.75298226 -0.5075195   1.1647782 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 34.3671, LR: 1.000e-03\n",
      "Final parameters: [[1.073365   0.6386564  1.0903507  0.80861986 0.96155095 0.03038208\n",
      "  0.8623191  0.28999883 0.5310506  0.91675633]]...\n",
      "Epoch [60/100], Cumulative Loss: 49.5908, LR: 1.000e-03\n",
      "Final parameters: [[ 1.5453318   0.68540096  1.235002    0.66266054 -0.00923012  1.08615\n",
      "   1.0606338  -0.20768295  0.97057617  1.0324575 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 49.1139, LR: 1.000e-03\n",
      "Final parameters: [[ 1.6096925   0.04155336  0.49637765  0.6414865  -0.02943031  1.3606329\n",
      "   0.53081346  0.6214523   0.46582943  0.88361996]]...\n",
      "Epoch [80/100], Cumulative Loss: 25.1300, LR: 1.000e-03\n",
      "Final parameters: [[ 1.25557     0.35655928  1.9943143  -0.00353353  0.5574667   0.36002177\n",
      "   0.04732101  0.57606506  1.2152748   0.9438717 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 3.9625, LR: 1.000e-03\n",
      "Final parameters: [[1.1315175  0.81442165 0.2689842  1.1558502  1.1776304  0.79298675\n",
      "  1.5314027  0.5999515  0.23053305 1.2689247 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 18.0421, LR: 1.000e-03\n",
      "Final parameters: [[ 1.1638454   0.73023593 -0.91122675  1.3544812   1.7870902   0.77798975\n",
      "   2.4989662   0.41128504 -0.58366895  1.497375  ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10108955204486847\n",
      "Final parameters: [[ 1.5604236   0.22410159  1.5740684   0.5502297   0.6522894   0.79553705\n",
      "  -0.10118663  1.1148645   0.7872809   0.76234716]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8851a9f9614cfbaa27f4aa9f0684d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 690565.6875, LR: 1.000e-03\n",
      "Final parameters: [[-35.510696 -36.95648  -35.78062  -35.71172  -36.209694 -35.455154\n",
      "  -40.19661  -40.513863 -40.108624 -37.70981 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 2.3801, LR: 1.000e-03\n",
      "Final parameters: [[1.2955385  0.6017042  1.007606   0.8358114  0.88453066 0.7350635\n",
      "  0.64454585 0.8778035  0.6384119  1.0100452 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 20.8423, LR: 1.000e-03\n",
      "Final parameters: [[0.19667731 2.3528948  0.97802436 1.2658257  1.1278677  1.9070171\n",
      "  1.8521829  1.3594611  2.306548   0.9645561 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 9242.3330, LR: 1.000e-03\n",
      "Final parameters: [[ 1.2643993  4.4577723  3.3559437  5.8808756  4.3642607  9.608824\n",
      "  12.475751   8.718078   8.928857   4.0330205]]...\n",
      "Epoch [50/100], Cumulative Loss: 69.9667, LR: 1.000e-03\n",
      "Final parameters: [[-1.552021    3.5208123  -0.78611493  2.2184274   2.6969712   1.5048018\n",
      "   3.694567    2.0556705   2.4231415   1.3329409 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 1.0149, LR: 1.000e-03\n",
      "Final parameters: [[0.8655632  1.1811857  1.0659755  1.0776845  1.0318943  1.1195195\n",
      "  1.0869694  1.0314037  1.153689   0.96079606]]...\n",
      "Epoch [70/100], Cumulative Loss: 1.0914, LR: 1.000e-03\n",
      "Final parameters: [[0.6344771  1.4861541  0.58337784 1.210437   1.1396322  1.0522604\n",
      "  1.740635   0.9081971  1.1302544  1.1582912 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 0.8617, LR: 1.000e-03\n",
      "Final parameters: [[0.47479647 1.5890856  0.47949368 1.3390492  1.2416433  1.1044722\n",
      "  1.9101774  0.7755933  1.1012728  1.1759334 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 1.5396, LR: 1.000e-03\n",
      "Final parameters: [[1.1120194  0.85407394 1.5892631  0.7744695  0.7586901  1.0615183\n",
      "  0.35095057 1.2423891  1.4217484  0.77888405]]...\n",
      "Epoch [100/100], Cumulative Loss: 1.2248, LR: 1.000e-03\n",
      "Final parameters: [[0.3158555  1.9529852  0.04725254 1.5239347  1.4984648  1.0916843\n",
      "  2.5730844  0.6396263  1.0328095  1.4306757 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10758653283119202\n",
      "Final parameters: [[0.58867854 1.5872427  0.5391754  1.3020782  1.2198837  1.0257199\n",
      "  1.8326439  0.8373494  1.140361   1.2273006 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80db66f5f2b84c1097ac917929a10872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1.4837, LR: 1.000e-03\n",
      "Final parameters: [[0.251435   1.9633739  0.15200551 1.5481122  1.4695761  1.2357233\n",
      "  2.46543    0.8362832  1.2663385  1.347946  ]]...\n",
      "Epoch [20/100], Cumulative Loss: 0.3386, LR: 1.000e-03\n",
      "Final parameters: [[0.8527968  1.1933498  0.86091363 1.0874513  1.0565115  1.0052192\n",
      "  1.2423048  0.9405756  1.0574645  1.0888116 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 0.3626, LR: 1.000e-03\n",
      "Final parameters: [[1.2054505 0.7888997 1.2132678 0.8795615 0.9228395 0.9903994 0.692695\n",
      "  1.0993975 0.9733969 0.9169263]]...\n",
      "Epoch [40/100], Cumulative Loss: 0.4954, LR: 1.000e-03\n",
      "Final parameters: [[1.2561276  0.71441823 1.3986927  0.8072749  0.80682164 0.9856966\n",
      "  0.42166957 1.0878129  1.0372778  0.823684  ]]...\n",
      "Epoch [50/100], Cumulative Loss: 0.5735, LR: 1.000e-03\n",
      "Final parameters: [[0.7247144  1.405236   0.8142145  1.1881926  1.1095321  1.1052197\n",
      "  1.5001669  0.95219487 1.1925559  1.1129807 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 0.6863, LR: 1.000e-03\n",
      "Final parameters: [[0.48661453 1.6344755  0.42410183 1.3782369  1.3113611  1.1739845\n",
      "  2.065224   0.76970077 1.1451701  1.2418063 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 0.5843, LR: 1.000e-03\n",
      "Final parameters: [[0.6210259  1.3957582  0.65050274 1.2430322  1.179987   1.1110175\n",
      "  1.6547532  0.88187045 1.1214842  1.1327031 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 0.3786, LR: 1.000e-03\n",
      "Final parameters: [[0.8273841 1.1963937 0.8446739 1.1453809 1.1210997 1.0813074 1.300342\n",
      "  0.9477759 1.0575609 1.0695637]]...\n",
      "Epoch [90/100], Cumulative Loss: 0.4846, LR: 1.000e-03\n",
      "Final parameters: [[1.0809424  0.8234476  1.2890972  0.8312415  0.8338705  0.94149065\n",
      "  0.52484477 1.0892166  1.1014684  0.8702363 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 2.3862, LR: 1.000e-03\n",
      "Final parameters: [[ 0.50277025  1.76873    -0.34885025  1.6379515   1.5517224   0.99057937\n",
      "   2.8277147   0.5056786   0.48081577  1.4849664 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.061159648001194\n",
      "Final parameters: [[0.5352776  1.5412006  0.38822728 1.2924592  1.2434391  1.0417583\n",
      "  2.0185273  0.76825047 1.0842457  1.2425904 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8c3e1f44ab4b4385ecb6a396893950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1325232.7500, LR: 1.000e-03\n",
      "Final parameters: [[52.971962 53.60106  54.6115   53.66979  52.700935 57.221287 60.067997\n",
      "  58.97629  56.57265  55.00469 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 81.8304, LR: 1.000e-03\n",
      "Final parameters: [[ 0.00746383  2.1467996  -0.8080026   1.9483395   2.9706497   0.8210478\n",
      "   2.971868    1.9350698   0.38112378  1.4948225 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 1239.9465, LR: 1.000e-03\n",
      "Final parameters: [[ 1.7981057  -2.3523204  -4.147284    0.15388629  1.9801304   1.5460348\n",
      "   2.0705955   1.8137037  -0.28214744 -1.5228446 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 217.3334, LR: 1.000e-03\n",
      "Final parameters: [[ 3.2777438  -1.2662773   2.1980739  -0.8436632  -0.74562824  0.22049603\n",
      "  -0.42626816 -0.34086412  0.6216042  -0.2279318 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 26.8955, LR: 1.000e-03\n",
      "Final parameters: [[ 2.865678   -0.8514421   1.492713    0.4120065   0.847373    0.44545585\n",
      "  -1.0577203   1.5136322  -0.4700408   0.4805364 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 2.4650, LR: 1.000e-03\n",
      "Final parameters: [[ 2.396269   -0.7883702   2.8455129  -0.03487445  0.09715876  0.68098414\n",
      "  -2.0681672   1.5502565   0.75704414  0.29727066]]...\n",
      "Epoch [70/100], Cumulative Loss: 1.3771, LR: 1.000e-03\n",
      "Final parameters: [[ 1.864321   -0.13620985  2.2251987   0.35181817  0.40700325  0.8146982\n",
      "  -1.0408349   1.3500378   0.88028145  0.5065325 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 1.5720, LR: 1.000e-03\n",
      "Final parameters: [[ 1.8161784  -0.02493631  1.9703135   0.3905161   0.50696963  0.75092864\n",
      "  -0.7155601   1.3473133   0.77156764  0.64070255]]...\n",
      "Epoch [90/100], Cumulative Loss: 1.5161, LR: 1.000e-03\n",
      "Final parameters: [[ 1.8369731  -0.07362931  1.9766939   0.3742084   0.52481776  0.76619446\n",
      "  -0.7576801   1.3897605   0.81393474  0.56433904]]...\n",
      "Epoch [100/100], Cumulative Loss: 0.8894, LR: 1.000e-03\n",
      "Final parameters: [[1.2639531  0.6198094  1.2389361  0.7670789  0.80483615 0.8616579\n",
      "  0.45648086 1.0810448  0.8885177  0.8972477 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.045356057584285736\n",
      "Final parameters: [[1.3568698  0.508487   1.3394657  0.77187043 0.87618774 0.8799243\n",
      "  0.31547213 1.142569   0.8305481  0.8727705 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4effe0648fae44e49c6e46aa0807d898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 4.4270, LR: 1.000e-03\n",
      "Final parameters: [[1.664736   0.04881512 1.2610643  0.60886437 0.6660063  0.656706\n",
      "  0.03427867 0.8592129  0.3234664  0.9128431 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 0.6183, LR: 1.000e-03\n",
      "Final parameters: [[0.3734385  1.7936358  0.12160977 1.4420393  1.3960598  1.0923876\n",
      "  2.4203148  0.69219977 1.0685942  1.3485073 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 5.7905, LR: 1.000e-03\n",
      "Final parameters: [[0.29571882 1.6821228  0.7774876  1.0351187  1.1214334  1.1179144\n",
      "  1.8328946  0.8475578  1.7002941  1.168805  ]]...\n",
      "Epoch [40/100], Cumulative Loss: 0.4385, LR: 1.000e-03\n",
      "Final parameters: [[0.9630605  1.0852915  0.83206064 1.0762353  1.0933669  0.9643168\n",
      "  1.2103008  0.9082111  0.8922623  1.0546865 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 0.5069, LR: 1.000e-03\n",
      "Final parameters: [[1.125268   0.7353674  1.1068683  0.86182815 0.91620576 0.92519474\n",
      "  0.70132446 0.99571943 0.86629224 0.9259975 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 2.7227, LR: 1.000e-03\n",
      "Final parameters: [[0.36488223 1.7590035  0.42520007 1.3694351  1.3293424  1.1473345\n",
      "  1.920256   0.9257242  1.250695   1.168745  ]]...\n",
      "Epoch [70/100], Cumulative Loss: 0.6084, LR: 1.000e-03\n",
      "Final parameters: [[1.1726289  0.64105695 1.1314167  0.7704285  0.86264527 0.90800875\n",
      "  0.5844794  1.0590638  0.9008153  0.91241115]]...\n",
      "Epoch [80/100], Cumulative Loss: 0.5943, LR: 1.000e-03\n",
      "Final parameters: [[1.0682225  0.81647766 0.92542773 0.951841   1.0378934  0.97888047\n",
      "  1.0055091  0.99344474 0.82229626 0.9676232 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 0.5251, LR: 1.000e-03\n",
      "Final parameters: [[0.59122044 1.5064743  0.31769675 1.334188   1.3542186  1.077282\n",
      "  2.036367   0.73960024 0.93568444 1.2627121 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 0.6073, LR: 1.000e-03\n",
      "Final parameters: [[0.45603433 1.5944656  0.20815778 1.3430502  1.3492293  1.0180268\n",
      "  2.1456327  0.6809403  0.9688242  1.2678725 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.0844523012638092\n",
      "Final parameters: [[ 1.6625615   0.19772355  1.7967901   0.56527406  0.67554015  0.9008185\n",
      "  -0.299538    1.258317    0.83694685  0.6853569 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f760110e53b2450889172395b8a89ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 378870.7188, LR: 1.000e-03\n",
      "Final parameters: [[-25.947302 -24.837069 -22.63337  -24.601492 -23.794838 -28.94256\n",
      "  -35.61303  -32.811512 -32.397644 -26.574942]]...\n",
      "Epoch [20/100], Cumulative Loss: 1.0905, LR: 1.000e-03\n",
      "Final parameters: [[1.1122539  0.8006468  1.0542152  0.93534505 0.93197423 0.91759497\n",
      "  0.8454598  0.90281284 0.84454167 0.98721147]]...\n",
      "Epoch [30/100], Cumulative Loss: 4.1246, LR: 1.000e-03\n",
      "Final parameters: [[-0.02045661  2.318726   -0.2915365   1.7728301   1.7158645   1.2407796\n",
      "   3.0014994   0.85074323  1.2121484   1.5028743 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 10104.7080, LR: 1.000e-03\n",
      "Final parameters: [[-2.9941118 12.642021  14.196784  13.005314   7.751782   5.44036\n",
      "   3.1671546  6.9581695  8.209994   2.77736  ]]...\n",
      "Epoch [50/100], Cumulative Loss: 5.0685, LR: 1.000e-03\n",
      "Final parameters: [[-0.47228616  2.8497796  -0.88541764  2.1555374   2.0894775   1.4098623\n",
      "   3.9557772   0.6187732   1.1853906   1.667405  ]]...\n",
      "Epoch [60/100], Cumulative Loss: 7.1591, LR: 1.000e-03\n",
      "Final parameters: [[0.29273936 1.9437561  0.38510168 1.3429177  1.36907    1.3359915\n",
      "  1.9023731  1.1287043  1.4325348  1.188705  ]]...\n",
      "Epoch [70/100], Cumulative Loss: 4.5195, LR: 1.000e-03\n",
      "Final parameters: [[-0.16415565  2.6351974  -0.7712704   1.9703639   1.96258     1.2781637\n",
      "   3.8862572   0.62948674  1.1090447   1.6981107 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 2.2559, LR: 1.000e-03\n",
      "Final parameters: [[1.0255504  1.0025411  1.4691849  0.83311784 0.8348787  1.0854566\n",
      "  0.4934526  1.2179011  1.3536203  0.89803773]]...\n",
      "Epoch [90/100], Cumulative Loss: 2.9353, LR: 1.000e-03\n",
      "Final parameters: [[0.56765324 1.7433338  0.65897155 1.3850663  1.2330186  1.1719369\n",
      "  1.8184571  0.9069181  1.2917129  1.2463456 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 515.1787, LR: 1.000e-03\n",
      "Final parameters: [[-1.5189092   5.222787    4.428757    1.7242739  -0.21375474  1.7467121\n",
      "   1.7930496   0.08590762  4.321274    2.3956397 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.8193626999855042\n",
      "Final parameters: [[-1.0800749   3.8154035  -1.8072549   2.673122    2.4942243   1.5933516\n",
      "   5.856921    0.14719257  1.3437154   2.112019  ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b845d18b7f234cfaa52ea9365016d2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.6643, LR: 1.000e-03\n",
      "Final parameters: [[-0.10907411  2.4518516  -0.2896857   1.8426203   1.708436    1.377574\n",
      "   3.2464387   0.7716345   1.4116678   1.4678583 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 0.3235, LR: 1.000e-03\n",
      "Final parameters: [[0.8495149  1.2320554  0.9253528  1.0920693  1.0735232  1.0649972\n",
      "  1.2643389  0.99800414 1.1475273  1.0532882 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 0.4294, LR: 1.000e-03\n",
      "Final parameters: [[0.6493181  1.4571834  0.594379   1.283869   1.2168149  1.0489781\n",
      "  1.718395   0.85043514 1.1047229  1.1852971 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 0.2594, LR: 1.000e-03\n",
      "Final parameters: [[1.1483604  0.82998514 1.1639776  0.91398025 0.87688935 0.952021\n",
      "  0.70947737 1.054645   0.96698207 0.9495594 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 0.2779, LR: 1.000e-03\n",
      "Final parameters: [[0.8078366  1.1619687  0.83075535 1.092417   1.068833   1.0186788\n",
      "  1.2687432  0.9395051  1.0769093  1.0595633 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 0.4153, LR: 1.000e-03\n",
      "Final parameters: [[1.1877096  0.7474178  1.1415503  0.88029987 0.9383513  0.91968274\n",
      "  0.64978397 1.0870548  0.8980688  0.9144219 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 0.3879, LR: 1.000e-03\n",
      "Final parameters: [[0.8865903  1.1022842  0.93150645 1.07566    1.0341649  1.029127\n",
      "  1.1801388  0.96897125 1.0706712  1.0695823 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 0.4079, LR: 1.000e-03\n",
      "Final parameters: [[1.1673374  0.76706564 1.2836093  0.8727507  0.8788084  0.9351067\n",
      "  0.4872662  1.0936276  0.9558083  0.881306  ]]...\n",
      "Epoch [90/100], Cumulative Loss: 0.7046, LR: 1.000e-03\n",
      "Final parameters: [[ 1.6024773   0.2886844   1.7407163   0.5804825   0.6340717   0.8560468\n",
      "  -0.26874492  1.2446439   0.8910958   0.72884667]]...\n",
      "Epoch [100/100], Cumulative Loss: 0.2761, LR: 1.000e-03\n",
      "Final parameters: [[0.948818   1.120193   0.87470514 1.0666709  1.0882807  1.0351572\n",
      "  1.2274859  0.9714694  1.0083721  1.0867602 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10193293541669846\n",
      "Final parameters: [[ 1.7367773  -0.00882421  2.1242633   0.37333518  0.4130141   0.8637345\n",
      "  -0.8065584   1.424763    0.9799078   0.5698028 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0df2af4e0c34d8ba5590b549a8dd4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.6643, LR: 1.000e-03\n",
      "Final parameters: [[-0.10907411  2.4518516  -0.2896857   1.8426203   1.708436    1.377574\n",
      "   3.2464387   0.7716345   1.4116678   1.4678583 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 0.3235, LR: 1.000e-03\n",
      "Final parameters: [[0.8495149  1.2320554  0.9253528  1.0920693  1.0735232  1.0649972\n",
      "  1.2643389  0.99800414 1.1475273  1.0532882 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 0.4294, LR: 1.000e-03\n",
      "Final parameters: [[0.6493181  1.4571834  0.594379   1.283869   1.2168149  1.0489781\n",
      "  1.718395   0.85043514 1.1047229  1.1852971 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 0.2594, LR: 1.000e-03\n",
      "Final parameters: [[1.1483604  0.82998514 1.1639776  0.91398025 0.87688935 0.952021\n",
      "  0.70947737 1.054645   0.96698207 0.9495594 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 0.2779, LR: 1.000e-03\n",
      "Final parameters: [[0.8078366  1.1619687  0.83075535 1.092417   1.068833   1.0186788\n",
      "  1.2687432  0.9395051  1.0769093  1.0595633 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 0.4153, LR: 1.000e-03\n",
      "Final parameters: [[1.1877096  0.7474178  1.1415503  0.88029987 0.9383513  0.91968274\n",
      "  0.64978397 1.0870548  0.8980688  0.9144219 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 0.3879, LR: 1.000e-03\n",
      "Final parameters: [[0.8865903  1.1022842  0.93150645 1.07566    1.0341649  1.029127\n",
      "  1.1801388  0.96897125 1.0706712  1.0695823 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 0.4079, LR: 1.000e-03\n",
      "Final parameters: [[1.1673374  0.76706564 1.2836093  0.8727507  0.8788084  0.9351067\n",
      "  0.4872662  1.0936276  0.9558083  0.881306  ]]...\n",
      "Epoch [90/100], Cumulative Loss: 0.7046, LR: 1.000e-03\n",
      "Final parameters: [[ 1.6024773   0.2886844   1.7407163   0.5804825   0.6340717   0.8560468\n",
      "  -0.26874492  1.2446439   0.8910958   0.72884667]]...\n",
      "Epoch [100/100], Cumulative Loss: 0.2761, LR: 1.000e-03\n",
      "Final parameters: [[0.948818   1.120193   0.87470514 1.0666709  1.0882807  1.0351572\n",
      "  1.2274859  0.9714694  1.0083721  1.0867602 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10193293541669846\n",
      "Final parameters: [[ 1.7367773  -0.00882421  2.1242633   0.37333518  0.4130141   0.8637345\n",
      "  -0.8065584   1.424763    0.9799078   0.5698028 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726caa733a274c6b99bb6289f828480f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 55145.5586, LR: 1.000e-03\n",
      "Final parameters: [[ 7.156487  8.567442  9.605099 10.300525  9.896771 18.116533 20.257769\n",
      "  19.59545  15.353335  9.653481]]...\n",
      "Epoch [20/100], Cumulative Loss: 1.8792, LR: 1.000e-03\n",
      "Final parameters: [[0.25882918 1.9126992  0.27160236 1.5393112  1.4378369  1.1747446\n",
      "  2.3738606  0.74386406 1.1978915  1.3253478 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 56.9132, LR: 1.000e-03\n",
      "Final parameters: [[ 3.8837216  -2.8686438   2.1249998  -0.37240723  0.11634634 -0.02873537\n",
      "  -2.8324099   0.8790602  -1.5712477   0.4921749 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 1054.1113, LR: 1.000e-03\n",
      "Final parameters: [[ 3.028984   -3.0394974  -2.8560052  -0.5447544   1.4817808   1.3222718\n",
      "   0.65737134  1.6676389  -0.7723293  -1.4507815 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 3.2930, LR: 1.000e-03\n",
      "Final parameters: [[ 2.1498463  -0.41490316  2.4454143   0.18995366  0.31746927  0.77361757\n",
      "  -1.2436024   1.3669012   0.80500656  0.50051355]]...\n",
      "Epoch [60/100], Cumulative Loss: 1.6808, LR: 1.000e-03\n",
      "Final parameters: [[1.3105268  0.52451694 1.2086209  0.797401   0.93629026 0.8792435\n",
      "  0.41583908 1.0371366  0.7355862  0.84779257]]...\n",
      "Epoch [70/100], Cumulative Loss: 1.3686, LR: 1.000e-03\n",
      "Final parameters: [[0.7815063  1.1368551  0.62800103 1.0964904  1.0934638  0.955441\n",
      "  1.5206032  0.8485661  0.9268632  1.1337277 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 1.1050, LR: 1.000e-03\n",
      "Final parameters: [[0.84214413 1.0953956  0.67738855 1.0837731  1.0687053  0.95120674\n",
      "  1.4041623  0.90933985 0.91730195 1.0616767 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 1.1502, LR: 1.000e-03\n",
      "Final parameters: [[1.2012343  0.724964   1.302697   0.8462732  0.8586494  0.9010064\n",
      "  0.53287303 1.0675356  0.8947394  0.90135384]]...\n",
      "Epoch [100/100], Cumulative Loss: 1.9448, LR: 1.000e-03\n",
      "Final parameters: [[1.5491004  0.4224199  1.5280432  0.68632454 0.7558427  0.84208477\n",
      "  0.07511887 1.1649848  0.81981003 0.80012995]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.13164947926998138\n",
      "Final parameters: [[ 1.7488832   0.13898553  1.9373399   0.52202946  0.55401146  0.8368122\n",
      "  -0.5697088   1.262574    0.8153596   0.62890416]]...\n",
      "Times_Train [86.86043238639832, 106.05999040603638, 123.19882440567017, 142.9445357322693, 160.29705834388733, 177.49205541610718, 214.33200645446777, 216.13004279136658, 240.35727310180664]\n",
      "Times_Test [0.9014897346496582, 1.2495019435882568, 1.5960447788238525, 1.9974737167358398, 2.2961010932922363, 3.153033971786499, 3.511620044708252, 3.361135721206665, 3.685124635696411]\n"
     ]
    }
   ],
   "source": [
    "times_train, times_test = [], []\n",
    "for count in range(1, 10):\n",
    "    # shutil.rmtree(f\"quad_train/{count}_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    # shutil.rmtree(f\"quad_test/{count}_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": list(np.arange(0.01, (count+1)*0.01, 0.01))}\n",
    "\n",
    "    initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    t0 = time.time()\n",
    "    # writer = SummaryWriter(f\"quad_train/{count}_optim\") \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=0.9)\n",
    "    t1 = time.time()\n",
    "    # writer = SummaryWriter(f\"quad_test/{count}_optim\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000)\n",
    "    t2 = time.time()\n",
    "\n",
    "    times_train.append(t1-t0)\n",
    "    times_test.append(t2-t1)\n",
    "\n",
    "print(\"Times_Train\", times_train)\n",
    "print(\"Times_Test\", times_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626dad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d977a5ed0680499fb1487fa5269595af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 22.0672, LR: 3.487e-03\n",
      "Final parameters: [[0.7458178  0.48710084 0.7246131  1.1129686  1.2794619  0.03819936\n",
      "  0.7985495  0.69870615 0.12967493 1.140651  ]]...\n",
      "Epoch [20/100], Cumulative Loss: 2202.9973, LR: 1.216e-03\n",
      "Final parameters: [[ 0.87161595 -2.5762007  -1.0964168  -1.740776   -0.8742697  -3.1788495\n",
      "  -4.399133   -2.7612875  -2.9782355  -0.340473  ]]...\n",
      "Epoch [30/100], Cumulative Loss: 230.4241, LR: 4.239e-04\n",
      "Final parameters: [[ 2.8051443  -0.9810329   0.5666674  -0.59469014 -0.09969285  0.35050195\n",
      "   0.62905145 -0.07248963  0.1487947  -0.04518276]]...\n",
      "Epoch [40/100], Cumulative Loss: 1222.1696, LR: 1.478e-04\n",
      "Final parameters: [[ 2.831877   -3.6281383  -3.741819   -2.1823993   0.93304986 -1.2606273\n",
      "   0.45565212  0.36349714 -2.3948965  -0.8059255 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 94.7108, LR: 5.154e-05\n",
      "Final parameters: [[ 1.0056808   0.29039174  0.8656239   0.585132    1.0019178  -0.60585976\n",
      "   0.48516583  0.0814171   0.11609113  0.70684564]]...\n",
      "Epoch [60/100], Cumulative Loss: 47.4990, LR: 1.797e-05\n",
      "Final parameters: [[ 1.5969338   0.6225572   1.171201    0.69032097  0.08125164  0.97432226\n",
      "   1.0539823  -0.25147638  0.7532842   1.0770541 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 62.5633, LR: 6.266e-06\n",
      "Final parameters: [[ 1.7417235  -0.31579262  0.1285098   0.73596525  0.03192016  1.221336\n",
      "   0.44687214  0.5972471  -0.04696742  0.95158255]]...\n",
      "Epoch [80/100], Cumulative Loss: 46.9751, LR: 2.185e-06\n",
      "Final parameters: [[ 1.1807746   0.19505437  1.8658985  -0.27771926  0.5969834   0.11483549\n",
      "  -0.00781688  0.43615025  1.1015688   1.0378885 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 24.5018, LR: 7.618e-07\n",
      "Final parameters: [[ 1.6855156   0.17913589 -0.43603107  1.0813596   1.6081052   0.48930717\n",
      "   1.4912779   0.62259316 -0.6859136   1.1805383 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 148.1085, LR: 2.656e-07\n",
      "Final parameters: [[ 2.2771316  -0.28168896 -2.2981527  -0.00258277  2.2241263   0.74057364\n",
      "   2.060474    0.30366558 -1.8267165   1.1415157 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.208175390958786\n",
      "Final parameters: [[1.4757491  0.35278034 1.1109564  0.789371   0.84569764 0.77519244\n",
      "  0.41923726 0.8958048  0.46655616 0.97443557]]...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(meta_optimizer, gamma=0.9)\n",
    "\n",
    "writer = SummaryWriter(f\"quad_train_lr/exp\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=0.9, writer=writer, scheduler=scheduler)\n",
    "writer = SummaryWriter(f\"quad_test_lr/exp\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb60fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e8e01e05594b46bf20c6aba5fc032d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1566403.2500, LR: 1.000e-03\n",
      "Final parameters: [[57.911163 64.48485  65.12734  64.53079  66.00644  77.2321   69.50971\n",
      "  63.033337 59.33736  58.664852]]...\n",
      "Epoch [20/100], Cumulative Loss: 211578.9844, LR: 1.000e-03\n",
      "Final parameters: [[19.878113 27.124207 23.42374  25.65271  25.073217 32.0088   28.965328\n",
      "  25.711296 23.307188 21.79594 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 720.3181, LR: 1.000e-03\n",
      "Final parameters: [[-3.5164587  8.447317   1.8405029  4.256819   2.5125027  4.838291\n",
      "   7.192434   3.2630103  7.986643   1.4101461]]...\n",
      "Epoch [40/100], Cumulative Loss: 188.4844, LR: 1.000e-03\n",
      "Final parameters: [[-1.6718128  5.3076997  1.327415   2.630596   1.7788293  3.2326741\n",
      "   4.4692245  2.07053    4.763102   1.3009313]]...\n",
      "Epoch [50/100], Cumulative Loss: 109.9922, LR: 1.000e-03\n",
      "Final parameters: [[-2.0991793   5.4988213   0.11261505  2.8293707   2.2051938   2.698927\n",
      "   5.504279    1.5587337   4.321713    1.4922891 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 75.9469, LR: 1.000e-03\n",
      "Final parameters: [[-0.67359364  3.6499918   1.8133471   1.6142433   1.2746853   2.257631\n",
      "   2.440771    1.985567    4.0411615   0.74911547]]...\n",
      "Epoch [70/100], Cumulative Loss: 48.3645, LR: 1.000e-03\n",
      "Final parameters: [[-0.55594516  3.4131448   1.267285    1.6888924   1.3964974   2.0488818\n",
      "   2.7205744   1.6620227   3.39564     0.9403374 ]]...\n",
      "Epoch [80/100], Cumulative Loss: 23.5102, LR: 1.000e-03\n",
      "Final parameters: [[0.14599302 2.2784834  1.5046494  1.394053   1.1389843  1.6988301\n",
      "  1.74844    1.5323983  2.4339306  0.9656564 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 16.9835, LR: 1.000e-03\n",
      "Final parameters: [[0.05940776 2.3323941  1.14543    1.4917706  1.2626164  1.6016574\n",
      "  2.0843885  1.3719298  2.253059   1.0614411 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 12.7954, LR: 1.000e-03\n",
      "Final parameters: [[-0.33200705  2.882688    0.26950854  1.8624101   1.612313    1.6225497\n",
      "   3.1705134   1.0544723   2.1361597   1.3147912 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.6293569803237915\n",
      "Final parameters: [[-0.05600916  2.5917268   0.92949235  1.4846572   1.3245621   1.6349208\n",
      "   2.327149    1.3156433   2.4155128   1.0346022 ]]...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"quad_train/lstm_no_preproc\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=0.9, writer=writer)\n",
    "writer = SummaryWriter(f\"quad_test/lstm_no_preproc\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49795c8e",
   "metadata": {},
   "source": [
    "## NN Optimizee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec63638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "y = (y-np.mean(y))/np.std(y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef05c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 10]) torch.Size([50])\n",
      "torch.Size([50])\n",
      "tensor(6.1015, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.1015, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = XYNNOptimizee(X, y, num_samples=50)\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "indices = torch.randint(0, X.shape[0], (50,))\n",
    "X_loss = torch.tensor(X[indices], dtype=torch.float32)\n",
    "y_loss = torch.tensor(y[indices], dtype=torch.float32).squeeze()\n",
    "print(X_loss.shape, y_loss.shape)\n",
    "\n",
    "out = opt.forward(X_loss).squeeze()\n",
    "print(out.shape)\n",
    "\n",
    "print(nn.MSELoss()(out, y_loss))\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "opt.compute_loss(opt.all_parameters(), return_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477fbfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6df21f7d5ed4ff68a370a1cd291e2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 188.5305, LR: 1.000e-03\n",
      "Final parameters: [[-5.00232   -4.271275  -3.8146198 -5.3463044 -4.31671   -5.051714\n",
      "  -4.3848834 -5.4401565 -6.6796904 -6.7991896 -4.688251  -4.0982018\n",
      "  -4.540951  -4.3195634 -6.4075446 -3.155995  -5.8427973 -4.937065\n",
      "  -4.9463043 -4.72288   -5.063637  -4.9825754 -2.0175254 -5.7878437\n",
      "  -8.036008  -3.1914062 -3.8180373 -6.362682  -6.5460677 -6.552737\n",
      "  -3.1508644 -6.081562  -5.2259865 -4.301706  -4.3299723 -5.457913\n",
      "  -5.337422  -3.3684278 -5.65675   -5.0899405 -5.1289296 -5.2214065\n",
      "  -6.647633  -4.0242653 -4.4766145 -4.6060386 -5.3128114 -5.258121\n",
      "  -4.4312263 -4.122672  -4.2232976 -4.2077928 -6.2140527 -5.3323884\n",
      "  -5.7695274 -5.633792  -6.4506516 -6.982246  -4.9281034 -5.116632\n",
      "  -3.4738486 -6.2441077 -4.5609994 -3.2823188 -4.0803595 -5.0488086\n",
      "  -4.0124993 -6.419482  -6.1446214 -5.3685822 -6.0068045 -3.6715684\n",
      "  -4.8357706 -4.2404127 -5.758171  -4.050371  -6.1975193 -5.9214997\n",
      "  -4.975137  -5.123497  -5.5436993 -4.9571953 -4.4303417 -6.1992555\n",
      "  -3.8552086 -4.1676517 -6.121565  -3.909282  -4.729394  -4.8664117\n",
      "  -5.582165  -4.677637  -7.006148  -4.541587  -6.984249  -7.14132\n",
      "  -3.66986   -4.310648  -3.5018752 -3.7525501 -5.327598  -4.8530116\n",
      "  -4.0438075 -4.784871  -3.703032  -3.6848822 -4.403405  -6.0620975\n",
      "  -2.456963  -4.983717  -4.9915304 -4.312846  -5.4589686 -4.796306\n",
      "  -3.4132864 -4.449579  -5.4240217 -5.225331  -5.9888515 -4.758742\n",
      "  -5.4254932 -4.9871745 -3.787354  -4.742603  -5.828885  -6.2041345\n",
      "  -4.4047403 -6.7381124 -7.5761437 -7.7924175 -6.410042  -6.9785333\n",
      "  -3.6629806 -5.271132  -7.510035  -4.232984  -5.1491385 -5.355043\n",
      "  -4.934682  -4.9125857 -1.6123732 -3.7548306 -5.5220942 -4.3411736\n",
      "  -3.7613416 -3.9070575 -4.62675   -3.384787  -5.058002  -4.2999425\n",
      "  -6.1481605 -5.6793447 -4.968585  -5.406743  -5.581665  -4.745978\n",
      "  -4.7163734 -5.9703155 -6.3443265 -4.600397  -3.8373733 -5.59901\n",
      "  -4.294838  -5.830343  -3.7226021 -4.94649   -4.4359035 -5.476016\n",
      "  -5.0431066 -5.195107  -5.574991  -5.510224  -5.4669647 -5.0175447\n",
      "  -4.5538707 -4.7058725 -5.220524  -4.8801265 -4.8628097 -4.639138\n",
      "  -4.4031024 -5.0025024 -4.9906654 -4.978128  -3.7911172 -4.1840568\n",
      "  -5.3100624 -3.766819  -4.8609347 -5.655237  -4.045609  -4.6087556\n",
      "  -4.3749723 -4.7871428 -6.8329554 -5.9728127 -4.467828  -5.885647\n",
      "  -5.455478  -5.473784  -4.375422  -5.575863  -4.15501   -4.746976\n",
      "  -3.993053  -5.1577425 -5.274887  -3.4191318 -5.464883  -4.3818154\n",
      "  -5.026036  -5.237256  -5.3825526 -4.0455995 -4.005794  -5.6610384\n",
      "  -4.311653  -5.4192786 -7.11023   -4.857662  -4.061989  -5.2512712\n",
      "  -4.614322  -3.22395   -3.6892617 -5.5555897 -5.53088   -1.6510316\n",
      "  -2.2844799 -5.774418  -5.563465  -4.284071  -3.3516746 -3.9344053\n",
      "  -1.9926219 -5.7471986 -5.5612707 -4.487038  -3.6762397 -5.777439\n",
      "  -4.266878 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 4.8185, LR: 1.000e-03\n",
      "Final parameters: [[ 1.4912112e+00  1.0927342e+00  1.5591351e+00  1.7628958e+00\n",
      "   2.5005705e+00  2.3091159e+00 -4.8572582e-01  1.3174661e+00\n",
      "   2.8326049e+00  1.4178632e+00  4.4425397e+00  1.1434850e+00\n",
      "   1.1980941e+00  5.2186984e-01  3.0450461e+00  2.8186891e+00\n",
      "   3.4892361e+00  4.2909751e+00  3.6703544e+00  2.7255034e+00\n",
      "   1.3063579e+00  4.5401950e+00  8.7599170e-01  1.5064521e+00\n",
      "   2.1800179e+00  8.6507469e-01 -5.2767050e-01  9.0358490e-01\n",
      "   5.4733515e-01  1.1820407e+00  2.1001871e+00  7.9035908e-01\n",
      "   7.9786527e-01  2.0331445e+00 -1.9709143e-01 -9.2022932e-01\n",
      "  -3.5243613e-01 -1.0753524e-02  4.1584802e+00  5.7640332e-01\n",
      "   3.7169955e+00  3.1259344e+00  4.3081021e+00  4.6092892e+00\n",
      "   2.8525546e+00  3.7505114e+00  4.1842666e+00  3.4552491e+00\n",
      "   8.6764371e-01  3.0742521e+00  2.7705090e+00  4.1475248e+00\n",
      "   3.5971158e+00  1.3880641e+00  2.9481494e+00  2.2717192e+00\n",
      "   2.5145860e+00  2.7582326e+00  3.5546100e+00  3.9309428e+00\n",
      "   1.6218473e+00  4.2402496e+00  1.7040724e+00  2.1134276e+00\n",
      "   3.3980055e+00  2.1882637e+00  6.9069618e-01  1.8206028e+00\n",
      "   3.4091748e-02  3.2872264e+00  4.6149758e-01  3.0385404e+00\n",
      "   4.2927570e+00  1.7441453e+00  2.8220506e+00  3.4725454e+00\n",
      "   3.0430911e+00  2.9253681e+00  2.9979150e+00  2.5052195e+00\n",
      "   2.0323317e+00  6.9164306e-01  1.4348173e+00  6.2814951e-01\n",
      "   1.7391182e+00  2.4184668e+00  1.4349788e+00  1.2212536e+00\n",
      "   1.6206146e+00  4.9456608e-01  2.2312481e+00  1.5105693e+00\n",
      "   2.5597227e+00  2.3342175e+00  2.4167655e+00  1.7892365e+00\n",
      "   2.0707793e+00  7.1103871e-01 -1.3271734e+00 -3.4841534e-01\n",
      "   4.5411482e-01  1.6742516e-03  9.3998045e-01  1.9872056e-01\n",
      "   1.1581450e+00  2.8273754e+00  9.5628923e-01  2.4892983e+00\n",
      "   1.9274459e+00 -6.5038139e-01  1.6530565e+00  7.0522320e-01\n",
      "   2.8156843e+00  2.1637530e+00  3.2850964e+00  1.7011235e+00\n",
      "   2.1104596e+00  4.8729143e-01 -1.2625195e+00  2.0917780e+00\n",
      "   1.2058872e+00  7.4936581e-01  4.8659864e-01  1.1243603e+00\n",
      "   3.2321284e+00  4.1311412e+00  8.6346567e-01  6.4676136e-01\n",
      "   2.4018509e+00  2.1299100e+00  6.3040775e-01  9.0856767e-01\n",
      "   2.1504936e+00  3.1509705e+00  4.9938645e+00  3.3219504e+00\n",
      "   4.1015530e+00 -3.4919873e-01  2.5251417e+00  2.6936851e+00\n",
      "   2.1395879e+00  8.1400627e-01  2.8340771e+00  2.4812133e+00\n",
      "   1.6178155e+00  1.6534696e+00  1.4930562e+00  2.4537208e+00\n",
      "  -3.6145806e-01  4.9238372e-01  2.5750194e+00  2.4429481e+00\n",
      "   1.3851246e+00  1.5116732e+00 -4.5890942e-01 -5.5996698e-01\n",
      "   1.6329432e+00  8.0304289e-01 -1.2499484e+00  2.1907534e-01\n",
      "   8.2678571e-02  1.8173687e+00  4.4115381e+00  1.9060613e+00\n",
      "   3.4250031e+00  1.0696784e+00  3.2080791e+00  3.2230897e+00\n",
      "   1.0747459e+00  1.8275977e+00  2.4605422e+00  1.9817932e+00\n",
      "   2.8902452e+00  3.4562635e-01  2.3782053e+00  6.9627863e-01\n",
      "   3.3150303e+00  2.9890814e+00  2.6631720e+00 -5.3464705e-01\n",
      "   1.7053727e+00  1.1663789e+00  2.8112786e+00  4.9100721e-01\n",
      "   1.5995036e+00  3.3527851e+00  1.3433071e+00  1.8395232e+00\n",
      "   1.1774600e+00  4.3552056e-01  1.6845945e+00  2.5067893e-01\n",
      "   9.2216298e-02  2.0920765e+00 -1.7764058e+00  1.3134228e+00\n",
      "  -2.3458195e-01 -1.1259361e-01 -2.6958010e-01 -7.3217160e-01\n",
      "   4.8100017e-02 -7.9088861e-01  1.4699403e+00 -1.4587767e-01\n",
      "  -1.1100490e+00 -9.4258767e-01  5.7261810e-04 -6.9698286e-01\n",
      "   8.5614049e-01 -1.3356696e+00  3.3929899e+00  4.0907225e-01\n",
      "  -9.2290439e-02 -1.0934875e+00 -8.4780890e-01  1.8346804e-01\n",
      "  -1.1196568e+00  2.2868387e-01 -3.2842761e-01  1.0858559e-01\n",
      "   7.2264552e-01 -4.5807564e-01  2.8188440e-01 -1.3543426e+00\n",
      "   2.8014171e-01  8.1583875e-01  2.2114236e+00 -8.5745007e-01\n",
      "  -1.9840970e+00  6.2420720e-01  1.2934454e-01 -2.7994068e-02\n",
      "  -6.7750686e-01  9.5399731e-01  6.4473909e-01  7.3745334e-01\n",
      "   1.7981441e+00  1.0829786e+00  9.9319017e-01  5.8767036e-02\n",
      "   8.5833207e-02]]...\n",
      "Epoch [30/100], Cumulative Loss: 11.9029, LR: 1.000e-03\n",
      "Final parameters: [[ 4.6684623   3.4519498   5.464783    3.229773    5.4685836   5.5235543\n",
      "   4.6409755   6.4933515   5.466471    5.9149237   5.159251    4.775029\n",
      "   6.8398757   4.991274    5.362619    3.2723937   4.114052    3.0743134\n",
      "   6.0080514   5.81388     5.8149104   6.4721813   7.106273    6.817842\n",
      "   6.7427044   5.2163625   4.4097004   6.0705795   6.830874    5.218067\n",
      "   7.6603127   4.6891937   4.47187     5.134563    4.531575    5.9432607\n",
      "   5.4484267   3.934105    3.4948606   4.7032623   5.876269    6.971118\n",
      "   6.0552583   5.1982145   4.485139    6.868687    7.5553775   5.8548384\n",
      "   6.595729    4.4914513   6.815925    6.690174    6.663351    6.4204926\n",
      "   4.5141377   3.6717985   0.3542003   3.520084    5.0057583   5.3723016\n",
      "   6.609987    8.036155    7.7624617   6.8070292   5.0489893   5.6935916\n",
      "   7.507397    6.228591    5.398524    5.4573183   5.554235    7.2530885\n",
      "   5.8160133   5.7868304   5.021321    3.9080484   3.3129823   2.9446685\n",
      "   5.4078274   6.5062385   7.445834    6.208524    5.332165    5.794408\n",
      "   3.4448519   5.164313    5.7174263   5.364663    7.9366226   6.1004148\n",
      "   6.7479467   4.906202    4.8695245   4.2961283   4.036588    4.979583\n",
      "   3.278779    1.6285098   5.025519    4.193189    4.253231    6.2124815\n",
      "   7.23072     5.256266    6.4335084   3.7704244   6.0266676   6.2138767\n",
      "   3.871312    6.42116     4.6337295   4.292167    5.2670107   5.2394447\n",
      "   3.5515203   4.6595545   4.474814    4.1636844   4.3592515   4.730305\n",
      "   5.299453    4.36106     7.0554013   7.0053253   5.105246    7.839628\n",
      "   6.6319857   5.6554885   5.856906    5.9226184   7.087914    8.614578\n",
      "   5.1017733   7.471561    6.1465807   7.7337275   6.2832665   4.9167156\n",
      "   4.9880505   6.519838    5.4971128   5.5271983   5.5313716   6.920258\n",
      "   4.452388    4.8635926   5.7200522   3.6594753   6.4897346   5.665113\n",
      "   5.7251005   5.793877    3.544246    4.95079     6.402637    4.1360674\n",
      "   3.5592082   5.1540937   5.37834     5.296075    4.4520025   7.579311\n",
      "   6.521511    5.1366806   3.5703058   6.8516893   4.638795    6.0639334\n",
      "   7.0511584   6.735638    5.3752084   5.847421    6.875729    6.2894397\n",
      "   5.8250017   2.7793233   3.253213    2.8800354   7.7335596   4.0064673\n",
      "   6.651011    5.226042    7.607763    5.3959246   5.338613    6.7415123\n",
      "   6.120836    5.086818    4.8580794   4.7254095   6.3383913   5.906859\n",
      "   6.9071784   5.458822    3.8938704   5.4137      4.529268    2.4865487\n",
      "   4.570132    6.723596    7.0248666   4.8563485   2.9127066   2.4576402\n",
      "   0.10254659  2.1045842   4.2432237   2.1457748   0.5769375   1.8341845\n",
      "   0.54639924  0.6377925   2.4382448   1.7919894   3.1151977   1.718228\n",
      "   3.152879    1.4077855   1.0939151   0.09559878  0.17984655  0.70155156\n",
      "   1.0454319  -1.2304502   1.2963172   0.1620682  -0.28508997 -0.9278782\n",
      "  -0.43681985  0.20212379 -0.06076306  1.3417811   0.5636442   0.9889317\n",
      "  -0.95182717 -1.578892   -0.9192371   0.48872972  0.22572571 -1.2470675\n",
      "   1.0220021 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 33.7609, LR: 1.000e-03\n",
      "Final parameters: [[ 1.9715672   4.557949    8.281257    6.116708    9.38873     9.200068\n",
      "   8.917957   11.509236    8.552606    7.215383    6.9388385   9.91426\n",
      "   8.514454    7.88912     9.353361    7.1104813   8.080997    6.7909236\n",
      "   6.2710614   3.0021653   4.5180945   6.591665    5.2609954   5.278428\n",
      "   8.061442    8.490948    7.878646    9.981896    7.865432   10.554324\n",
      "   7.8723946   9.243611    7.263683    8.505078    7.2328606   7.29996\n",
      "   7.577302    7.9639716   7.6769366   7.451732    7.991882    6.1565366\n",
      "   5.873214    4.988201    6.7426996   6.7350945   8.894147    8.759879\n",
      "   7.294226    8.788256   10.913474    9.82256     7.1006145   7.401384\n",
      "   7.384039    6.08819     7.6343765  11.394588    9.952279   11.734935\n",
      "   8.671744    8.28034     5.378696    4.6252775   6.4444656   7.2605624\n",
      "   7.7527523   7.6684384   8.627441    9.550091    9.469957    8.500041\n",
      "  10.083442    8.9466305   6.4183064   7.3382     10.532767   10.021125\n",
      "  10.7626505  11.833465   11.056305    6.3436737   4.693471    4.6545873\n",
      "   6.8826137   6.0333157   6.5403733   6.537265    7.9065595   8.750808\n",
      "  12.490199   11.225384    9.699345    8.191792    4.890981    8.8946905\n",
      "  10.454099   11.302678   12.717078   12.140413    9.349903    8.9245\n",
      "   5.7000046   4.157183    7.2506557   6.3535886   8.467997    7.703177\n",
      "   9.333412   10.840016   11.572106   11.274856    9.237335    7.5625577\n",
      "   7.62415     9.38901    10.124119   12.556075   12.936973   11.097783\n",
      "   9.556807    6.8365235   9.176439    4.968795    7.4597945   8.627367\n",
      "   8.519219    6.501372    8.148524   10.286518   12.302144   10.135095\n",
      "   9.211475   10.4166155   8.461418   11.106472   11.441563   10.267271\n",
      "  12.630964   12.734002   10.1191845   9.128976    6.0562925   4.916317\n",
      "   6.433448    4.895091    9.511926    8.392385   10.171122   11.620529\n",
      "  11.484663   11.215481   11.339106    8.567011    7.084981    9.510395\n",
      "   8.407544   10.385612   12.322958   12.094722    8.425089    9.027175\n",
      "   7.3537803   6.154158    8.399121    6.9430223   9.1451      7.86815\n",
      "   9.512354    8.037031    9.885643    8.830609   10.713828   10.206462\n",
      "   7.231292    9.567514   11.361906   12.6973715  10.798436   10.321483\n",
      "   8.831379    8.450502    5.8933353   7.124561    7.1620183   6.410664\n",
      "   8.846933    6.9938846   6.701754   11.471322   10.914401   10.386605\n",
      "   8.133091    9.917843    9.141633    9.164002   10.88707    11.513847\n",
      "  10.691958   10.03363     7.092397    4.4207754   3.1213603   2.4405785\n",
      "   3.4146116   2.175065    1.9842267   3.7750685   4.510796    6.121999\n",
      "   6.2152824   5.5813456   6.35163     2.968734    4.618332    5.099508\n",
      "   7.474482    7.7696347   7.7779346   8.009131    6.8205724   4.2070723\n",
      "   2.4862952   1.1127758  -0.25893465  0.9531128  -0.17597255  1.8262131\n",
      "  -1.404006   -1.7561532  -1.1670895   0.01350635 -0.47542188 -0.3130795\n",
      "  -0.16603428 -1.6735532  -0.98707086 -1.3824388  -1.9876375  -1.3059762\n",
      "  -1.4851807 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 7.7148, LR: 1.000e-03\n",
      "Final parameters: [[ 0.7221717   3.1646526   3.1274152   3.6823862   4.1586003   4.275367\n",
      "   5.1015744   3.9829156   4.999715    5.612207    3.5290172   4.785698\n",
      "   3.9963493   5.469363    5.3986883   4.5951304   4.3953977   4.638613\n",
      "   3.7202244   3.9849095   5.8496995   6.9158344   3.4914486   2.5101411\n",
      "   3.7232616   4.5285363   4.6504536   2.9023545   5.561006    3.6834996\n",
      "   4.0945373   4.6421943   4.555446    4.4670196   3.9551022   3.8863583\n",
      "   5.001142    7.046095    4.283299    6.2089353   4.0748625   2.3229718\n",
      "   3.3744774   5.9270415   4.7969737   3.7716818   4.828579    4.9728107\n",
      "   3.021093    3.6697457   6.0414414   5.185791    3.7410765   5.176807\n",
      "   5.1032596   4.716319    3.6499918   3.5582767   4.7136817   2.929424\n",
      "   3.001831    4.7017226   1.7958798   2.9391196   6.165676    6.58427\n",
      "   4.8033533   4.711666    5.0518103   3.147721    5.572275    4.5657144\n",
      "   4.975508    4.3918033   5.048897    4.3329606   1.9679792   3.6118145\n",
      "   4.969039    4.387993    3.7163243   0.83158416  2.4636147   4.0480046\n",
      "   2.0193965   4.376349    3.6877415   4.0191917   3.7251718   2.8096957\n",
      "   2.292692    6.653906    5.188061    4.356752    5.53913     4.204938\n",
      "   5.6462746   3.8865972   3.476661    4.3979034   2.4508283   4.0182743\n",
      "   3.3909411   4.633886    3.1944563   6.1928897   5.107499    4.30469\n",
      "   5.4340906   2.8275075   5.281113    5.034889    5.4936814   4.5298715\n",
      "   5.792965    4.212381    3.8806317   4.191276    4.084365    5.093098\n",
      "   6.532373    4.228397    4.907957    3.0612316   5.894732    4.5746255\n",
      "   6.8648987   5.6207905   3.2427464   5.125817    6.4752464   6.2228217\n",
      "   6.7360487   3.9830716   6.958611    5.5292687   6.2285485   4.5427775\n",
      "   3.741028    4.8341546   4.168162    4.279842    4.702605    3.1718242\n",
      "   3.874242    5.132424    5.276254    3.140341    4.1817045   4.814058\n",
      "   7.573864    4.473993    6.304585    4.022434    5.827702    4.5834846\n",
      "   4.841573    5.677385    5.943219    3.8777592   3.5956056   3.0642648\n",
      "   2.678817    4.04103     4.9858603   5.3110867   5.360274    6.405822\n",
      "   3.7805293   3.5278995   3.3967228   5.0397296   5.231058    5.1561337\n",
      "   5.6618      5.7579956   3.267922    4.192282    4.5499687   1.3703609\n",
      "   3.1189294   3.24971     4.8349924   2.028374    3.8027368   5.8070602\n",
      "   4.5387306   4.954087    3.8164904   6.5257816   4.5468235   4.696132\n",
      "   5.378843    4.807719    5.548044    3.8961751   4.619364    3.712915\n",
      "   5.7308187   2.6882238   2.492381    0.8192575   0.9467381   0.52657115\n",
      "   0.1215743   1.1118273   0.39105672  1.1043195   0.7966474   1.2233622\n",
      "   1.5700796  -0.07634349 -0.68342304  2.3443534   0.5542448   0.3188274\n",
      "   0.6513201   1.0724672   0.47571948  0.17446408 -1.2862909   2.2473543\n",
      "   1.0206306   1.4131528  -0.20229907 -0.12964731  3.5229642   0.66265\n",
      "  -1.0949398  -1.4776233   0.32494408 -0.435183    1.1569963   0.7430852\n",
      "  -0.7968381  -1.5110133  -0.67033243 -0.7842126   0.40982935 -2.4918728\n",
      "   0.59838194]]...\n",
      "Epoch [60/100], Cumulative Loss: 35.2947, LR: 1.000e-03\n",
      "Final parameters: [[ 3.7170675   6.557611    9.809956   10.13206    13.20326    10.668624\n",
      "  11.110749   11.492323    9.997433    9.682915    8.421916    7.769719\n",
      "   9.71484     7.352712    6.8726664   3.5989003   5.916311    8.163677\n",
      "   5.0014477   3.4282203   6.7991853   7.550539   10.253984   11.905391\n",
      "  13.377962   10.594155   10.623287   12.046673   11.359629   11.109906\n",
      "   9.4819565   8.70142     7.6436253   5.8475156   6.7889633   5.6240554\n",
      "   5.266844    5.261265    5.396923    3.9273903   6.2417316   7.051098\n",
      "  11.46537     9.731514   12.733971   11.340629    9.696019   11.578787\n",
      "  11.327409   11.686       8.443008    6.905619    7.6580424   7.153099\n",
      "   5.4198284   6.5553274   4.420139    6.1957192   7.4466844   5.50032\n",
      "   6.0799565   8.895996   12.501195   11.949911   12.127779   11.406361\n",
      "  10.527607   10.872855   12.433978   10.433013   10.447548    7.0246267\n",
      "   9.834446    6.8885374   7.1989393   5.759364    4.611834    7.340378\n",
      "   3.756145    5.760395    6.692355    9.312445   10.896016   13.074987\n",
      "  12.511435   10.29346     9.610092   11.176647   12.067908    9.950757\n",
      "   8.332464    6.9462585   7.282936    4.96143     3.4492292   1.2815372\n",
      "   4.039775    2.418299    5.0048265   4.4287124   3.42867     7.803234\n",
      "  10.906279   11.112254   11.270373   10.15894    10.700234    9.132869\n",
      "  10.403294    9.841831    9.262412    7.116684    8.938749    5.167836\n",
      "   7.268559    5.4451265   4.9077024   5.4541726   5.9636583   4.9429426\n",
      "   6.3989143   8.139909   12.457305   12.945284   12.766967   10.9824095\n",
      "   9.189697   11.639275   13.772504   11.823765   10.908247   11.242046\n",
      "   8.723554    8.247402    9.47511     6.7356424   6.417958    8.898038\n",
      "   7.239246    6.257331    9.054702    7.808153    7.5962596  11.420975\n",
      "   9.767758    8.255644   10.640788    9.957494   11.50728     9.418071\n",
      "   8.804087    8.774794    7.3030567   8.201785    7.8915997   5.312098\n",
      "   7.4841056   6.782238    5.9940047   6.7057056   7.277775    8.457139\n",
      "   9.192353   10.798739    9.732315   11.52369    10.088229    9.54044\n",
      "   9.475718    9.289609    8.563749    7.7214      7.277433    4.75599\n",
      "   7.4122343   7.1007066   5.3039937   4.6823106   4.970922    5.589283\n",
      "   6.9350843   8.283873   10.945242   13.095227   12.175994   10.166542\n",
      "   8.660702   11.4974375  10.77184     7.033708    9.414794    7.491808\n",
      "   5.653592    4.011014    5.1317134   7.9916487   4.7685385   5.6744466\n",
      "   6.4862256   4.6298985   6.157693    4.880916    6.4934516   4.769576\n",
      "   5.646752    4.752922    2.905039    4.038205    6.036676    4.4496737\n",
      "   2.4984734   2.7767422   3.325316    1.7793698   2.8415868   1.6314516\n",
      "   1.8181756   0.46976843  2.3004122   1.6070471   0.4023778  -0.25945497\n",
      "  -0.02916327 -0.7674633  -1.4381554   0.8213127  -1.0851631  -0.82250327\n",
      "  -1.1071931   0.32471406  0.5468843  -0.0318398   0.6672874   1.3109123\n",
      "   0.19803992  1.6104777   1.4723825   0.8576779   1.4237386   1.1349828\n",
      "   1.6330721 ]]...\n",
      "Epoch [70/100], Cumulative Loss: 7.2871, LR: 1.000e-03\n",
      "Final parameters: [[ 1.078819    2.3004382   3.962701    5.289482    4.724437    6.9166183\n",
      "   7.4999704   5.017327    4.2108154   2.998999    6.347719    3.5532844\n",
      "   6.70837     5.752998    6.8783116   6.371161    5.998547    6.191132\n",
      "   6.057378    7.3721275   5.3688297   4.553954    3.868015    4.3278847\n",
      "   2.9103022   5.012561    6.2768054   7.4109774   6.687474    7.4489746\n",
      "   5.7796803   7.1375294   5.0060115   4.368791    3.8426073   5.0323167\n",
      "   5.6751266   6.220397    4.669295    4.09217     4.359438    3.6325767\n",
      "   6.6348205   6.7526364   7.74946     6.703032    4.947705    5.0642743\n",
      "   1.8238999   2.6146667   4.566485    3.4347777   6.3708506   3.2252102\n",
      "   6.115944    3.9777956   4.5923944   5.8324766   5.3738675   4.6230564\n",
      "   7.2300763   6.4796567   5.3550577   5.195216    3.7150674   7.727305\n",
      "   6.6494303   3.1561337   6.5644493   3.3456717   4.317801    4.0612864\n",
      "   6.054706    4.5406146   5.850519    4.0011325   3.970162    3.8737285\n",
      "   3.6408546   5.498478    4.6254725   5.575325    3.632359    5.675211\n",
      "   4.263368    3.3132691   4.0401974   4.0803704   2.9907744   4.1038003\n",
      "   3.4844978   6.263213    5.502934    2.6632833   5.502058    5.611761\n",
      "   1.38806     2.4116397   1.0236112   2.9661489   2.9344227   3.559099\n",
      "   2.5418034   4.336695    4.568686    4.641746    3.953795    6.419248\n",
      "   4.429089    3.761926    2.2528613   4.387317    6.093378    5.759373\n",
      "   5.457708    4.187386    5.14733     5.21308     4.664678    3.4830449\n",
      "   2.771084    5.197058    3.7104564   5.8865128   3.908252    5.050243\n",
      "   7.954109    7.3956976  10.584909   11.20219    12.703905   11.385067\n",
      "   8.412374    7.28605     7.1163235  10.306006   11.461273   11.435347\n",
      "   9.319995    7.949661    5.3922057   6.2910447   4.6904664   4.281534\n",
      "   5.405787    2.9733477   4.1283617   6.46271     4.165462    2.97056\n",
      "   3.0222383   4.5801816   3.0135448   5.604118    4.701849    4.3981853\n",
      "   5.785039    4.024076    4.9824657   4.4938874   3.657434    4.706703\n",
      "   4.456239    4.4912853   6.491168    6.492475    3.9408946   2.9585526\n",
      "   3.6603522   3.1148148   5.434617    4.3561087   5.16597     4.877969\n",
      "   5.2797985   4.821454    5.4091196   4.129158    3.9933453   3.9058194\n",
      "   5.223322    4.18624     4.6545362   4.6669393   4.9752116   5.261146\n",
      "   5.027243    4.951684    5.215349    5.886295    2.170612    7.085005\n",
      "   5.7369633   3.1564434   3.244159    4.329656    2.7318327   4.732692\n",
      "   3.299869    2.2433624   1.7027326   0.7936764   0.4935487  -0.11947332\n",
      "   1.1743178   0.7866819   2.110498    0.8061199  -2.1065395  -1.7952142\n",
      "  -0.930484   -0.26456487  0.24840628  0.04889759  0.33623984 -0.8688502\n",
      "  -1.0463198  -1.1530069  -0.1848134   0.821385    1.7550853  -1.6900649\n",
      "   1.0611649  -1.346009    1.1478257  -0.29349938 -1.3671303  -1.3749411\n",
      "   2.3349671   0.67931294  0.45475185 -0.50254923  0.40461403  1.2987428\n",
      "  -0.8771536  -0.1140477  -0.06522899  0.26376367  1.782267    0.7325973\n",
      "  -0.18563169]]...\n",
      "Epoch [80/100], Cumulative Loss: 59.2916, LR: 1.000e-03\n",
      "Final parameters: [[-3.5902016  -1.9473324  -1.692589   -5.3546185  -2.9936256  -2.6451871\n",
      "  -2.727703   -2.6803386  -1.9725496  -4.0466585  -4.450731   -3.0306115\n",
      "  -2.3065112  -3.312189   -2.8546338  -3.4938962  -3.7715313  -3.558885\n",
      "  -3.0670679  -3.1516595  -3.048746   -4.27768    -1.1117467  -2.0636547\n",
      "  -2.7991626  -4.720712   -2.941796   -2.7643478  -1.8383853  -1.5922891\n",
      "  -1.4555998  -1.7374914  -0.48621768 -2.9348447  -3.5652044  -1.3524771\n",
      "  -3.0634208  -3.210249   -2.534428   -1.6741627  -2.5938826  -3.4340851\n",
      "  -5.277349   -3.199358   -2.9311266  -3.307872   -5.228446   -3.2769651\n",
      "  -1.1480249  -2.6620307  -2.2735634  -3.7647738  -2.9228387  -4.9807663\n",
      "  -5.186042   -3.0935981  -1.5930792  -3.1491337  -4.694949   -2.6409018\n",
      "  -4.033671   -4.6958733  -4.6291575  -5.013207   -4.366139   -1.5790657\n",
      "  -2.353137   -2.0819075  -4.134394   -1.7349298  -2.3886957  -4.479291\n",
      "  -3.8921869  -2.9500415  -4.140121   -2.6862652  -4.1319776  -5.8023148\n",
      "  -3.527872   -3.6979265  -3.7948785  -5.0010796  -2.6007946  -2.2277668\n",
      "  -3.6142917  -1.6940433  -3.6171153  -2.9918208  -3.1395183  -3.617407\n",
      "  -1.8265392  -3.4749572  -2.2796082  -4.356698   -2.6987486  -2.957794\n",
      "  -5.2029715  -3.2316124  -2.218366   -5.4774966  -4.687109   -4.2437105\n",
      "  -4.447823   -3.4849072  -1.3512664  -2.2724676  -1.2089635  -3.9759977\n",
      "  -1.0235082  -0.9596957  -3.9075358  -4.5300274  -2.2275724  -1.7999753\n",
      "  -4.2074585  -1.6269507  -2.301026   -2.38462    -4.7937083  -4.8940387\n",
      "  -4.58355    -4.0536785  -4.6682763  -2.4635696  -4.3411946  -2.9413338\n",
      "  -1.9136258  -1.4725652  -3.9664085  -1.6781996  -2.4664204  -2.9929152\n",
      "  -2.8763905  -2.4625893  -3.4145396  -1.6488663  -2.411301   -4.245456\n",
      "  -0.05598155 -2.9883819  -1.8072436  -1.7285527  -4.376854   -4.9645658\n",
      "  -5.506018   -3.4190621  -3.0734448  -2.5789192  -1.0480522  -4.964288\n",
      "  -3.226416   -2.4225843  -1.9078296  -3.4748285  -1.2591685  -2.7835274\n",
      "  -2.8136847  -2.6858308  -5.148933   -5.616041   -5.2987027  -5.5346465\n",
      "  -4.5940948  -5.287212   -6.029338   -3.814382   -4.044398   -3.469512\n",
      "  -3.995113   -3.3415482  -2.2449198  -3.7652733  -2.5323446  -2.2542272\n",
      "  -3.4745219  -3.1368008  -0.9623926  -1.8886877  -4.7038302  -3.729104\n",
      "  -4.891296   -2.407505   -4.5560546  -2.7056487  -3.0349152  -3.8978808\n",
      "  -3.8665621  -2.6799543  -0.2319437  -1.4390473  -1.4481841  -3.8109026\n",
      "  -1.6041647  -2.6679707  -2.5453758  -3.808892   -1.9324448  -3.5119076\n",
      "  -3.5276208  -1.4509257  -3.0031183  -5.6961794  -2.6822822  -4.313935\n",
      "  -4.626893   -2.7187512  -3.9383497  -4.3142323  -3.671053   -3.2668915\n",
      "  -2.575556   -4.379008   -3.5306206  -2.9694886  -3.2337189  -4.041748\n",
      "  -3.0990856  -3.8922913  -2.9197176  -3.415796   -3.8828962  -4.4944286\n",
      "  -3.3195755  -2.9232583  -3.6321797  -2.6820118  -1.9194138  -2.9253118\n",
      "  -2.9175386  -3.2382343  -3.0362957  -1.1283317  -2.8474088  -3.848129\n",
      "  -2.4960742  -1.6192299  -3.3329175  -2.770096   -3.2967172  -1.7800714\n",
      "  -2.2595923 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 237.4488, LR: 1.000e-03\n",
      "Final parameters: [[ -3.7695189   -3.5096383   -2.962281    -4.202882    -2.5342915\n",
      "   -2.246899    -1.809269    -0.8951514   -1.3441979  -11.835692\n",
      "   -8.254641    -7.041496    -5.6903095   -4.3333383   -3.8136666\n",
      "   -2.474629    -3.0129082   -4.176931    -2.6099877   -3.1159694\n",
      "   -3.0004132   -3.0554028   -1.9639851   -4.2986307   -3.8913193\n",
      "   -2.6569965   -2.088597    -2.0913553   -2.6042073  -10.743393\n",
      "   -7.69924     -8.874778    -5.511496    -3.3154428   -6.005074\n",
      "   -5.3804917   -3.9011056   -3.043088    -3.5636818   -4.0492554\n",
      "   -4.1160927   -3.2034683   -2.9403555   -2.8127205   -1.6166091\n",
      "   -1.5897477   -1.6553515   -2.3768318   -2.4071283   -7.842844\n",
      "   -6.8359485   -6.7960405   -4.086148    -6.212455    -3.4788916\n",
      "   -1.9993409   -3.2443721   -4.2483974   -2.528003    -2.7453852\n",
      "   -2.1767612   -2.70437     -2.8733103   -3.4689808   -2.2936916\n",
      "   -2.8824487   -4.7329497   -2.7787032   -3.001292    -7.68966\n",
      "   -8.1427      -6.0849867   -6.0521774   -4.5420814   -3.8182526\n",
      "   -2.602812    -4.147758    -3.2582412   -2.0618212   -3.6087668\n",
      "   -5.4892974   -2.9510627   -2.5855207   -2.6554759   -2.2161353\n",
      "   -3.5362823   -1.611296    -2.8858018   -2.4929173  -11.149611\n",
      "   -7.6036663   -7.1694202   -5.9155602   -4.8415046   -4.296273\n",
      "   -2.3323493   -4.45472     -2.0245776   -2.1254108   -2.3326592\n",
      "   -3.693414    -1.7764757   -3.3891356   -3.0865548   -2.3697686\n",
      "   -1.0671939   -4.018139    -3.2911148   -2.7153542  -10.216768\n",
      "   -8.383424    -6.6730533   -4.762828    -5.9998384   -4.177736\n",
      "   -2.1609838    0.04896402  -3.2945743   -2.8968441   -4.1072354\n",
      "   -1.4173558   -2.9993527   -3.149244    -3.4639926   -3.5805836\n",
      "   -2.994593    -1.7135135   -2.5702322   -3.3101568   -1.4094683\n",
      "   -4.8029537   -3.8452668   -5.329227    -5.3495584   -4.3605523\n",
      "   -3.8076112   -3.7496345   -3.9243903   -2.3259819   -4.100463\n",
      "   -2.445946    -2.7694328   -3.6290429   -4.6943636   -3.44535\n",
      "   -1.5754339   -3.5122638   -3.6871076   -1.2117456  -11.388729\n",
      "   -7.284489    -8.698439    -3.7573407   -5.680024    -3.99938\n",
      "   -3.9379196   -3.4168637   -3.0283368   -5.5503483   -1.8269836\n",
      "   -2.7245526   -2.0007713   -3.1196032   -3.3250165   -4.1888943\n",
      "   -2.0449803   -3.4155738    0.06212912  -1.4274813   -9.581182\n",
      "   -9.650734    -7.368479    -5.0351377   -4.417964    -3.571681\n",
      "   -2.933489    -5.908728    -4.953385    -4.2052107   -2.9750838\n",
      "   -1.9417539   -4.361421    -2.7204735   -2.4396164   -2.7559388\n",
      "   -2.5060282   -4.6637115   -3.2964854   -3.0049105   -9.334303\n",
      "   -8.041823    -6.9190702   -5.2114873   -2.978325    -1.9312042\n",
      "   -3.252254    -3.9780686   -2.9162514   -3.00552     -1.4970154\n",
      "   -4.435508    -2.8109398   -3.46069     -4.5397086   -3.4830425\n",
      "   -3.5858657   -3.0152388   -4.184461    -3.4960926   -3.1967492\n",
      "   -5.407855    -6.6127057   -4.8757935   -5.987492    -5.264516\n",
      "   -3.6890354   -4.5066423   -3.5304434   -4.2148533   -5.071238\n",
      "   -3.6050663   -3.0336099   -3.430579    -1.5530027   -2.2239218\n",
      "   -2.5385559   -3.9585845   -2.9464517   -3.9850864   -0.19558899\n",
      "   -1.2413571   -1.7263423   -1.2400311   -3.0948799   -2.40846\n",
      "   -1.900047    -3.4321358   -3.3254402   -2.4350264   -2.4792645\n",
      "   -4.8224897 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 5.2285, LR: 1.000e-03\n",
      "Final parameters: [[-0.5062007   1.1903315  -2.7000415   0.14814061 -1.3516643  -0.6932758\n",
      "   0.9076963   0.36910576  0.02242863  0.8101203   0.5396299  -1.2107401\n",
      "   1.3191227   0.8411748  -0.68427634 -0.8907066   0.17664465 -1.4880687\n",
      "   0.45445734  1.4431378  -1.359923   -0.77122426 -2.0323155  -1.8938584\n",
      "   3.2668314   0.5034858   2.8746464   1.8906449   0.31291458  2.0661995\n",
      "   1.2000897   1.6365539   4.458329    0.8449139   1.6063553   0.56418425\n",
      "   3.291933    1.7665318   2.3536513   2.9420683   3.530581    4.7634463\n",
      "   5.6374707   5.552494   -0.3287282  -1.6422642  -2.1621273   1.9013386\n",
      "  -2.3100393  -1.3105162  -0.11547058  0.12148483  0.42157096  0.65517914\n",
      "  -0.80263126  0.99556583 -0.96781856 -4.090647   -0.01889161  1.9007771\n",
      "   2.4228108   2.775275    2.44349     2.1469595  -1.5547389  -3.2501621\n",
      "  -1.1432236   0.2459819  -1.1722966  -1.2525994   0.43894595 -1.03081\n",
      "   1.2282643   2.7939448   2.2676249   1.9834332  -2.413292   -2.8206148\n",
      "   0.5850159  -0.01326755  0.19444782 -1.7229781  -3.0185215  -3.560548\n",
      "  -3.2416275  -2.6527448   0.01184661  1.6297488  -0.71638787 -2.179311\n",
      "  -0.03507279 -1.2572533   0.7380072   0.08136712  0.36192602  0.14302945\n",
      "   0.21183342 -0.6290382  -0.18609062  1.9014642  -1.2421852   0.65117747\n",
      "  -0.0076308   1.2633241  -0.44668597  1.06307     1.472814    2.73529\n",
      "   0.54939234  1.3090228   0.59896845 -0.07013884 -0.64237094 -1.1869633\n",
      "   0.9756992   0.4035857   1.8225651   1.0120745  -0.4973761   3.0797193\n",
      "   0.33341044 -2.0886092  -6.1682024  -3.8302596  -4.045755   -3.3982024\n",
      "  -0.6617355   1.5752208   1.9438004   0.2629845   2.0533197  -2.53339\n",
      "  -1.2723546  -1.0600376   0.01953017  0.8854018   0.6579019   0.63553786\n",
      "   2.260849    0.51134264  3.4146187  -0.6963443   0.19338666  0.2389431\n",
      "  -1.3025144  -0.05263735  2.12967     1.3706458  -0.9004625  -0.93739533\n",
      "   0.23270933  1.1934267   1.5987513   0.45094857 -0.2656249   1.3917738\n",
      "  -1.5344985  -1.5884883   1.9544874  -0.06201211  3.1011534   4.3402386\n",
      "   2.176766    1.3309025  -1.6533722  -2.310283   -1.6075774   1.1112928\n",
      "  -0.9137095  -2.1296465  -0.72264946  0.39324343  2.1944144   2.3563094\n",
      "   1.1109811   0.76521534 -4.422781   -1.091172   -0.08377825  0.52550894\n",
      "   1.1892318   1.1750098   2.144932    2.4874425   1.1572881   1.0555073\n",
      "   1.0384176   1.4024669   0.7653118  -0.96892047  1.5124161   0.16586274\n",
      "   1.1721324  -0.4715045   1.43044     2.386657   -1.4206667  -1.2648581\n",
      "   1.3827775   2.6738162   0.2228787  -0.2053542  -1.3342167  -1.4966103\n",
      "  -0.8178625  -0.8614652  -0.37139732 -0.9209189  -0.15371548 -0.6280998\n",
      "  -0.38408843 -0.3475776  -2.1033068  -0.69450176 -0.4414285  -0.7320606\n",
      "  -0.36109766 -1.0983484  -0.38394964 -1.4953334   0.81843716  1.3231127\n",
      "   1.3469648   0.6284834  -1.3807089  -2.0935102  -0.6623501   1.2786226\n",
      "  -2.0762694  -1.3982705  -0.67629397 -0.48451585 -0.36629194  0.19644067\n",
      "  -1.1446065   0.31620976 -1.789763   -0.38550776 -0.6917928   0.27627453\n",
      "  -0.32760525]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 10.034215927124023\n",
      "Final parameters: [[ -1.3792224    2.4543564    3.1090465    3.2922742    1.8724324\n",
      "   -3.3815186   -1.3211317    1.3667802    2.8776648    2.8868184\n",
      "    2.0853522    3.2386384    4.4213405    4.2230716    4.2983794\n",
      "    6.762956     4.6589365    6.874332     6.947861     4.236727\n",
      "   -0.02095715   0.58628243   2.2338274    0.9713115    2.6669812\n",
      "  -10.180881   -11.203421   -12.690328    -6.7256045   -4.5135455\n",
      "   -0.94892913  -0.14968088   5.6039524    2.1529467    1.5757389\n",
      "   -1.5693793   -4.407265    -1.9286371   -1.9950417   -0.03886595\n",
      "   -1.169215     0.5359016    1.7966483   -0.98295444   1.3840009\n",
      "    0.7771605    4.407796     5.1342077    3.1715055    3.5706878\n",
      "    2.779659     4.303517     6.2949953    3.8022916    0.7606746\n",
      "    4.433283     2.0587575    4.8944435    6.677784     4.627561\n",
      "    5.0135384    3.2668896    4.818319     2.532694     3.5432734\n",
      "   -2.3954988    0.92679465   0.6435666    2.234933     3.0953043\n",
      "    2.625914     2.187624     5.377066     4.305862     6.657335\n",
      "    9.16168      7.0004826    5.903046     6.6567497    4.544199\n",
      "    3.719733     1.495788     4.5007677   -2.8369472   -2.687875\n",
      "    0.963111    -8.361548    -7.588464    -3.8573158   -3.412483\n",
      "   -2.0721524    3.0103052    5.237408     4.220492     4.471386\n",
      "    6.8423405    6.146963     4.7916865    5.4831424    5.3645678\n",
      "    2.4866142    3.6923807    3.0746777   -2.005068    -1.0129746\n",
      "    0.57797015  -1.9609865   -4.230288    -1.9361906   -0.9350534\n",
      "   -0.8293076    0.9114351    5.161808     4.5690284    2.9925244\n",
      "    5.566031     5.11124      6.135008     5.0428834    3.442679\n",
      "    2.895287     2.8620403    4.495496     4.1786747    3.5667613\n",
      "   -2.5090225  -17.276258   -18.783882   -12.229556    -8.32815\n",
      "   -5.4987655   -3.7360156   -9.1597185   -6.6644816   -3.019348\n",
      "   -1.9875106   -1.0969553   -2.8935797  -11.470458    -7.952946\n",
      "   -6.011831    -4.1775246    0.5595468   -8.026526    -7.3781776\n",
      "   -5.7401395   -2.443574    -2.829047     0.56942195   1.4272033\n",
      "    1.4960113    1.6368221    3.6815033    4.090164     4.047594\n",
      "    1.9013895    1.8793975    4.147891     3.4620137    0.9333674\n",
      "    3.1563153    0.21532139  -0.48665     -3.8827946   -2.4959886\n",
      "   -0.9867528    2.6624691    3.0942454    3.106937     4.257514\n",
      "    3.6194685    2.8146513    7.348882     2.5139172    4.5745163\n",
      "    6.378457     4.421164     6.337763     6.103897     3.7790961\n",
      "    1.9879735    1.8577802    2.3351123    0.32974273  -1.213737\n",
      "   -1.9883577   -0.54696304   1.8216531    1.9165114    1.8571657\n",
      "    3.2649698    1.664249     7.604863     6.7998753    2.9200726\n",
      "    1.4056435    2.2255614    2.680005     3.7756588    2.826173\n",
      "   -0.9844214   -1.5992082   -1.5204109    0.04885538  -1.3976668\n",
      "   -0.63001686  -1.0299144   -2.3612783   -2.071825    -1.7979715\n",
      "   -1.6682959   -1.6281039   -1.5885162   -2.5048466   -2.5017498\n",
      "   -1.606391    -2.465064    -2.5905128   -3.173679    -3.1589932\n",
      "   -2.172937    -0.26507136   1.5098399    3.5579824    3.9829576\n",
      "    5.350697     5.0963      -1.7491217   -0.74823684  -0.6873318\n",
      "   -3.443471    -3.160924     1.963851    -0.15215571   1.7131687\n",
      "    3.7672524    3.665407    -3.1483262   -3.631269    -5.856715\n",
      "   -4.736662  ]]...\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "initializer = Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "writer = SummaryWriter(\"diab_train/lr1e-3_MSE_2_layers\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=0.9, writer=writer)\n",
    "writer = SummaryWriter(\"diab_test/lr1e-3_MSE_2_layers\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4612ff7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be05e4802b864a3f8923bcd90208b2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], Cumulative Loss: 5001.1738, LR: 5.000e-04\n",
      "Final parameters: [[-23.396877 -24.608862 -23.49851  -23.936354 -26.391235 -23.971785\n",
      "  -22.233263 -22.525581 -24.933506 -23.00616  -25.495253 -23.448362\n",
      "  -21.919514 -22.958647 -21.785585 -24.376057 -22.45343  -21.993933\n",
      "  -21.390047 -22.530174 -21.478031 -22.816158 -22.059082 -21.983599\n",
      "  -21.396011 -23.81721  -21.901081 -22.355263 -23.101    -22.637474\n",
      "  -23.580708 -22.207954 -23.177761 -21.942753 -21.46756  -23.402388\n",
      "  -23.70645  -21.9947   -23.57644  -22.215427 -21.887955 -23.699196\n",
      "  -24.087643 -21.345852 -22.376211 -21.441242 -22.500937 -22.729412\n",
      "  -23.451931 -21.952562 -22.06522  -22.2372   -22.513393 -21.229807\n",
      "  -20.639444 -21.741886 -25.373957 -22.485346 -23.829636 -22.915464\n",
      "  -22.118513 -23.524437 -22.987751 -21.907087 -21.971085 -22.720703\n",
      "  -24.189623 -22.173397 -21.19357  -23.48855  -21.814814 -23.417967\n",
      "  -23.241966 -23.847506 -22.367666 -23.414423 -23.894053 -23.628983\n",
      "  -20.992355 -22.34199  -23.493004 -23.547495 -22.190636 -21.06872\n",
      "  -22.024174 -22.708574 -22.507944 -22.714407 -24.126442 -23.739332\n",
      "  -22.129414 -22.370392 -20.54661  -23.64086  -22.516422 -23.13942\n",
      "  -21.849852 -21.88822  -21.775196 -23.296904 -25.148916 -23.590233\n",
      "  -23.230629 -23.925783 -22.865553 -23.19863  -23.149052 -23.470385\n",
      "  -23.225069 -22.951395 -21.863504 -21.959024 -22.264221 -22.841534\n",
      "  -23.92766  -23.945047 -21.72443  -23.660418 -23.599934 -24.435675\n",
      "  -22.978657 -23.510574 -22.00487  -19.966991 -24.483847 -22.35673\n",
      "  -20.928072 -24.218702 -23.394655 -22.735712 -23.009132 -23.930407\n",
      "  -22.96202  -21.543562 -23.307882 -23.923594 -22.460032 -23.08971\n",
      "  -23.712053 -23.153572 -22.523506 -21.433979 -21.84715  -21.004314\n",
      "  -22.798256 -21.634361 -21.481766 -23.906887 -22.906416 -23.935854\n",
      "  -25.217562 -24.712013 -21.212955 -23.658094 -22.79786  -24.150253\n",
      "  -23.166967 -23.514677 -21.001616 -22.415585 -24.079716 -21.747055\n",
      "  -20.479372 -23.319185 -23.141716 -21.550306 -22.938662 -23.451424\n",
      "  -21.676935 -22.16558  -22.987179 -24.428532 -22.955734 -24.554798\n",
      "  -23.387396 -21.608086 -23.042078 -22.735258 -23.267279 -21.824518\n",
      "  -23.61403  -22.729525 -21.229635 -21.88051  -24.307028 -22.158873\n",
      "  -21.228834 -21.016376 -21.761541 -22.212282 -23.433159 -21.785448\n",
      "  -25.44105  -24.261839 -22.081135 -26.156355 -22.59207  -22.173758\n",
      "  -22.68413  -22.572018 -25.179182 -22.115889 -22.885221 -22.620773\n",
      "  -24.606184 -22.547194 -23.879381 -22.473873 -22.88904  -22.697481\n",
      "  -23.44903  -24.19543  -22.372992 -24.191359 -23.344448 -22.69417\n",
      "  -24.4062   -22.258595 -22.432333 -25.39983  -22.314299 -23.126783\n",
      "  -22.91817  -24.124298 -21.857416 -23.784416 -22.495914 -24.918869\n",
      "  -24.243786 -23.154646 -21.90546  -23.452465 -21.532654 -22.69213\n",
      "  -22.657345 -21.664412 -22.229008 -22.000444 -23.190311 -20.882029\n",
      "  -22.798672]]...\n",
      "Epoch [100/500], Cumulative Loss: 48.9934, LR: 5.000e-04\n",
      "Final parameters: [[ 3.7700782   5.5907865  10.282126    9.796659   14.0788765   9.457167\n",
      "   6.005129    1.3336685   0.01438665 -0.6556819   1.0734386   1.3526306\n",
      "   6.3317633   8.556393    9.310591    9.995587   11.442416   13.431012\n",
      "   7.790282    9.500332   12.326384    9.306266   10.1603365   9.426922\n",
      "  12.793697   12.795095    9.440591    6.309924    8.128393    9.008398\n",
      "  10.26194    10.544949    9.422574   11.104521   10.585342   11.078458\n",
      "   9.954769   11.270613   12.082043    8.501469    8.490574    5.408744\n",
      "   5.0730586   7.524205    7.4574347   8.272199    8.453531    8.242767\n",
      "   8.253676   11.130253   11.766954    7.380251    6.734141    7.5392036\n",
      "   8.546904    7.979701    7.4906416   6.95831     8.617804    7.613821\n",
      "   7.4374475   7.261057    7.976548   10.389015   12.769098    9.008467\n",
      "   8.145548    2.4332678   2.5039577   2.7258058   6.945463    8.542364\n",
      "   7.631121    8.838019   10.88129     6.0613117   9.157852    8.7470255\n",
      "   8.658672   10.142817    8.498658    9.155261    7.787542    9.439226\n",
      "   9.167104    8.396147    8.192451    2.6483924   2.2305782   3.3691254\n",
      "   5.555597    5.931302    8.235065    7.8762884   9.292527    6.247778\n",
      "   8.508543    7.31831     7.4478664   6.0623107   7.1008105   5.8769193\n",
      "   5.8372955   7.4981017   8.083835    8.473289    6.835527    4.730947\n",
      "   3.2738469   5.953742    9.349854   11.574057    8.363747    8.440515\n",
      "   9.447188    7.867591    8.954273    9.614999    8.648676    6.7681227\n",
      "   7.4415      6.8081274  10.331987   11.455948   10.6781025  10.291806\n",
      "   6.722001    4.0747237   5.3869486   5.133282    5.179471    9.650668\n",
      "  11.034507   12.607575   11.466661   11.30804    12.830892   13.737817\n",
      "  12.736967   10.295807    8.846408    8.608541    6.3774676   8.417874\n",
      "  10.792744    9.995149    8.098039    7.1633215   6.4180017   8.406667\n",
      "   6.0062866   9.482454    8.645617    8.02886     8.570285    7.8251343\n",
      "   8.516656    8.799665    9.569803    6.579356    5.7491393   6.825434\n",
      "   6.4652843  11.015934    8.313915   11.540167    6.200175    6.197333\n",
      "   4.4390726   5.4830465   8.497946    7.0749235   8.184059    9.369634\n",
      "   9.145018    8.698221    9.479314    8.326292    7.864597    8.210959\n",
      "   6.2137136   7.142678    7.101613    7.028844    8.466421    9.2560215\n",
      "   5.607767    3.9875245   4.298221    2.8588457   6.683578    7.1194634\n",
      "   8.758996    7.6175337   9.369828   11.856583   10.836918    7.960342\n",
      "   9.172245    7.3072953   3.129408   -2.0962634  -4.448198   -3.962106\n",
      "  -0.8258497  -1.4052677   1.7834628   3.3455906   1.3415322   3.8580139\n",
      "   5.1004314   2.3139536  -1.143732   -0.8635063  -3.1156468  -2.3090167\n",
      "  -2.7707105  -2.4528482  -3.0821843  -3.770922   -4.2014594  -3.3437343\n",
      "  -2.1637113   4.05894     2.2774343   2.4113836   2.0269382   0.57444334\n",
      "   0.44005972  0.6804029   0.2351875  -0.06831137 -0.38111743 -0.26267403\n",
      "  -1.8908788  -0.5449901   0.6120076  -2.2701707   0.5098928  -2.727987\n",
      "   0.10826822]]...\n",
      "Epoch [150/500], Cumulative Loss: 4874.5308, LR: 5.000e-04\n",
      "Final parameters: [[ 7.9318624  15.142695   16.6883     19.999445   24.06072    23.826332\n",
      "  22.811335   19.178318   18.025686   15.977982   19.899862   17.069614\n",
      "  21.857141   18.29016    23.22965    25.113808   27.146338   29.036013\n",
      "  30.754126   32.401558   33.629387   37.029053   37.024464   36.964725\n",
      "  35.593887   36.145195   32.1019     28.848911   25.640358   24.454527\n",
      "  26.789425   24.126276   25.572662   26.264841   26.386158   27.000975\n",
      "  29.56705    32.964138   30.90525    32.339886   34.939964   36.697\n",
      "  36.843365   37.5187     37.90714    38.471813   33.83083    29.668432\n",
      "  24.407265   18.665247   20.961369   18.546785   20.768133   19.048468\n",
      "  22.730516   26.013948   25.135017   27.46041    28.907656   32.41818\n",
      "  34.82351    38.867554   35.819435   36.938866   37.15644    35.070667\n",
      "  32.711494   28.770071   25.221395   17.936012   20.881123   19.390944\n",
      "  21.49606    20.179094   21.43992    26.466825   25.279854   27.029867\n",
      "  29.753601   30.820642   32.278492   37.775955   36.893402   38.077354\n",
      "  34.480568   33.11709    26.582422   23.900091   20.534647   17.816345\n",
      "  19.433516   20.302103   20.59278    20.186594   21.933918   24.41061\n",
      "  23.355745   26.630419   27.297945   28.734312   29.727789   34.906143\n",
      "  32.85529    36.379993   32.743412   33.005665   29.165876   27.360277\n",
      "  23.051895   20.098076   19.673925   19.755766   22.021185   19.213001\n",
      "  21.226425   27.432545   23.352415   25.120234   26.325584   27.957539\n",
      "  25.216455   23.777676   26.399397   26.379827   27.146244   26.776974\n",
      "  27.967213   29.909122   32.66192    33.964897   35.585316   36.340794\n",
      "  35.636623   37.68917    38.239132   41.00692    39.126522   37.65339\n",
      "  37.9172     37.48796    38.292282   43.029934   39.545956   38.37638\n",
      "  36.682312   35.232605   34.614902   29.070204   25.127874   21.266487\n",
      "  23.964865   22.29842    23.178291   22.953884   22.643753   24.471992\n",
      "  24.338814   27.865843   28.311005   29.277401   30.872915   33.88779\n",
      "  32.38579    32.053505   34.87541    34.1413     31.173557   28.238636\n",
      "  24.288706   19.592058   19.377514   20.604042   20.072062   20.30021\n",
      "  20.977268   25.378975   24.871222   28.738398   29.939249   32.44593\n",
      "  34.8434     37.455547   35.56194    37.762814   36.20108    34.70193\n",
      "  28.94359    29.208963   23.029514   20.54202    20.842224   20.29954\n",
      "  22.144577   20.224302   22.660868   24.532415   23.33906    28.505522\n",
      "  29.664827   29.268963   25.78117    21.414978   18.350042   16.67253\n",
      "  13.518474   10.998561   11.366906   12.005957    9.609081   10.383308\n",
      "   8.936511   11.253133    8.755735   11.027528    8.869721    8.064313\n",
      "   8.738491    8.985429    6.7092834   5.5013995   3.8836558   1.3679771\n",
      "  -1.9463953   0.24290496 -1.2714148  -0.66552305 -1.4572606  -2.730773\n",
      "  -1.8421038  -2.0054426   0.3121939  -1.4356347   0.7894757  -1.4927053\n",
      "   0.22846773  0.43095723 -0.8840419   1.0682216   2.1265008   2.7048929\n",
      "   0.5849806 ]]...\n",
      "Epoch [200/500], Cumulative Loss: 1189.3928, LR: 5.000e-04\n",
      "Final parameters: [[ 8.80194378e+00  1.55038328e+01  1.89080849e+01  1.98047962e+01\n",
      "   1.61793060e+01  1.49523163e+01  1.65592957e+01  1.23612862e+01\n",
      "   1.10310974e+01  9.94803524e+00  1.22501183e+01  1.30768757e+01\n",
      "   1.10498667e+01  1.59493980e+01  1.63317127e+01  1.64188576e+01\n",
      "   1.86457710e+01  2.03655224e+01  2.17794991e+01  2.20561085e+01\n",
      "   2.43857002e+01  2.78811798e+01  3.05273037e+01  2.71144314e+01\n",
      "   2.24129486e+01  2.08700047e+01  2.04715042e+01  1.76423302e+01\n",
      "   1.69197845e+01  1.56533251e+01  1.68063393e+01  1.62728252e+01\n",
      "   1.59857216e+01  1.78519821e+01  1.85254135e+01  1.97481232e+01\n",
      "   2.01447449e+01  2.04626751e+01  2.32412472e+01  2.15749493e+01\n",
      "   2.51604710e+01  3.31654053e+01  3.51092720e+01  3.09817104e+01\n",
      "   2.64032307e+01  2.13977909e+01  1.93457966e+01  1.50705404e+01\n",
      "   1.00786848e+01  6.05651712e+00  8.20293427e+00  1.09160080e+01\n",
      "   7.59678364e+00  1.12239389e+01  1.54789772e+01  1.66026325e+01\n",
      "   2.02946301e+01  2.41434956e+01  2.57910786e+01  2.66929951e+01\n",
      "   3.17802486e+01  3.29888802e+01  3.49806137e+01  3.23351326e+01\n",
      "   2.61938305e+01  2.02860031e+01  1.86218891e+01  1.57302971e+01\n",
      "   1.06557379e+01  6.07387400e+00  7.75370693e+00  8.90871239e+00\n",
      "   1.02677746e+01  1.22729731e+01  1.50812216e+01  1.53397903e+01\n",
      "   2.08825188e+01  2.12495117e+01  2.24628048e+01  2.34049625e+01\n",
      "   2.77417850e+01  3.01686020e+01  2.98872490e+01  2.78589630e+01\n",
      "   2.32109737e+01  2.00414104e+01  1.84296970e+01  1.42194815e+01\n",
      "   1.02986460e+01  9.85151196e+00  1.25449715e+01  1.08931255e+01\n",
      "   1.10231314e+01  1.52430954e+01  1.50371046e+01  1.74676590e+01\n",
      "   1.82777100e+01  1.86677608e+01  1.78838329e+01  1.76686687e+01\n",
      "   2.22348919e+01  2.49903316e+01  2.87680225e+01  2.49249992e+01\n",
      "   2.28990898e+01  1.75122890e+01  1.76018734e+01  1.41540298e+01\n",
      "   1.07842941e+01  1.07390375e+01  1.31264515e+01  1.36784906e+01\n",
      "   1.33733873e+01  1.40022211e+01  1.59134350e+01  1.74059525e+01\n",
      "   1.97535191e+01  1.85357475e+01  1.84698906e+01  1.85691071e+01\n",
      "   1.72517853e+01  1.01414232e+01  1.02657881e+01  1.61699200e+01\n",
      "   1.93919945e+01  2.51290302e+01  2.36806183e+01  2.66795616e+01\n",
      "   3.11091537e+01  3.19933338e+01  3.16237679e+01  3.08346291e+01\n",
      "   3.40536728e+01  3.29550858e+01  3.19374886e+01  2.77430820e+01\n",
      "   2.53867188e+01  2.30499134e+01  2.08416176e+01  1.81251278e+01\n",
      "   2.36603317e+01  2.63035851e+01  2.80159855e+01  2.59072495e+01\n",
      "   2.10057182e+01  1.60192776e+01  1.68317223e+01  1.25094223e+01\n",
      "   1.01306505e+01  9.58575249e+00  1.11447277e+01  1.31748848e+01\n",
      "   1.24799433e+01  1.34361458e+01  1.48204708e+01  1.42239161e+01\n",
      "   1.88129272e+01  2.04137039e+01  1.94814034e+01  2.18438702e+01\n",
      "   2.76566086e+01  2.98774815e+01  2.96115341e+01  2.79808388e+01\n",
      "   2.48396893e+01  1.78049812e+01  1.83116055e+01  1.37756538e+01\n",
      "   7.73808765e+00  8.41192722e+00  1.02907715e+01  5.79667854e+00\n",
      "   6.77513361e+00  1.10072794e+01  1.25736980e+01  1.47539282e+01\n",
      "   1.75600243e+01  2.06300621e+01  2.16897240e+01  2.19032860e+01\n",
      "   2.68106422e+01  3.06649780e+01  2.93285351e+01  2.58001728e+01\n",
      "   2.33011341e+01  2.04865322e+01  1.99770679e+01  1.43869457e+01\n",
      "   1.19250612e+01  1.05629625e+01  1.51367102e+01  1.17120066e+01\n",
      "   1.14861078e+01  1.47544165e+01  1.53582687e+01  1.83235416e+01\n",
      "   2.04643745e+01  2.11324596e+01  2.01135693e+01  2.03332329e+01\n",
      "   1.68844090e+01  1.37899723e+01  8.51465702e+00  7.71284771e+00\n",
      "   5.85949278e+00  1.94172740e+00  1.35520983e+00  1.49709225e+00\n",
      "  -1.36327541e+00 -3.58179259e+00 -3.07741570e+00 -3.70306325e+00\n",
      "  -3.13925338e+00 -2.50475121e+00 -2.00077248e+00  1.30856442e+00\n",
      "   1.76159155e+00  1.67086422e-01  1.14206004e+00  2.78022742e+00\n",
      "   1.83520794e+00 -2.58542225e-02 -4.96800125e-01 -2.21056294e+00\n",
      "  -2.19062471e+00 -2.52796006e+00 -7.33178258e-01 -3.35201740e+00\n",
      "  -2.00164127e+00 -2.29951310e+00  8.82997036e-01 -1.17330790e+00\n",
      "   7.06307888e-01  1.84193301e+00  2.11524081e+00  1.32033777e+00\n",
      "   2.39169931e+00  1.82619596e+00  3.25311518e+00  2.06457829e+00\n",
      "   1.99650753e+00]]...\n",
      "Epoch [250/500], Cumulative Loss: 4585.3115, LR: 5.000e-04\n",
      "Final parameters: [[ 3.3325737  13.529241   22.259851   32.697903   34.366993   34.852844\n",
      "  38.430786   38.452282   38.743187   38.193573   33.87614    29.503548\n",
      "  25.63635    20.30798    12.448802   11.404384    9.8424225   4.7861958\n",
      "   3.7134106  -0.71138823  5.58665    16.648851   27.322802   32.491264\n",
      "  38.54125    38.334743   41.106773   39.65224    38.568306   38.611237\n",
      "  36.488377   30.416525   26.02829    22.830801   18.230894   13.796672\n",
      "  12.088745    7.508513    8.333147    6.625734   13.291206   20.253674\n",
      "  25.635317   31.4171     33.15444    31.11562    31.966642   30.795567\n",
      "  34.683773   34.581104   35.892345   32.78657    29.852566   24.67403\n",
      "  22.037228   18.82779    17.603945   14.555712   10.836479   11.770456\n",
      "  17.901623   22.02585    28.440014   31.336338   30.013807   31.577765\n",
      "  30.297958   32.91334    33.095856   33.394676   36.918644   34.25419\n",
      "  29.261885   29.141243   24.549093   23.043512   18.980246   17.071259\n",
      "  16.164589   14.228034   21.681852   22.251617   25.24038    29.912361\n",
      "  30.465702   28.379547   27.143688   26.52274    27.920862   31.15498\n",
      "  31.554367   27.82716    29.48404    25.569517   24.240023   23.02974\n",
      "  23.118961   20.178038   18.418991   18.621922   24.474941   26.523336\n",
      "  29.723381   30.653255   30.55285    29.138111   28.667316   26.875032\n",
      "  28.742422   30.49814    32.51385    30.68744    31.91164    28.140228\n",
      "  26.87644    24.07904    22.926723   20.507063   19.228426   20.357143\n",
      "  23.4128     25.609245   28.760569   32.43018    34.92209    41.1979\n",
      "  42.97071    42.79772    42.725727   40.107075   37.22371    34.909893\n",
      "  31.284584   31.350836   31.14418    27.657145   27.804276   26.432484\n",
      "  25.777687   24.226482   26.841156   27.189672   29.305607   28.061403\n",
      "  29.442455   26.187357   25.701632   29.47541    29.356136   32.642273\n",
      "  36.929035   36.16345    36.097897   34.83059    33.636364   33.373295\n",
      "  31.707281   29.269001   28.291845   28.364687   32.454605   33.62088\n",
      "  30.80636    31.571714   30.310146   27.740746   27.64487    28.524761\n",
      "  31.883417   32.956154   35.390682   36.819927   35.449688   32.56995\n",
      "  32.34092    29.488937   29.736862   28.496548   27.9314     27.139082\n",
      "  29.640408   31.262997   30.696125   32.02456    29.853683   28.708965\n",
      "  27.196175   28.288614   30.272366   33.717323   34.655125   34.293377\n",
      "  35.06422    33.06833    33.624496   29.901539   27.943968   27.622625\n",
      "  27.557892   26.098413   24.168833   20.240826   13.0224905   9.358497\n",
      "   5.689836    3.7469625   5.0850587   5.210501    7.441678    9.816923\n",
      "   9.581579   15.088517   15.596747   18.617014   18.18313    20.938372\n",
      "  22.33591    22.518034   21.070263   22.261744   16.72969    12.770973\n",
      "   5.6717925   1.074253    0.57244384  0.250722   -0.25463897 -0.6440135\n",
      "  -1.643827   -1.5654593  -0.7611938  -0.4636739  -2.5172572  -1.9909395\n",
      "  -3.761807   -3.0574887  -5.756518   -6.499118   -5.395789   -5.9018645\n",
      "  -5.3294697 ]]...\n",
      "Epoch [300/500], Cumulative Loss: 8153.2290, LR: 5.000e-04\n",
      "Final parameters: [[ 4.757904  15.966163  21.760374  28.634445  30.419548  33.854794\n",
      "  36.15999   34.098316  33.89169   29.385565  26.921305  21.440914\n",
      "  19.425064  15.7917385 12.068749  11.602304   9.185925   5.151523\n",
      "   5.0019665  3.51561   11.6514435 19.753063  25.731766  31.141638\n",
      "  33.60877   35.719265  38.44693   36.329193  34.474247  32.16855\n",
      "  26.22931   25.934498  21.870123  22.459553  19.575016  15.392312\n",
      "  14.732401  13.256867  12.244358  12.980331  19.031443  22.914356\n",
      "  27.507189  28.680346  26.06368   27.705     30.130724  32.170116\n",
      "  32.69792   32.51443   30.234035  26.050148  24.837852  25.71703\n",
      "  22.584866  23.450947  20.231194  20.388687  20.161781  22.224209\n",
      "  24.56678   26.914373  27.384342  28.009453  29.170887  27.101795\n",
      "  31.598307  33.345497  34.415104  31.248463  30.865118  28.401266\n",
      "  27.12543   26.670986  26.849432  28.582138  25.898157  25.236198\n",
      "  26.08644   24.262085  27.331718  27.896784  28.67672   27.124308\n",
      "  25.768372  24.940655  28.179123  31.012161  33.141544  33.643566\n",
      "  33.468063  31.345078  30.322355  30.280725  30.702604  32.360756\n",
      "  29.619553  30.695461  30.441742  29.796299  30.72697   31.841484\n",
      "  31.621717  29.643452  26.434505  25.632917  28.669413  32.14393\n",
      "  32.70397   33.907223  32.7751    31.180199  32.84513   32.966217\n",
      "  33.934067  31.947254  31.01483   29.314041  30.023558  27.483957\n",
      "  30.21348   30.701881  34.079174  36.132347  38.45295   41.904694\n",
      "  41.853905  39.95444   38.01442   36.56339   35.60601   35.963707\n",
      "  35.81759   35.40665   33.70405   31.704039  32.128704  30.591908\n",
      "  30.967827  31.927444  30.19158   29.620375  32.97206   29.805882\n",
      "  27.345997  28.458265  30.525042  34.78279   37.7471    35.57791\n",
      "  34.242584  33.809483  35.11084   32.407284  34.475418  32.2496\n",
      "  33.474716  30.183895  29.520706  31.289755  32.293373  30.777239\n",
      "  32.744427  31.294796  25.465834  26.80942   28.643406  31.486197\n",
      "  35.276733  34.82608   35.777065  32.03929   34.52032   32.757885\n",
      "  32.67064   32.505768  31.23947   29.703596  29.411682  31.411404\n",
      "  31.669252  32.71737   30.93951   29.899704  27.664778  25.643192\n",
      "  29.770382  33.38708   36.0015    36.23018   34.319233  33.454086\n",
      "  32.80234   33.462296  32.73409   31.242907  30.210323  31.265434\n",
      "  34.397484  30.490013  26.100824  21.237196  15.043743   8.180734\n",
      "   6.226267   6.7697434  6.822404  11.7636595 13.9879675 16.606754\n",
      "  17.751503  16.657797  18.892923  23.08111   22.605326  24.512629\n",
      "  25.947184  26.71679   25.732666  25.927797  20.722214  16.397955\n",
      "   7.610295   5.5829587  3.0255556 -0.6678755 -2.0056272 -3.39021\n",
      "  -1.4833105 -3.5671437 -2.534144  -2.053955  -3.978873  -4.148085\n",
      "  -4.5022774 -4.759613  -4.0530515 -5.243623  -5.052295  -4.0211434\n",
      "  -3.2127483]]...\n",
      "Epoch [350/500], Cumulative Loss: 8285.5176, LR: 5.000e-04\n",
      "Final parameters: [[ 4.315916   17.03113    25.73872    29.451464   34.210167   39.106716\n",
      "  38.48494    39.20411    34.298573   35.335884   35.20188    31.573193\n",
      "  28.945627   27.028723   19.245823   13.869026    9.274435    4.674803\n",
      "   3.038792    0.1418797   7.670645   18.507847   28.88829    32.68828\n",
      "  36.404026   39.38018    40.66064    44.028484   38.163      37.823265\n",
      "  37.940674   34.26144    28.447514   27.153246   22.573116   16.414875\n",
      "  12.761724   11.181661    7.95452     5.276576   11.582314   20.506021\n",
      "  27.521538   29.933424   32.650738   35.33356    35.56188    35.284664\n",
      "  34.794693   34.4258     34.251877   32.36824    30.343792   27.592352\n",
      "  24.010262   19.754345   18.659725   13.720401   11.51393    11.377174\n",
      "  16.879095   23.343819   27.448984   29.611351   30.29881    34.812763\n",
      "  34.167587   34.803207   35.59418    35.03393    35.70927    32.973576\n",
      "  30.136766   30.098503   26.060534   23.552538   18.93494    18.308365\n",
      "  15.191755   15.900295   19.374361   24.775646   25.936516   27.057016\n",
      "  28.386862   27.858992   29.200476   33.088608   32.585957   33.053288\n",
      "  33.302032   33.035923   30.366446   31.02177    27.362988   25.626635\n",
      "  25.495771   23.851198   22.899837   21.913761   23.501373   28.185999\n",
      "  30.676418   29.550486   29.743557   30.468166   30.222603   33.781555\n",
      "  34.862507   32.73753    32.631462   32.759544   30.247555   28.976612\n",
      "  29.185774   26.538485   23.803991   22.149357   24.631023   23.056164\n",
      "  23.728334   27.158665   31.487976   37.02447    41.8334     47.496773\n",
      "  46.867786   47.726482   46.67925    45.48361    41.29536    37.478783\n",
      "  35.50757    32.058037   31.731058   31.240376   27.350536   27.717798\n",
      "  27.363329   25.052525   27.09998    29.292717   30.018053   28.748892\n",
      "  29.616632   28.41103    29.041914   32.257244   33.511215   32.496048\n",
      "  35.922916   35.622215   35.307533   34.213478   32.56602    32.477654\n",
      "  30.265282   28.887789   27.498844   26.960552   29.325327   30.1657\n",
      "  30.507902   27.28972    30.604937   29.991304   29.576029   33.493187\n",
      "  32.833614   33.44947    35.54635    36.33467    33.999363   33.68525\n",
      "  32.79009    31.183481   28.71919    31.12109    28.349367   27.462725\n",
      "  30.196587   30.89739    30.19227    29.47404    29.098692   28.798964\n",
      "  25.984669   30.685339   32.463177   35.223232   35.78881    38.465076\n",
      "  36.6425     36.927567   32.83214    33.378708   30.38138    28.60763\n",
      "  29.42907    27.850422   23.307053   18.044847   12.713191    6.5762877\n",
      "   8.702993    9.847111    7.214141   10.585409   14.334891   13.361022\n",
      "  12.86914    16.485033   15.473354   17.9406     19.552462   21.507662\n",
      "  21.852673   23.814882   23.13193    24.849632   19.785969   13.346877\n",
      "   6.8090563   4.423055    0.18477468  0.14209636  0.5549228  -1.7307414\n",
      "  -2.4951735  -0.46594694 -1.0715526  -4.194852   -1.241487   -2.273223\n",
      "  -4.225445   -4.7707415  -2.9031665  -4.2135897  -5.6771355  -5.4907455\n",
      "  -5.1574616 ]]...\n",
      "Epoch [400/500], Cumulative Loss: 6753.9131, LR: 5.000e-04\n",
      "Final parameters: [[ 5.358391   13.54947    24.822117   30.16289    34.14662    35.66754\n",
      "  35.062515   35.217323   32.94231    31.619078   28.79914    26.947422\n",
      "  21.888313   17.37377    13.13036     9.42665     8.804527    4.059094\n",
      "   4.56417     4.105351   12.900775   21.093182   29.485098   35.17752\n",
      "  37.424934   37.317043   40.070675   36.3986     36.620125   33.90968\n",
      "  34.82195    29.939302   27.22406    21.49293    20.37599    17.07302\n",
      "  16.83495    15.278113   13.248802   14.552277   19.816626   23.853415\n",
      "  26.859165   29.259953   33.45015    34.67722    37.29439    35.25021\n",
      "  35.461662   35.920795   33.896767   32.45602    28.039206   27.711523\n",
      "  25.866508   23.741268   20.164719   21.210123   18.340712   19.701574\n",
      "  24.124666   26.599768   29.560568   32.38848    32.269176   31.841099\n",
      "  36.080116   37.678207   35.546814   33.43142    35.80841    33.661953\n",
      "  31.749979   28.212122   27.350868   24.600117   24.425238   21.791399\n",
      "  24.140131   23.065575   26.368305   28.05858    27.977268   28.585358\n",
      "  31.025433   31.03033    34.148933   35.996994   33.57767    33.875652\n",
      "  36.017464   34.296894   32.48923    32.515736   29.963188   28.912859\n",
      "  27.13184    27.225443   24.63552    25.754555   28.19571    28.845087\n",
      "  31.4777     30.567636   31.27349    30.629732   34.06166    35.842308\n",
      "  32.94872    34.68079    37.521988   37.00229    35.635162   31.267115\n",
      "  28.848026   26.743446   28.052725   24.599297   26.383226   24.054562\n",
      "  28.170223   29.877531   31.717306   34.747955   38.1835     40.299614\n",
      "  42.012146   40.834263   39.957813   40.5088     38.619564   37.291805\n",
      "  33.396202   32.05943    30.781916   30.82858    31.258291   30.01105\n",
      "  27.233158   31.392904   31.201876   29.727724   29.904812   30.331343\n",
      "  30.937155   30.28282    32.724037   35.944664   36.52297    36.946484\n",
      "  39.21648    39.037033   36.449497   34.09751    32.074806   29.741594\n",
      "  31.660528   30.108257   27.349932   26.74681    30.643986   31.720322\n",
      "  30.16147    29.194998   29.681635   31.03241    32.44903    35.963127\n",
      "  35.28692    34.858803   36.146202   34.56428    33.246918   33.43514\n",
      "  32.012844   30.080519   30.83088    29.761429   26.493214   28.048788\n",
      "  31.551641   33.041897   29.365227   30.670265   30.54096    32.75926\n",
      "  34.00279    34.503036   36.96683    37.20719    37.851578   36.185474\n",
      "  34.710102   33.124767   32.887844   31.2528     30.656218   29.539143\n",
      "  28.338667   26.893654   26.94336    20.647976   13.5047655   9.943399\n",
      "  10.796317    9.157879   11.587614   12.710452   13.015538   14.556764\n",
      "  15.903897   17.839487   21.012009   22.761744   22.37873    23.56157\n",
      "  26.818718   24.397896   26.28439    23.646774   19.435024   11.397268\n",
      "   9.753206    3.5864573  -0.04598263  0.7251962  -1.3470665  -1.4774722\n",
      "  -0.87710816 -1.9095075  -2.2776     -3.9333484  -4.1793103  -3.6534927\n",
      "  -3.6304057  -3.7461212  -3.1822789  -5.669785   -4.911386   -2.9898047\n",
      "  -2.4593453 ]]...\n",
      "Epoch [450/500], Cumulative Loss: 8166.0005, LR: 5.000e-04\n",
      "Final parameters: [[ 4.08823    14.077832   22.362597   28.438858   33.791283   33.888287\n",
      "  37.041252   33.993767   28.646282   26.469387   23.390568   18.153067\n",
      "  20.618355   16.80116    13.373875   13.892566    7.621103    9.606174\n",
      "   4.8552523   4.0660815  11.167086   20.946234   25.77522    33.927532\n",
      "  37.93223    36.913548   39.799255   37.266693   33.439487   31.868313\n",
      "  29.038187   23.929583   24.68063    23.864433   20.16178    20.06077\n",
      "  14.63659    14.572272   13.186988   11.882275   16.769356   23.055748\n",
      "  25.945251   30.833593   32.6389     32.13539    34.837486   33.928837\n",
      "  31.01714    29.589231   30.072165   28.532854   28.744282   27.549795\n",
      "  26.041433   23.871576   19.89747    19.141447   18.053122   18.545984\n",
      "  22.186064   25.244469   27.537144   29.289082   31.626245   35.042023\n",
      "  33.88432    32.117664   30.437508   31.278214   32.246452   30.277012\n",
      "  29.456007   27.036806   27.805635   28.282486   23.677294   24.747398\n",
      "  24.285122   23.127123   25.293974   27.184683   27.84059    27.599844\n",
      "  29.720282   31.233932   31.591814   34.191242   32.305454   32.619823\n",
      "  33.609863   31.03023    30.8515     31.693169   29.207293   29.747828\n",
      "  26.882544   26.520054   25.94426    24.79702    27.46733    27.752449\n",
      "  28.739065   28.684586   29.49692    31.202562   29.941349   33.585243\n",
      "  32.79623    31.773108   32.905922   32.61748    32.502007   31.978762\n",
      "  31.993002   30.576582   26.872526   27.838337   25.215048   25.704721\n",
      "  25.90804    26.378487   31.03572    32.043625   34.049564   34.360893\n",
      "  37.199123   38.946777   36.513145   36.988403   36.75468    32.88832\n",
      "  34.744873   30.946995   32.325184   30.604246   29.139282   29.643042\n",
      "  26.959978   27.044382   26.751488   26.592804   28.916069   26.820847\n",
      "  29.396122   30.880556   31.830423   35.23254    33.019268   32.781975\n",
      "  32.981865   30.942604   32.952435   34.352528   30.17075    31.266796\n",
      "  29.333801   29.013111   28.439646   27.22489    29.52953    29.423391\n",
      "  30.12576    29.789568   32.84171    31.493086   32.18112    33.002003\n",
      "  32.28939    33.067753   34.17165    32.257668   31.569355   30.88255\n",
      "  32.037674   30.549084   31.015934   30.040636   30.752817   27.772251\n",
      "  29.97515    29.091297   29.94163    29.391829   29.628304   31.212994\n",
      "  32.260326   32.164833   33.45789    33.085583   34.824352   34.34018\n",
      "  35.690685   34.374355   33.16456    32.253662   32.367893   29.535654\n",
      "  32.781757   28.201544   25.505583   19.616783   13.339261   11.16354\n",
      "   8.160138   10.72274     9.623618   12.875387   17.42267    15.841948\n",
      "  16.06851    19.009842   20.87378    20.316374   24.388939   23.701609\n",
      "  25.659723   25.339828   24.694181   24.466286   21.17991    14.268064\n",
      "   9.559034    2.4156725   1.5547025  -1.7092003   0.08571984 -1.9572256\n",
      "  -3.3842664  -0.9690856  -2.0229714  -3.5246155  -1.2638218  -3.448875\n",
      "  -2.9816387  -3.075591   -6.031451   -4.197795   -4.8396115  -6.2332606\n",
      "  -3.4519467 ]]...\n",
      "Epoch [500/500], Cumulative Loss: 4422.8467, LR: 5.000e-04\n",
      "Final parameters: [[ 9.104213   16.30794    25.998821   25.675209   21.792053   23.776205\n",
      "  21.868341   16.59625    14.251657   10.25654     8.104223    5.7530394\n",
      "   1.5704713   5.3836236  11.059463   12.8769045  17.877596   22.193636\n",
      "  26.622204   29.962149   29.90742    32.83366    33.04961    32.914413\n",
      "  30.10483    26.701885   30.342957   27.77363    25.408642   25.590368\n",
      "  25.824594   21.017239   18.717596   18.740705   21.190233   22.016014\n",
      "  23.006678   23.915117   22.696003   23.840878   27.001331   30.335249\n",
      "  34.484245   33.344093   29.716183   28.407473   28.931189   24.880503\n",
      "  23.93033    21.394863   18.747576   17.325148   15.876845   14.774263\n",
      "  19.894663   18.99384    18.43017    22.892252   23.263386   23.386095\n",
      "  26.383476   32.372986   32.1293     33.043156   27.277466   28.897505\n",
      "  26.32797    23.114342   21.784727   20.779144   16.375732   14.561599\n",
      "  10.308982   10.547975   14.52376    16.63612    18.631424   20.996025\n",
      "  20.625172   23.138344   26.21975    26.725632   28.49241    29.32947\n",
      "  27.238417   23.765017   23.050518   19.010433   19.33324    18.312735\n",
      "  14.966033   10.491954    7.1005564   7.418818   10.988991   14.837372\n",
      "  15.859906   19.867859   21.277225   20.419426   24.522556   26.498661\n",
      "  24.743784   28.049988   26.036945   24.473911   23.832293   21.583221\n",
      "  23.235518   22.781141   20.850647   15.9476     13.304236   11.795074\n",
      "  14.955801   17.753819   20.596498   17.816784   18.61477    20.45209\n",
      "  21.303246   24.955103   29.139065   29.443447   29.753702   30.931868\n",
      "  31.723333   28.576666   28.482544   28.268942   27.487467   27.21335\n",
      "  27.107569   27.963978   32.664642   31.243916   36.262264   40.85567\n",
      "  42.365093   45.077026   45.4075     40.71179    38.98503    36.773502\n",
      "  34.30747    30.664827   31.499687   29.391134   29.4503     28.848911\n",
      "  27.0153     25.140762   23.188133   22.734503   20.352074   25.115618\n",
      "  22.746502   23.307648   22.488367   21.287983   24.118034   25.627707\n",
      "  28.992542   26.867842   26.74558    27.399372   25.78478    22.745552\n",
      "  22.807299   23.032616   20.099827   16.718203   14.973228   14.662344\n",
      "  16.681269   17.377884   20.970293   20.524555   22.06875    22.368505\n",
      "  25.009062   27.423452   28.996407   30.725933   27.716557   27.720093\n",
      "  24.517685   23.793814   23.157692   20.609756   15.861927   15.043132\n",
      "  11.308667   11.178361   14.787568   15.214823   19.364914   19.51141\n",
      "  21.490791   22.203732   18.535175   16.718637   12.643805   14.103706\n",
      "  10.956157    9.871289   11.615389    9.985587   11.118107   12.8988905\n",
      "  12.829007   12.209333    8.220289    8.950338    7.425518   11.514485\n",
      "   8.750061    5.0727315   4.934095    4.720228    3.6580107   2.146975\n",
      "   1.0456285  -1.6896601  -0.39438048 -0.19110136 -1.7737724  -0.946012\n",
      "  -2.1153276  -1.6489402  -1.4640236  -0.9839116  -0.85493183 -0.771221\n",
      "   0.51522416 -0.9188862  -0.0587994   1.3389122   0.44473034  0.53287566\n",
      "   0.11524734]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 39.44459915161133\n",
      "Final parameters: [[ 1.49608517e+01  3.25690117e+01  3.55990791e+01  3.11290970e+01\n",
      "   2.06588402e+01  1.11690722e+01  3.24444675e+00 -1.95119619e+00\n",
      "   7.37841797e+00  2.21419830e+01  3.78035126e+01  4.13834419e+01\n",
      "   3.81730499e+01  4.51739922e+01  4.82061958e+01  4.16736641e+01\n",
      "   2.91560936e+01  1.80914631e+01  3.29428406e+01  4.41580849e+01\n",
      "   5.50817795e+01  6.35485420e+01  6.55569305e+01  5.79170685e+01\n",
      "   4.67578773e+01  3.89902916e+01  3.06979713e+01  2.48726330e+01\n",
      "   2.90699387e+01  3.40184555e+01  4.49551926e+01  4.59022141e+01\n",
      "   4.67019730e+01  5.14954529e+01  5.88882637e+01  5.61383705e+01\n",
      "   4.56126022e+01  4.06343002e+01  4.82518196e+01  5.24275703e+01\n",
      "   6.08257217e+01  6.40441818e+01  6.15914459e+01  5.54653893e+01\n",
      "   4.80757065e+01  3.77197876e+01  3.33246269e+01  2.73997612e+01\n",
      "   3.13696404e+01  3.56173668e+01  4.28971176e+01  4.51280746e+01\n",
      "   4.67977409e+01  5.10386353e+01  5.52188683e+01  5.22427559e+01\n",
      "   4.59170837e+01  3.86677399e+01  4.66064491e+01  4.87688560e+01\n",
      "   5.34016609e+01  6.34439087e+01  6.30977592e+01  5.40656395e+01\n",
      "   4.30045967e+01  3.43644409e+01  2.72943192e+01  2.25481148e+01\n",
      "   2.50358372e+01  3.15167465e+01  4.11678543e+01  4.49139786e+01\n",
      "   4.28869324e+01  5.05136909e+01  5.43937073e+01  4.81364746e+01\n",
      "   4.09154587e+01  3.40924034e+01  4.32415199e+01  4.81120148e+01\n",
      "   5.54057693e+01  6.35189934e+01  6.21385727e+01  5.05568275e+01\n",
      "   4.09768677e+01  3.04618893e+01  2.21339874e+01  1.74290447e+01\n",
      "   1.84831371e+01  2.59888458e+01  3.63401108e+01  4.01914253e+01\n",
      "   4.10276527e+01  4.53605995e+01  5.05555191e+01  4.66641922e+01\n",
      "   3.72875977e+01  2.83868389e+01  4.06073799e+01  4.44221992e+01\n",
      "   5.11027336e+01  5.78812599e+01  5.67798843e+01  5.14154778e+01\n",
      "   4.18308220e+01  3.62826233e+01  2.87471161e+01  2.53562546e+01\n",
      "   2.66827126e+01  3.03538418e+01  3.73467941e+01  4.02083588e+01\n",
      "   4.21797523e+01  4.77416954e+01  5.24706230e+01  5.01849442e+01\n",
      "   4.25236282e+01  3.80600624e+01  4.60907898e+01  4.58803024e+01\n",
      "   5.23541985e+01  6.18358803e+01  6.23881683e+01  5.87635612e+01\n",
      "   5.02636108e+01  4.38906860e+01  3.75984535e+01  3.25540390e+01\n",
      "   4.08893852e+01  5.16809349e+01  6.44573669e+01  6.20762367e+01\n",
      "   5.26599197e+01  5.84929466e+01  6.65922546e+01  6.32885590e+01\n",
      "   5.51441460e+01  4.69983253e+01  5.34958878e+01  7.04421616e+01\n",
      "   7.43652802e+01  7.04098892e+01  6.93056870e+01  6.24700661e+01\n",
      "   5.77252121e+01  4.93599243e+01  4.47131920e+01  4.10285873e+01\n",
      "   3.95043640e+01  4.08704071e+01  4.15205383e+01  4.70259857e+01\n",
      "   4.81182404e+01  5.08492928e+01  5.44629021e+01  5.32711143e+01\n",
      "   4.95564003e+01  4.57623787e+01  4.99333305e+01  5.02014313e+01\n",
      "   5.46840248e+01  5.83955994e+01  5.79184380e+01  5.12230377e+01\n",
      "   4.40611191e+01  3.49857140e+01  2.88008842e+01  2.79805889e+01\n",
      "   2.89997158e+01  3.29454803e+01  4.07871132e+01  4.37053032e+01\n",
      "   4.47655487e+01  4.88605652e+01  5.35061798e+01  5.10122375e+01\n",
      "   4.15984955e+01  3.88055725e+01  4.35346107e+01  4.83080139e+01\n",
      "   5.44467201e+01  6.02749939e+01  5.98128204e+01  5.17786713e+01\n",
      "   4.29240646e+01  3.39222260e+01  2.70727558e+01  2.18133450e+01\n",
      "   2.64114418e+01  2.86499081e+01  3.70527000e+01  4.17568512e+01\n",
      "   4.28916168e+01  4.78864937e+01  5.04996338e+01  4.98489609e+01\n",
      "   4.32034264e+01  3.42489166e+01  4.52252083e+01  4.73556023e+01\n",
      "   4.03559837e+01  3.16064796e+01  2.85842495e+01  2.60315342e+01\n",
      "   2.62366390e+01  2.11317387e+01  2.25516319e+01  2.13580418e+01\n",
      "   1.75539398e+01  1.38864613e+01  1.30633745e+01  1.40029535e+01\n",
      "   1.39727488e+01  1.41826935e+01  1.42484503e+01  2.01632977e+01\n",
      "   1.96257305e+01  1.99895191e+01  2.08229752e+01  1.25926600e+01\n",
      "   7.72843075e+00  4.14960146e+00 -5.90886295e-01 -1.38998151e+00\n",
      "  -4.40120220e+00 -2.05282068e+00 -3.03401542e+00 -4.09852362e+00\n",
      "  -1.19447219e+00 -6.60118937e-01 -1.61890835e-02 -9.36268806e-01\n",
      "  -1.41734993e+00  1.65236759e+00  7.44747221e-01 -3.40747327e-01\n",
      "  -1.80130851e+00 -7.27224946e-01  4.46582615e-01  2.21604615e-01\n",
      "  -5.56563795e-01]]...\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.0005)\n",
    "initializer = Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "writer = SummaryWriter(\"diab_train/lr5e-4_MSE_2_layers\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=500, discount=0.9, writer=writer)\n",
    "writer = SummaryWriter(\"diab_test/lr5e-4_MSE_2_layers\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7ba6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c243cc9e6294a7284a23ae4f5de79d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], Cumulative Loss: 622416.6250, LR: 1.000e-02\n",
      "Final parameters: [[-258.9308  -255.476   -250.7169  -247.74266 -244.85626 -242.66255\n",
      "  -242.75192 -239.99274 -241.9641  -241.69618 -240.84    -242.4618\n",
      "  -240.18727 -240.99045 -240.35583 -239.26526 -239.0608  -240.35892\n",
      "  -241.9938  -243.39935 -240.61446 -240.34555 -241.08736 -240.9596\n",
      "  -240.42476 -241.6978  -241.83907 -240.49252 -240.93233 -241.28664\n",
      "  -240.46823 -240.14223 -239.9669  -241.83862 -240.32803 -241.61313\n",
      "  -240.25885 -241.3645  -239.669   -241.53156 -241.24248 -241.8147\n",
      "  -239.38412 -241.32516 -242.10023 -239.6766  -241.76685 -240.849\n",
      "  -241.12642 -239.94559 -240.742   -243.29272 -242.24384 -239.00012\n",
      "  -239.50458 -240.4185  -239.21141 -240.53125 -240.85172 -240.31442\n",
      "  -241.5686  -241.8736  -240.39442 -241.11095 -241.54852 -241.6148\n",
      "  -239.00084 -239.41054 -240.25255 -240.20898 -240.6845  -238.85994\n",
      "  -240.31863 -240.55476 -242.15332 -242.5638  -241.43033 -241.88904\n",
      "  -240.64154 -241.92067 -242.55489 -241.71481 -241.29636 -240.50475\n",
      "  -240.52893 -239.88293 -239.63554 -239.5883  -241.54097 -240.93767\n",
      "  -241.79141 -241.63504 -240.7961  -240.5313  -240.27502 -241.78879\n",
      "  -239.85802 -241.39616 -240.52951 -240.0436  -241.40707 -241.41856\n",
      "  -241.89728 -240.16013 -240.15984 -241.36142 -241.62608 -241.05139\n",
      "  -240.81085 -239.5118  -241.50354 -239.0001  -240.93831 -242.2069\n",
      "  -242.78618 -240.80844 -240.38245 -241.96707 -239.47441 -242.37135\n",
      "  -243.26714 -240.16342 -240.1817  -242.26694 -241.05907 -241.203\n",
      "  -240.6774  -239.98743 -240.46913 -240.11945 -241.07944 -239.97946\n",
      "  -240.73396 -242.09218 -240.17995 -241.58261 -241.6725  -242.43436\n",
      "  -239.86383 -241.37778 -239.79362 -241.84407 -240.68274 -240.62775\n",
      "  -240.39015 -240.33456 -243.85455 -239.50587 -239.6795  -241.39961\n",
      "  -241.83157 -240.33992 -241.69705 -241.47958 -241.78665 -241.81094\n",
      "  -240.04659 -239.52138 -239.40527 -239.88795 -239.62274 -241.41258\n",
      "  -240.48033 -241.1455  -240.90419 -241.32117 -238.79301 -241.57246\n",
      "  -239.4466  -242.41852 -239.63289 -240.33125 -238.8339  -239.73178\n",
      "  -242.23004 -241.67072 -240.87833 -242.44566 -240.03346 -240.66315\n",
      "  -241.66563 -238.694   -240.86072 -239.83595 -240.42879 -240.844\n",
      "  -240.78064 -242.1068  -238.80066 -240.5254  -239.41902 -242.7255\n",
      "  -240.78214 -240.71785 -239.40833 -240.38618 -241.34073 -241.3377\n",
      "  -240.41702 -240.04663 -242.92651 -239.47241 -241.94916 -240.16306\n",
      "  -241.83368 -240.19812 -242.1677  -240.39119 -240.85616 -241.67479\n",
      "  -240.86487 -240.74553 -240.5178  -239.47388 -241.99675 -240.47597\n",
      "  -240.17319 -239.8478  -240.40762 -240.77588 -238.95042 -241.47104\n",
      "  -238.63358 -241.16917 -241.13084 -241.40031 -239.07323 -242.13339\n",
      "  -242.65475 -242.07205 -240.42833 -240.80872 -239.26245 -240.36586\n",
      "  -240.29202 -241.27933 -240.72607 -240.10487 -239.49686 -241.61923\n",
      "  -254.55063]]...\n",
      "Epoch [100/500], Cumulative Loss: 621089.0000, LR: 1.000e-02\n",
      "Final parameters: [[-256.53516 -254.5024  -251.50275 -247.47276 -245.81104 -243.89246\n",
      "  -241.82927 -239.98174 -241.26251 -238.67574 -240.79639 -239.78883\n",
      "  -239.331   -239.15334 -241.19432 -239.15987 -238.37445 -240.62209\n",
      "  -241.71027 -239.55214 -241.66252 -240.35634 -241.02165 -239.01291\n",
      "  -242.20502 -239.29216 -239.41394 -240.64987 -241.15704 -238.02785\n",
      "  -239.87573 -238.95251 -238.25725 -240.13045 -238.38577 -239.93442\n",
      "  -240.26364 -240.89662 -238.64659 -240.40756 -239.08684 -240.61723\n",
      "  -239.64194 -241.47882 -240.50238 -238.4092  -239.34201 -241.74966\n",
      "  -238.4514  -239.12096 -240.71178 -238.5598  -239.827   -239.87848\n",
      "  -239.53073 -241.14737 -239.6153  -239.94788 -238.89467 -239.13193\n",
      "  -239.96558 -238.87946 -240.9523  -239.74385 -238.28445 -239.8471\n",
      "  -238.04448 -238.55875 -239.85492 -241.69662 -239.82713 -238.82515\n",
      "  -242.44804 -239.77034 -240.43307 -240.49753 -239.1917  -239.8121\n",
      "  -240.56848 -238.23376 -241.02177 -239.07762 -240.23044 -240.62546\n",
      "  -240.75302 -237.80057 -240.20386 -240.50337 -241.90158 -240.09831\n",
      "  -238.67809 -240.19363 -241.45065 -237.84076 -239.41562 -239.79424\n",
      "  -237.03189 -242.55254 -240.27867 -239.86221 -240.7299  -240.39795\n",
      "  -240.27512 -240.33412 -239.37526 -240.42328 -241.18642 -239.77646\n",
      "  -239.57204 -241.67625 -240.37935 -240.72264 -241.25699 -240.74966\n",
      "  -241.58168 -239.93762 -240.82367 -239.53137 -239.46312 -239.83205\n",
      "  -241.29932 -238.77228 -240.03569 -240.65707 -239.73659 -239.5085\n",
      "  -240.66975 -240.99687 -239.73247 -238.82974 -242.11151 -240.37794\n",
      "  -241.41418 -239.66017 -239.75795 -240.14883 -241.69427 -241.69061\n",
      "  -240.5356  -239.76546 -240.1224  -239.65361 -239.72127 -239.7262\n",
      "  -240.04211 -239.25224 -239.93251 -242.36351 -240.76985 -241.40953\n",
      "  -239.92026 -239.07825 -240.04288 -240.84435 -240.47609 -238.85074\n",
      "  -239.07352 -237.67775 -237.96454 -241.55075 -239.1118  -239.42494\n",
      "  -241.0512  -239.28983 -239.60535 -240.08719 -240.88242 -239.50829\n",
      "  -242.60347 -240.05727 -239.97891 -240.69684 -242.33934 -239.24051\n",
      "  -239.53523 -240.39908 -239.44475 -241.55043 -240.96236 -237.4298\n",
      "  -238.5022  -238.79585 -241.0465  -238.54169 -239.54169 -240.26788\n",
      "  -242.23712 -241.04005 -239.80753 -241.29195 -239.57642 -238.94214\n",
      "  -237.97423 -240.95087 -242.02731 -239.04276 -240.3079  -239.05182\n",
      "  -239.27972 -237.87213 -240.66243 -239.53197 -239.73697 -239.48355\n",
      "  -238.66757 -239.88396 -239.86688 -241.89343 -240.07408 -241.42119\n",
      "  -240.9002  -240.75919 -241.66942 -240.08705 -240.36925 -240.1002\n",
      "  -241.05188 -240.80806 -239.96834 -241.75822 -241.15833 -238.99704\n",
      "  -240.2098  -241.69066 -239.11794 -240.90318 -240.19675 -242.39595\n",
      "  -239.8303  -240.24112 -237.33658 -240.00327 -241.87463 -240.92776\n",
      "  -240.52684 -241.29817 -240.85793 -239.08627 -240.73628 -240.21735\n",
      "  -254.2462 ]]...\n",
      "Epoch [150/500], Cumulative Loss: 604006.0000, LR: 1.000e-03\n",
      "Final parameters: [[-253.74669 -249.9819  -244.17249 -243.66641 -239.76772 -239.47717\n",
      "  -235.77283 -237.09283 -237.9256  -234.3267  -235.20682 -236.16373\n",
      "  -235.61554 -234.68333 -234.83977 -235.83823 -234.85469 -234.49832\n",
      "  -235.83553 -235.23813 -233.30795 -235.4366  -234.96024 -236.17911\n",
      "  -235.14456 -233.98093 -234.40907 -235.40186 -234.13191 -234.81448\n",
      "  -234.60193 -234.60213 -236.20186 -233.8671  -237.04631 -235.36122\n",
      "  -235.93187 -234.56673 -233.9981  -236.61534 -235.6129  -236.439\n",
      "  -235.4095  -234.48013 -234.90288 -234.89664 -236.8815  -234.49324\n",
      "  -235.93079 -235.04573 -235.04778 -236.43347 -236.02194 -236.0513\n",
      "  -236.45036 -235.09052 -236.4522  -236.28625 -234.27583 -235.49419\n",
      "  -235.93333 -237.20221 -233.84422 -234.87363 -235.70963 -235.32994\n",
      "  -234.57901 -236.95064 -235.03384 -235.3259  -235.43065 -235.13597\n",
      "  -234.70325 -234.29395 -234.36217 -235.15735 -236.02722 -233.60999\n",
      "  -235.43588 -234.0399  -236.38284 -236.03122 -233.60706 -236.58594\n",
      "  -236.55301 -232.6106  -235.23512 -234.80199 -235.54227 -234.61061\n",
      "  -234.38568 -236.94478 -236.25139 -233.6232  -235.38641 -236.20296\n",
      "  -234.98573 -234.06258 -237.16103 -234.11935 -235.68677 -235.0094\n",
      "  -235.39796 -237.46677 -234.45409 -235.8764  -235.81615 -235.0565\n",
      "  -236.412   -234.99321 -236.3328  -236.51509 -235.6328  -234.12549\n",
      "  -237.25198 -235.08194 -235.68744 -235.56902 -235.32777 -234.04073\n",
      "  -235.42099 -235.02383 -236.14877 -235.08583 -234.30785 -235.4038\n",
      "  -235.50699 -235.05933 -235.60165 -235.34225 -235.59615 -234.87878\n",
      "  -234.55208 -233.54852 -234.69946 -235.978   -236.26845 -234.42493\n",
      "  -235.34041 -234.80006 -235.95277 -236.79529 -235.43556 -237.31956\n",
      "  -236.44261 -235.47769 -235.9886  -234.35152 -234.62935 -234.00504\n",
      "  -235.95348 -234.87105 -236.46495 -234.60693 -235.50854 -236.13246\n",
      "  -233.63606 -235.2587  -234.6302  -236.24419 -236.147   -234.37677\n",
      "  -236.43552 -235.69768 -236.3986  -236.093   -234.45634 -235.38884\n",
      "  -235.36882 -234.34296 -235.23581 -236.06113 -236.47346 -236.03392\n",
      "  -236.13219 -236.80528 -234.09052 -235.69006 -235.39429 -234.438\n",
      "  -235.31163 -234.2427  -234.5408  -235.45767 -234.77    -236.54872\n",
      "  -236.191   -236.02289 -234.9002  -234.36874 -235.37535 -235.49496\n",
      "  -235.8121  -233.93549 -234.2455  -234.55548 -235.27965 -235.7841\n",
      "  -235.83578 -234.8763  -234.7583  -235.79227 -237.60141 -234.71465\n",
      "  -235.57266 -236.35143 -234.37642 -234.94592 -235.223   -235.5331\n",
      "  -236.85919 -236.19052 -234.56642 -234.21066 -235.7679  -235.07863\n",
      "  -233.52618 -237.04082 -236.1133  -235.06787 -233.9352  -236.51855\n",
      "  -235.63672 -236.42032 -235.42966 -233.66757 -234.09758 -235.80821\n",
      "  -235.66194 -236.58974 -235.54128 -235.29741 -235.00589 -235.32431\n",
      "  -235.88159 -236.42998 -234.51778 -235.99672 -236.24089 -234.7165\n",
      "  -250.75163]]...\n",
      "Epoch [200/500], Cumulative Loss: 592133.9375, LR: 1.000e-03\n",
      "Final parameters: [[-252.14507 -248.90736 -246.49506 -242.2411  -239.1222  -237.38493\n",
      "  -237.47748 -236.54007 -235.91164 -234.4932  -236.20488 -233.40689\n",
      "  -234.15993 -234.2869  -234.97961 -234.31186 -235.03537 -235.45605\n",
      "  -236.33438 -235.7442  -234.46129 -234.88976 -234.06023 -233.86781\n",
      "  -235.24721 -235.08852 -234.67516 -235.91594 -234.15675 -232.97104\n",
      "  -234.44281 -234.56638 -232.91116 -234.0793  -235.77873 -235.24098\n",
      "  -235.26335 -234.82275 -233.79625 -235.15929 -234.71709 -235.7203\n",
      "  -237.3642  -235.03143 -236.29507 -235.23085 -234.06332 -235.75581\n",
      "  -234.64555 -233.59467 -234.61371 -235.86668 -235.0775  -233.56738\n",
      "  -236.07857 -235.96237 -235.79707 -233.64365 -234.6407  -236.37779\n",
      "  -236.32701 -233.69283 -235.26697 -235.09158 -234.3217  -235.02802\n",
      "  -236.552   -235.34001 -235.30913 -233.9667  -236.32445 -234.59569\n",
      "  -237.79114 -234.68668 -234.57445 -235.85693 -234.03435 -235.28108\n",
      "  -234.46567 -234.39507 -237.33667 -236.74643 -235.05504 -235.60573\n",
      "  -235.0168  -235.77042 -234.24112 -235.0466  -232.14085 -235.21198\n",
      "  -235.49751 -234.00183 -235.27542 -233.72736 -235.47733 -236.46951\n",
      "  -233.92159 -234.77039 -234.18713 -233.3019  -236.70346 -234.37502\n",
      "  -235.78894 -235.59894 -236.55293 -233.32568 -235.05269 -234.06985\n",
      "  -235.34023 -235.40146 -235.03748 -234.2978  -234.47629 -234.60905\n",
      "  -235.29771 -235.41989 -235.49908 -234.94249 -234.63686 -236.53836\n",
      "  -234.77773 -235.70284 -234.03046 -233.74258 -235.38615 -236.10594\n",
      "  -235.93611 -233.69022 -235.05946 -234.84175 -234.59254 -234.1847\n",
      "  -232.68625 -233.1913  -235.16281 -235.90056 -235.45314 -233.32314\n",
      "  -233.13718 -234.49263 -234.99544 -234.34883 -234.21857 -233.98965\n",
      "  -236.73659 -234.52979 -233.71593 -236.86777 -232.5589  -236.07082\n",
      "  -234.59204 -234.10423 -233.89569 -234.62146 -233.40154 -234.02272\n",
      "  -233.63104 -235.6926  -235.1944  -232.49179 -234.22653 -233.57285\n",
      "  -235.67816 -235.48222 -236.19283 -235.46721 -234.6938  -232.95808\n",
      "  -235.44298 -234.7899  -233.41109 -234.37125 -236.1314  -234.52449\n",
      "  -232.40784 -234.05118 -234.74184 -234.58429 -234.0035  -235.23523\n",
      "  -234.23413 -235.93893 -235.66939 -234.55463 -235.6413  -236.17628\n",
      "  -235.20364 -233.92004 -233.08498 -233.9213  -235.34415 -236.00826\n",
      "  -234.95271 -235.62418 -234.16486 -234.74669 -233.87077 -234.32605\n",
      "  -233.93683 -234.27933 -236.16512 -235.05862 -235.5319  -233.2726\n",
      "  -234.83244 -234.86496 -237.05667 -234.96115 -234.89996 -234.28949\n",
      "  -233.61778 -234.12024 -235.9663  -234.58632 -236.64651 -233.71083\n",
      "  -235.75142 -233.83987 -235.3796  -236.59749 -234.54904 -234.9591\n",
      "  -235.2437  -234.13147 -235.13399 -235.6607  -233.50986 -236.32233\n",
      "  -232.45628 -234.97246 -234.2776  -235.89671 -235.01305 -235.10944\n",
      "  -234.8799  -235.43341 -235.05132 -233.80913 -233.9881  -233.93756\n",
      "  -248.28616]]...\n",
      "Epoch [250/500], Cumulative Loss: 590193.6875, LR: 1.000e-03\n",
      "Final parameters: [[-250.12242 -249.7563  -245.75003 -239.60822 -238.95941 -237.34647\n",
      "  -235.75424 -235.07175 -233.49991 -233.0213  -233.52843 -232.3713\n",
      "  -233.12926 -233.65948 -234.41132 -235.13188 -234.2396  -233.73207\n",
      "  -234.27966 -234.37578 -234.31198 -233.08627 -233.24965 -234.24014\n",
      "  -234.33537 -234.68356 -232.22746 -236.004   -235.41554 -235.91014\n",
      "  -233.17532 -234.59245 -235.03023 -234.45784 -233.20016 -234.47462\n",
      "  -233.70724 -234.17833 -233.6195  -234.12267 -234.42198 -235.3211\n",
      "  -234.2751  -235.4027  -233.58188 -232.95181 -234.24678 -234.69916\n",
      "  -233.157   -231.39993 -233.8928  -235.11722 -233.93509 -233.6467\n",
      "  -232.21985 -232.44511 -234.43367 -232.66032 -234.02698 -234.7833\n",
      "  -236.1846  -232.23624 -235.18301 -233.2458  -234.16241 -234.56062\n",
      "  -233.2726  -234.58336 -233.70691 -235.62918 -234.7981  -233.23604\n",
      "  -233.58856 -234.1349  -234.89163 -234.19421 -234.57799 -233.8369\n",
      "  -234.43922 -232.97546 -234.79091 -233.91109 -232.83316 -235.36722\n",
      "  -234.6339  -233.64217 -233.18935 -232.98094 -234.61275 -233.75583\n",
      "  -236.7449  -234.06953 -236.26637 -232.50145 -233.80759 -233.96701\n",
      "  -234.7615  -234.83153 -232.77567 -232.6968  -233.22243 -232.71503\n",
      "  -234.30072 -232.95758 -234.67838 -234.64003 -233.3166  -234.65527\n",
      "  -233.88449 -232.89215 -233.46585 -234.29498 -235.2714  -234.94846\n",
      "  -234.94241 -233.19092 -233.66734 -235.8249  -231.73286 -234.29189\n",
      "  -234.59486 -234.63824 -235.58122 -232.62212 -234.0553  -235.54189\n",
      "  -233.80711 -235.08035 -235.74615 -234.92595 -234.67017 -235.13025\n",
      "  -233.75148 -235.50558 -234.11165 -231.43161 -233.33968 -233.69447\n",
      "  -234.27975 -232.63742 -234.82468 -233.6352  -237.5407  -233.96317\n",
      "  -235.24203 -235.75385 -233.67223 -231.9982  -234.89084 -235.18066\n",
      "  -234.10875 -233.71877 -235.56673 -234.52216 -236.81805 -234.51509\n",
      "  -234.0881  -233.21999 -233.88287 -234.31468 -233.61134 -232.53285\n",
      "  -235.51068 -234.79347 -232.95712 -233.02728 -233.56464 -234.00801\n",
      "  -235.85332 -234.08232 -234.09242 -232.69669 -234.49956 -233.93988\n",
      "  -233.87845 -233.35658 -234.98914 -234.16452 -233.62675 -234.5862\n",
      "  -235.0293  -234.4702  -232.9633  -234.50395 -233.6625  -233.61958\n",
      "  -235.0321  -234.16682 -234.90991 -232.64816 -235.17729 -233.50099\n",
      "  -233.12848 -234.97632 -234.03317 -235.4177  -235.813   -234.04184\n",
      "  -234.57317 -234.07333 -235.3423  -234.36517 -235.93503 -234.24591\n",
      "  -232.83505 -234.59918 -232.5349  -235.69301 -234.46115 -234.8733\n",
      "  -233.01945 -235.8516  -235.36082 -234.35364 -235.07063 -233.78355\n",
      "  -234.46033 -233.92595 -235.51038 -233.37502 -234.46014 -235.12932\n",
      "  -233.38141 -233.5975  -233.34706 -233.0897  -237.32004 -235.33202\n",
      "  -234.92517 -234.81056 -233.02097 -234.83582 -234.91405 -234.64082\n",
      "  -233.65659 -233.27045 -236.12173 -234.99199 -234.16415 -234.78056\n",
      "  -247.88766]]...\n",
      "Epoch [300/500], Cumulative Loss: 589539.1875, LR: 1.000e-04\n",
      "Final parameters: [[-250.48509 -248.24417 -243.15741 -241.28093 -238.39584 -237.71812\n",
      "  -235.55803 -234.6398  -233.13596 -233.64636 -233.3937  -232.08018\n",
      "  -232.25702 -233.1572  -233.40309 -233.2248  -233.85402 -234.76877\n",
      "  -232.17133 -234.11    -234.43146 -232.73717 -234.70206 -233.36612\n",
      "  -234.22887 -231.4013  -233.75226 -233.60434 -234.55179 -235.23721\n",
      "  -232.80449 -234.90282 -232.17471 -234.3639  -234.39223 -233.43918\n",
      "  -235.08853 -233.06624 -234.28296 -233.15709 -232.33919 -234.62523\n",
      "  -232.87325 -234.00769 -233.66121 -232.93536 -234.77628 -233.51323\n",
      "  -232.53075 -232.30615 -233.28468 -234.23526 -233.95744 -233.28181\n",
      "  -233.15126 -233.5455  -232.41653 -232.69432 -233.31409 -232.54436\n",
      "  -231.98654 -232.86209 -232.58583 -233.40776 -232.32843 -235.12689\n",
      "  -232.60954 -234.38313 -233.20316 -232.70914 -233.09885 -234.13301\n",
      "  -234.21921 -233.71405 -232.07172 -232.73422 -234.32184 -233.32088\n",
      "  -232.98102 -235.83609 -232.23245 -233.63702 -234.43755 -233.64037\n",
      "  -234.15573 -233.59518 -233.90222 -232.94429 -234.01009 -232.21623\n",
      "  -233.3945  -234.38214 -234.20944 -233.16359 -233.81107 -232.67334\n",
      "  -234.45956 -232.77608 -232.1041  -234.40536 -232.46207 -234.02013\n",
      "  -234.86505 -233.3734  -233.97708 -232.6698  -235.4201  -233.9382\n",
      "  -231.19945 -234.29033 -234.04025 -233.33414 -231.81538 -233.39925\n",
      "  -233.02188 -231.91068 -231.59636 -234.48128 -233.00783 -234.42232\n",
      "  -233.66772 -231.98314 -233.69392 -233.59492 -232.105   -233.62065\n",
      "  -233.17657 -234.75116 -231.26183 -234.16089 -234.70757 -232.7591\n",
      "  -232.38521 -234.4515  -233.64163 -233.23668 -234.51451 -233.8873\n",
      "  -233.335   -232.52243 -234.34827 -232.05965 -235.10239 -232.76408\n",
      "  -232.80635 -232.98251 -231.89917 -232.51259 -233.90916 -234.57217\n",
      "  -233.62573 -234.65964 -233.59856 -234.79926 -231.35388 -232.49022\n",
      "  -231.3311  -232.13914 -233.35947 -234.27628 -235.06638 -232.56464\n",
      "  -233.4121  -233.5756  -234.51875 -234.43463 -233.30885 -234.9594\n",
      "  -232.74548 -233.0352  -232.71146 -231.58897 -233.66666 -233.03394\n",
      "  -232.52303 -234.12885 -231.87274 -236.30736 -234.14357 -233.33255\n",
      "  -233.48297 -232.84174 -235.10983 -234.04486 -232.18446 -232.85013\n",
      "  -234.56825 -232.80858 -231.89676 -233.61382 -233.54208 -235.41286\n",
      "  -234.20752 -232.87369 -233.13542 -231.60928 -233.02579 -233.93121\n",
      "  -233.60143 -233.48726 -232.69186 -234.28851 -233.72092 -233.135\n",
      "  -233.3204  -232.75386 -232.56905 -233.14293 -233.31628 -233.61507\n",
      "  -233.96863 -233.9601  -233.76593 -234.96942 -234.44315 -233.55757\n",
      "  -234.78458 -234.5984  -232.83067 -233.41301 -231.75682 -233.4878\n",
      "  -232.12808 -234.76526 -232.27489 -231.77231 -233.33969 -230.77054\n",
      "  -234.65535 -233.20192 -231.29532 -232.36436 -234.28758 -234.86716\n",
      "  -232.8375  -234.19864 -235.0054  -233.57884 -235.28809 -235.25063\n",
      "  -247.7461 ]]...\n",
      "Epoch [350/500], Cumulative Loss: 595664.9375, LR: 1.000e-04\n",
      "Final parameters: [[-251.12224 -246.24547 -244.15399 -241.68477 -237.21094 -236.64478\n",
      "  -233.13022 -233.50906 -235.05568 -232.86597 -233.759   -234.46808\n",
      "  -234.25566 -233.0498  -233.72974 -231.94    -232.94324 -234.26483\n",
      "  -231.89076 -232.29912 -232.15868 -233.71106 -234.6102  -232.22514\n",
      "  -234.63077 -232.6674  -233.56618 -232.7279  -233.08406 -233.08604\n",
      "  -232.13676 -235.37881 -235.09769 -232.47043 -232.76254 -232.5174\n",
      "  -233.20537 -233.93762 -234.4066  -232.7678  -231.54144 -233.53983\n",
      "  -234.90187 -234.30719 -236.19164 -235.26328 -232.96745 -233.58652\n",
      "  -232.50569 -234.456   -232.4614  -232.62505 -234.44261 -232.80066\n",
      "  -234.72931 -232.62027 -231.7803  -231.77385 -234.21262 -232.80838\n",
      "  -233.78558 -232.93898 -233.1145  -232.67787 -231.80247 -234.10211\n",
      "  -233.40817 -234.60156 -232.66486 -235.16194 -232.89279 -232.99194\n",
      "  -232.48729 -234.13934 -232.99165 -233.45836 -233.79668 -232.47107\n",
      "  -233.33058 -232.66881 -232.00485 -234.44711 -234.2448  -233.86308\n",
      "  -234.10956 -233.78433 -234.1402  -233.64374 -233.8501  -232.05234\n",
      "  -232.79784 -233.26561 -234.35435 -233.12114 -233.27045 -232.8081\n",
      "  -234.33167 -233.38611 -233.2689  -233.0511  -233.37195 -233.61086\n",
      "  -232.23729 -232.52199 -232.10716 -233.41988 -232.76935 -235.06488\n",
      "  -234.93369 -233.2784  -232.08334 -234.04561 -233.37924 -231.96687\n",
      "  -234.441   -233.52861 -232.90125 -233.27753 -234.47183 -234.03084\n",
      "  -234.34586 -234.81042 -232.45619 -232.63156 -234.05962 -233.48041\n",
      "  -234.44542 -234.00179 -233.0237  -233.17734 -233.94395 -232.65244\n",
      "  -234.69774 -232.65567 -233.41628 -232.51761 -233.57823 -233.37263\n",
      "  -233.31523 -233.29103 -234.37819 -233.15526 -232.75652 -234.05013\n",
      "  -232.38992 -234.33136 -233.86177 -233.22964 -233.313   -233.56221\n",
      "  -231.49768 -234.01595 -234.01135 -232.54953 -233.40388 -231.90834\n",
      "  -232.24747 -232.39513 -235.607   -234.73665 -231.87119 -233.4426\n",
      "  -235.04137 -234.12503 -234.056   -235.3726  -233.78375 -233.89708\n",
      "  -231.69806 -235.68036 -233.4159  -233.85081 -233.35257 -233.56012\n",
      "  -234.07079 -234.81822 -233.96515 -231.0416  -234.09756 -233.23685\n",
      "  -235.11354 -233.30319 -231.72183 -233.20677 -235.1933  -233.16985\n",
      "  -232.39246 -233.46242 -232.44441 -233.34644 -232.73265 -232.92322\n",
      "  -234.09193 -232.0844  -233.42316 -232.52975 -231.98456 -235.36034\n",
      "  -233.47908 -234.43787 -234.48119 -234.46428 -231.6141  -231.90103\n",
      "  -233.1657  -233.49625 -232.98299 -232.68839 -234.07047 -233.84506\n",
      "  -233.85854 -233.39697 -235.13339 -233.14636 -235.71812 -232.70413\n",
      "  -232.9442  -234.07388 -232.87068 -234.10905 -232.85988 -234.13525\n",
      "  -232.69073 -233.54224 -232.09494 -233.42236 -233.79016 -233.40282\n",
      "  -234.0773  -232.63982 -234.05681 -234.30649 -232.85736 -234.08632\n",
      "  -232.67097 -233.85979 -232.7648  -233.107   -232.87062 -233.31178\n",
      "  -248.94849]]...\n",
      "Epoch [400/500], Cumulative Loss: 579602.0000, LR: 1.000e-04\n",
      "Final parameters: [[-249.1488  -245.95952 -244.50758 -241.1406  -238.1548  -236.61378\n",
      "  -234.93565 -235.01942 -234.29195 -232.26244 -232.23149 -233.33673\n",
      "  -233.09456 -234.00919 -233.09207 -234.97763 -233.93971 -234.28664\n",
      "  -232.55911 -233.83295 -233.39551 -231.97298 -232.58334 -233.16138\n",
      "  -233.3449  -231.2753  -234.47382 -233.31125 -232.58722 -234.89325\n",
      "  -233.48174 -232.03714 -232.15309 -233.8652  -233.17438 -233.83339\n",
      "  -234.13998 -232.88681 -233.65938 -233.2988  -233.49825 -234.48665\n",
      "  -233.23994 -232.4801  -233.30055 -234.92401 -233.29253 -232.51054\n",
      "  -233.94476 -233.4974  -234.38513 -234.22937 -233.3597  -232.41809\n",
      "  -233.53539 -233.75264 -231.31773 -233.84338 -232.23294 -233.15631\n",
      "  -232.81956 -232.07626 -232.89957 -234.51259 -233.80566 -231.98871\n",
      "  -234.03242 -233.66339 -233.51839 -232.30276 -231.97604 -232.55475\n",
      "  -231.70074 -233.39413 -234.24043 -233.57315 -233.38287 -233.63968\n",
      "  -233.2886  -232.16025 -234.16608 -234.01407 -232.94615 -232.98157\n",
      "  -233.88617 -232.66972 -232.80772 -235.16508 -234.16772 -233.94603\n",
      "  -232.75401 -232.725   -234.57506 -233.51334 -232.59894 -232.88635\n",
      "  -233.85654 -231.58237 -234.22993 -233.36887 -232.4808  -232.90196\n",
      "  -233.81499 -232.82237 -232.07245 -232.19621 -233.11642 -233.99564\n",
      "  -233.01085 -231.68259 -234.12175 -232.87613 -233.81224 -234.486\n",
      "  -234.9862  -233.51872 -232.18307 -232.35861 -232.95804 -233.57527\n",
      "  -232.34407 -233.40569 -234.33093 -232.6003  -234.21999 -232.77988\n",
      "  -231.72653 -233.80649 -234.15274 -232.815   -234.49284 -233.25728\n",
      "  -234.47754 -234.26114 -231.4902  -233.01018 -232.3958  -232.39766\n",
      "  -230.86485 -233.55531 -235.02684 -232.86807 -234.95952 -233.09192\n",
      "  -231.45747 -233.75652 -231.9781  -234.743   -235.12733 -231.86435\n",
      "  -233.15906 -231.78717 -233.35356 -234.33046 -232.69568 -234.55208\n",
      "  -234.36378 -231.80646 -233.17865 -234.00531 -233.33698 -233.51456\n",
      "  -232.71309 -234.97313 -233.87724 -232.10797 -234.72105 -233.38316\n",
      "  -233.40009 -232.80902 -231.62302 -233.74174 -236.32025 -233.3231\n",
      "  -233.5275  -233.25928 -233.68071 -231.60835 -233.23555 -232.2263\n",
      "  -232.58899 -233.17207 -235.41792 -233.89256 -234.93384 -233.67053\n",
      "  -233.4456  -232.97589 -232.69936 -233.18779 -232.48672 -233.34827\n",
      "  -232.81961 -234.22278 -232.11847 -234.17065 -233.09424 -231.49014\n",
      "  -233.6152  -232.6916  -231.83093 -233.19583 -232.54906 -233.0189\n",
      "  -232.90262 -232.51924 -233.45078 -234.05495 -232.59384 -234.44344\n",
      "  -233.06224 -233.41116 -233.82263 -233.1739  -232.5864  -233.79236\n",
      "  -233.6816  -232.27246 -234.01262 -234.19446 -232.75758 -233.87546\n",
      "  -233.31877 -230.54921 -232.93925 -234.1336  -233.15091 -234.83994\n",
      "  -233.03772 -233.1767  -235.2626  -231.82954 -232.93077 -232.4047\n",
      "  -233.44365 -231.82582 -233.36356 -232.51341 -233.27954 -234.68\n",
      "  -245.70819]]...\n",
      "Epoch [450/500], Cumulative Loss: 585030.3750, LR: 1.000e-05\n",
      "Final parameters: [[-249.50375 -248.77867 -243.38278 -240.77962 -237.45653 -235.57286\n",
      "  -235.64195 -233.0263  -233.5678  -233.4622  -232.23378 -233.77203\n",
      "  -233.95052 -231.94228 -233.08566 -233.82196 -233.64127 -235.19766\n",
      "  -234.0782  -235.72128 -232.71095 -232.56412 -231.29472 -234.13411\n",
      "  -232.74156 -232.69539 -231.94197 -234.39394 -233.0803  -232.6836\n",
      "  -234.63976 -233.66028 -233.0859  -233.87802 -233.17517 -232.47176\n",
      "  -232.75923 -231.82623 -232.80772 -232.85406 -233.09695 -232.9397\n",
      "  -231.94685 -233.91441 -231.41066 -232.14369 -233.9164  -231.29187\n",
      "  -232.69989 -231.3901  -232.77557 -232.37207 -233.61417 -234.56378\n",
      "  -233.07759 -231.96774 -231.74925 -233.86446 -233.79033 -231.57849\n",
      "  -234.27725 -233.2717  -232.54837 -233.0969  -233.13997 -234.59302\n",
      "  -232.61841 -234.26254 -233.40602 -232.73631 -234.17145 -233.8198\n",
      "  -233.50311 -234.0668  -233.28716 -233.34044 -232.61366 -234.70502\n",
      "  -232.3947  -234.1649  -230.89531 -231.97343 -233.40265 -232.90059\n",
      "  -231.55255 -232.55385 -233.30809 -233.4726  -234.84645 -234.59354\n",
      "  -231.871   -232.17786 -231.42642 -233.23491 -231.40265 -232.02766\n",
      "  -232.57631 -233.4117  -234.96875 -234.99472 -231.8819  -233.04411\n",
      "  -235.07382 -233.16211 -233.01833 -233.29556 -234.98376 -233.50772\n",
      "  -234.18777 -232.04916 -233.10104 -231.98225 -231.25623 -232.91629\n",
      "  -232.94669 -232.67868 -233.17784 -234.44427 -232.44463 -233.0307\n",
      "  -232.58347 -231.97388 -232.80988 -233.53877 -232.76506 -233.4014\n",
      "  -232.65825 -234.3921  -231.68457 -233.59549 -233.80453 -232.31044\n",
      "  -233.44548 -234.37314 -233.50546 -232.94865 -232.3726  -233.5597\n",
      "  -231.09024 -233.18109 -233.49059 -231.80016 -234.20871 -232.97113\n",
      "  -233.71417 -234.62769 -234.1392  -233.46078 -232.28395 -232.0132\n",
      "  -232.6367  -232.82816 -232.34682 -233.32175 -232.47223 -231.61266\n",
      "  -232.8603  -233.11017 -234.19278 -233.20422 -234.15979 -232.85916\n",
      "  -232.6054  -232.59932 -232.4359  -232.06947 -232.52031 -231.7946\n",
      "  -233.00664 -231.54245 -229.49977 -232.23251 -234.43599 -231.42442\n",
      "  -232.6904  -232.74702 -233.12482 -233.03496 -232.13542 -232.54189\n",
      "  -233.06271 -232.35832 -233.40225 -233.12738 -234.18947 -232.45074\n",
      "  -234.6507  -233.12483 -233.59526 -234.74785 -235.51773 -235.06075\n",
      "  -232.711   -233.32933 -234.24423 -231.59581 -235.54584 -233.31564\n",
      "  -231.69148 -233.94043 -233.0994  -232.74365 -232.75954 -234.28777\n",
      "  -234.41089 -231.439   -232.86981 -231.90463 -232.98157 -232.98347\n",
      "  -234.20702 -233.16965 -234.80717 -234.56454 -234.19696 -232.98602\n",
      "  -232.37825 -232.8187  -233.83134 -232.26048 -234.47318 -232.82022\n",
      "  -234.4722  -231.85004 -235.9811  -232.57704 -231.92946 -231.49652\n",
      "  -232.74986 -233.02586 -232.06516 -233.55072 -235.38759 -233.88994\n",
      "  -233.61179 -232.45387 -235.10474 -232.05504 -233.10558 -233.44577\n",
      "  -246.80605]]...\n",
      "Epoch [500/500], Cumulative Loss: 589698.3750, LR: 1.000e-05\n",
      "Final parameters: [[-250.13821 -248.03445 -244.0472  -240.57906 -237.11223 -235.50398\n",
      "  -234.50067 -234.10416 -233.13416 -232.61098 -233.38368 -233.92877\n",
      "  -233.43668 -232.07904 -233.77295 -231.38826 -235.34442 -231.73055\n",
      "  -233.0683  -234.28442 -232.59879 -234.16779 -232.39317 -233.76373\n",
      "  -234.33542 -233.81143 -232.71915 -232.3165  -233.75949 -231.362\n",
      "  -235.15666 -232.8562  -233.02689 -233.17398 -232.15025 -230.27583\n",
      "  -234.61522 -232.10107 -230.55708 -233.25078 -233.3684  -231.46446\n",
      "  -233.74121 -234.10164 -233.88966 -232.79298 -233.46184 -232.4823\n",
      "  -232.35857 -232.6512  -233.66165 -234.09038 -231.85141 -231.01266\n",
      "  -234.46928 -235.21507 -233.71004 -232.77644 -232.28857 -233.00829\n",
      "  -233.58765 -233.49182 -233.64108 -231.90515 -233.87842 -233.76486\n",
      "  -232.4463  -234.65746 -232.39378 -235.11017 -232.09053 -233.20844\n",
      "  -233.98572 -233.1501  -233.74722 -232.07774 -232.68541 -232.31238\n",
      "  -233.62704 -232.84714 -233.43596 -232.04118 -233.3807  -233.22475\n",
      "  -232.035   -236.21564 -231.41173 -233.30878 -233.01361 -233.05206\n",
      "  -233.58795 -233.82361 -232.95543 -233.31946 -233.50122 -232.97684\n",
      "  -233.10791 -233.25185 -234.96095 -234.7898  -233.555   -234.11635\n",
      "  -234.0729  -232.12616 -232.99442 -233.44057 -233.3729  -232.14526\n",
      "  -231.54205 -232.95561 -231.92711 -233.53618 -234.02516 -233.7514\n",
      "  -233.72136 -232.74344 -232.04987 -235.36928 -232.43164 -233.54198\n",
      "  -233.66347 -232.92996 -231.90776 -233.45416 -233.33612 -234.0553\n",
      "  -232.26987 -234.43121 -232.10095 -232.20451 -234.84872 -232.5159\n",
      "  -232.6854  -235.27853 -232.56113 -232.3918  -231.02275 -232.21173\n",
      "  -234.12479 -233.5954  -232.60503 -234.75313 -232.40016 -234.36806\n",
      "  -231.84486 -234.68651 -232.99889 -231.95065 -233.701   -234.0781\n",
      "  -230.14641 -233.35689 -234.55925 -232.23798 -234.55635 -233.67435\n",
      "  -233.95265 -232.16815 -233.7928  -234.08458 -232.31517 -233.86392\n",
      "  -231.9313  -233.12123 -233.72081 -232.08147 -233.85803 -233.63034\n",
      "  -233.58067 -232.59529 -232.34274 -231.91554 -234.58534 -233.17232\n",
      "  -233.09161 -232.07448 -233.41646 -231.71841 -231.65225 -234.73666\n",
      "  -232.44179 -231.50882 -232.08531 -233.26897 -232.20982 -235.1744\n",
      "  -232.75076 -234.72876 -233.36685 -234.19528 -232.96307 -234.45007\n",
      "  -234.41112 -232.79083 -232.6773  -234.37947 -233.56837 -231.99614\n",
      "  -233.84148 -234.90433 -234.49443 -234.06583 -232.36592 -233.52542\n",
      "  -233.20131 -232.34341 -233.36783 -233.5533  -235.00835 -232.92072\n",
      "  -232.69034 -233.59306 -234.5772  -234.90364 -233.11536 -234.0432\n",
      "  -234.2812  -233.08952 -233.63673 -232.29118 -232.49632 -233.65512\n",
      "  -233.10606 -232.71323 -232.57367 -232.84494 -234.03859 -233.69229\n",
      "  -232.72209 -231.86852 -234.16179 -232.32869 -232.76686 -233.66989\n",
      "  -231.65544 -230.00668 -233.49355 -232.23938 -232.59474 -233.2283\n",
      "  -247.77063]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 244716.4375\n",
      "Final parameters: [[-501.67526 -499.61325 -488.27615 -481.41275 -476.1185  -471.61383\n",
      "  -470.27585 -469.0764  -468.36194 -465.90903 -466.35336 -467.01968\n",
      "  -465.93405 -467.80637 -464.67813 -465.55557 -467.2072  -466.85504\n",
      "  -467.1215  -466.30283 -466.38324 -465.5049  -464.9235  -466.73596\n",
      "  -466.19095 -468.54944 -463.98987 -466.04178 -466.31918 -465.67996\n",
      "  -464.58612 -465.04562 -464.76    -466.65372 -465.24063 -464.84457\n",
      "  -467.3082  -464.5167  -466.7659  -464.98438 -466.35748 -465.605\n",
      "  -464.34448 -466.9046  -466.36542 -464.23242 -465.99756 -465.61267\n",
      "  -464.54535 -466.19525 -465.8345  -466.11917 -467.24796 -465.58826\n",
      "  -465.638   -465.47372 -464.73203 -465.60794 -465.21298 -466.97592\n",
      "  -463.9152  -465.71378 -465.75137 -465.07175 -465.39764 -465.178\n",
      "  -465.58762 -465.08612 -464.67703 -466.70514 -467.51855 -465.97183\n",
      "  -466.53903 -466.70657 -466.1627  -465.22794 -466.10696 -467.12445\n",
      "  -467.06763 -464.57028 -464.62003 -465.3383  -465.98465 -466.09094\n",
      "  -466.84766 -467.23627 -465.74805 -465.48825 -466.32632 -466.55383\n",
      "  -466.11084 -465.2734  -467.14413 -465.69205 -465.9606  -466.0532\n",
      "  -464.7572  -466.09946 -465.68893 -464.72098 -466.87732 -466.2496\n",
      "  -466.5725  -463.8995  -465.7545  -464.52597 -466.23367 -465.6696\n",
      "  -465.08435 -465.9626  -463.49554 -465.48444 -466.2919  -466.1585\n",
      "  -468.97363 -464.81525 -464.15182 -466.7995  -467.68503 -466.95758\n",
      "  -466.78983 -464.89886 -466.25485 -466.9899  -466.94513 -464.84494\n",
      "  -465.08044 -465.64233 -466.9712  -464.6316  -466.7232  -465.57324\n",
      "  -465.56238 -466.9127  -464.47958 -466.61523 -466.8857  -466.762\n",
      "  -465.49585 -464.82172 -465.71713 -464.56    -463.89206 -466.19873\n",
      "  -465.60428 -465.56976 -466.087   -466.23224 -466.37854 -467.66913\n",
      "  -466.0203  -464.96143 -468.21933 -466.07114 -465.5962  -465.29898\n",
      "  -467.90775 -465.71033 -466.40646 -466.13867 -466.86298 -465.2396\n",
      "  -467.16568 -466.08456 -468.25955 -466.7953  -464.59012 -466.8923\n",
      "  -467.91635 -465.6311  -466.6786  -466.36664 -466.90448 -467.29358\n",
      "  -465.58426 -467.63736 -466.42963 -467.9628  -465.0962  -465.98016\n",
      "  -465.87424 -466.6702  -466.76312 -466.39993 -466.07788 -467.13657\n",
      "  -468.23575 -467.45004 -466.37592 -465.52008 -468.25476 -467.386\n",
      "  -467.59616 -464.7465  -464.86908 -466.08252 -466.22333 -466.70627\n",
      "  -466.0063  -465.3097  -466.5914  -466.08588 -465.31256 -466.17352\n",
      "  -466.52292 -467.00436 -465.41376 -466.0133  -465.2845  -467.508\n",
      "  -467.04932 -466.19348 -466.7455  -465.89865 -466.92767 -466.76456\n",
      "  -466.84695 -465.71146 -466.92004 -465.94846 -467.6914  -466.45905\n",
      "  -466.31326 -467.08362 -465.41595 -465.52988 -465.71118 -465.7605\n",
      "  -466.74097 -466.50223 -466.85815 -465.23465 -465.65903 -465.69452\n",
      "  -465.6078  -466.53262 -464.88315 -466.2251  -466.70093 -466.0645\n",
      "  -494.7365 ]]...\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "initializer = Initializer(XYNNOptimizee, kwargs)\n",
    "scheduler = optim.lr_scheduler.StepLR(meta_optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "writer = SummaryWriter(\"diab_train/lr1e-4_MSE_2_layers_scheduled\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=500, discount=0.9, writer=writer, scheduler=scheduler)    \n",
    "writer = SummaryWriter(\"diab_test/lr1e-4_MSE_2_layers_scheduled\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08bd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
