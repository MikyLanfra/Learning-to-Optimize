{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49f7bc7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdbf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "\n",
    "from src.optimizee import *\n",
    "from src.optimizer import *\n",
    "from src.initializer import *\n",
    "\n",
    "from src.train_lstm import *\n",
    "from src.test_optimizer import *\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f1ee7",
   "metadata": {},
   "source": [
    "## Sanity Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94e596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ac2f63a67040f6b4fcf3f1bf51bd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Cumulative Loss: 334928559128707072.0000, LR: 1.000e-03\n",
      "Epoch [2/10], Cumulative Loss: 223858282492592128.0000, LR: 1.000e-03\n",
      "Epoch [3/10], Cumulative Loss: 124885124843044864.0000, LR: 1.000e-03\n",
      "Epoch [4/10], Cumulative Loss: 97719551185125376.0000, LR: 1.000e-03\n",
      "Epoch [5/10], Cumulative Loss: 97970171116781568.0000, LR: 1.000e-03\n",
      "Epoch [6/10], Cumulative Loss: 115990866048319488.0000, LR: 1.000e-03\n",
      "Epoch [7/10], Cumulative Loss: 63587291464990720.0000, LR: 1.000e-03\n",
      "Epoch [8/10], Cumulative Loss: 176612559904833536.0000, LR: 1.000e-03\n",
      "Epoch [9/10], Cumulative Loss: 138666910391730176.0000, LR: 1.000e-03\n",
      "Epoch [10/10], Cumulative Loss: 97805201422942208.0000, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 848.5136108398438\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.02]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=10, min_horizon=50, max_horizon=50, discount=2)\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec303b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae15332316e45bbaac88dc4beb3b0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Cumulative Loss: 2909255.2500, LR: 1.000e-03\n",
      "Epoch [2/10], Cumulative Loss: 2027726.8750, LR: 1.000e-03\n",
      "Epoch [3/10], Cumulative Loss: 1426310.2500, LR: 1.000e-03\n",
      "Epoch [4/10], Cumulative Loss: 994299.5000, LR: 1.000e-03\n",
      "Epoch [5/10], Cumulative Loss: 987480.8750, LR: 1.000e-03\n",
      "Epoch [6/10], Cumulative Loss: 687177.8125, LR: 1.000e-03\n",
      "Epoch [7/10], Cumulative Loss: 472956.6875, LR: 1.000e-03\n",
      "Epoch [8/10], Cumulative Loss: 370703.6875, LR: 1.000e-03\n",
      "Epoch [9/10], Cumulative Loss: 230066.9219, LR: 1.000e-03\n",
      "Epoch [10/10], Cumulative Loss: 253672.6875, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 156190.65625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "y = (y-np.mean(y))/np.std(y)\n",
    "X.shape, y.shape\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss(), nn.L1Loss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=2, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=10, min_horizon=50, max_horizon=50, discount=0)\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dce75",
   "metadata": {},
   "source": [
    "## Quadratic Optimizee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36f3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427765aa11b740dc8ff69f0845ef51c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?time step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.3852],\n",
       "        [0.5164],\n",
       "        [1.5571],\n",
       "        [0.7063],\n",
       "        [0.7442],\n",
       "        [0.9559],\n",
       "        [0.1205],\n",
       "        [1.2055],\n",
       "        [0.9906],\n",
       "        [0.7883]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Optimizers/Adam\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "optimizee = QuadraticOptimizee(W, theta0)\n",
    "optimizee.set_params()\n",
    "optimizer_cls = optim.Adam\n",
    "optimizer_kwargs = {\"lr\":0.05}\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Optimizers/Adam\")\n",
    "test_optimizer(optimizer_cls, optimizee,optimizer_kwargs, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73de3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63240ef617240a185adc379c7bc1ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 18863.7344, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0161, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 9.6801, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0483, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0059, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0231, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0157, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0169, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0197, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0495, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.025036029517650604\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Optimizers/LSTM\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=200, max_horizon=500, discount=0)\n",
    "writer = SummaryWriter(f\"Quadratic/Optimizers/LSTM\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c88dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54c716394944fbebd5f3e4e79cbe25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 87100.5781, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.3382, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1890, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0665, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.3615, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.3239, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1530, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0884, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.3373, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.2057, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.16304242610931396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f661bf5fe3bb413288ed1f1915ad7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 82497.3750, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.2789, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1875, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.2674, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.2013, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.3278, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.2189, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0411, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1276, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5433, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.49988266825675964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f3a931dacb4ae087d7f874261642f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 136763.3594, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.2124, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1286, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.1214, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.1698, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1458, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3471, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.1434, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1870, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1961, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.2404858022928238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fe927745d64365a3c1882cd8d0eebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 38981.0156, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 754.7361, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1826, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.1219, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.1317, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0988, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3531, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0957, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0760, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1229, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.1588994711637497\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    th = i * 100\n",
    "    shutil.rmtree(f\"Quadratic/Horizon/{th}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500-th, max_horizon=500+th, discount=0)\n",
    "    writer = SummaryWriter(f\"Quadratic/Horizon/{th}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b56d749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cdedea587349ea9bfeaaf9bebaea1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 19161.7090, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1679.9042, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0424, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0149, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0937, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1046, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1442, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.3084, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1321, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1007, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.14113982021808624\n",
      "Final parameters: [[ 1.8663387  -0.18337986  2.13553     0.29716218  0.44743246  0.79809546\n",
      "  -0.9425076   1.343382    0.7459527   0.5683814 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1515ce073ad472884a28b8e325bedac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 19180.8203, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1743.2588, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0289, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0382, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0766, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1223, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1372, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.2658, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1685, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0767, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10835688561201096\n",
      "Final parameters: [[ 1.9424584  -0.22430722  2.132454    0.29362375  0.4429594   0.79527706\n",
      "  -0.936571    1.3651272   0.74557936  0.5520531 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12959396f67b4ca09d675ce5f691dc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 21282.2812, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1922.0547, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0245, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0248, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0975, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1307, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1743, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.2652, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1349, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1239, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.15897709131240845\n",
      "Final parameters: [[ 2.0037286  -0.35737175  2.2502873   0.20876706  0.43233424  0.74677634\n",
      "  -1.177658    1.3873047   0.8225865   0.50722057]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a5d047893e4765942aa4046ee632f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 185512.4844, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 16820.3613, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.3394, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.2994, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6529, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.1399, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.7025, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 2.2102, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.1801, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.6545, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.1259104311466217\n",
      "Final parameters: [[ 2.0240297  -0.30898386  2.3234987   0.21957426  0.3159482   0.7972845\n",
      "  -1.1508235   1.3413458   0.83222246  0.5255313 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105f6c8542c64a6f8588fd7ade581ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 3495373.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 299706.8438, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 14753.2412, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 3852.6289, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 1498.2699, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1612.4880, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1393.4733, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1211.7590, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1708.2153, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1392.5122, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.06501167267560959\n",
      "Final parameters: [[ 1.6859447   0.13510853  1.7694694   0.54262686  0.61068016  0.82488966\n",
      "  -0.34569275  1.2157702   0.7859552   0.71407807]]...\n"
     ]
    }
   ],
   "source": [
    "for discount in [0, 1e-3, 0.1, 0.9, 1]:\n",
    "    \n",
    "    shutil.rmtree(f\"Quadratic/Discount/Train_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Quadratic/Discount/Test_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"Quadratic/Discount/Train_{discount}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=discount, writer=writer)\n",
    "    writer = SummaryWriter(f\"Quadratic/Discount/Test_{discount}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6edcdfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f7ae9ca2024f4a8f4c9a644cdac2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 91058821413208064.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 122737228878184448.0000, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 105265430767075328.0000, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 188573854845632512.0000, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 76045557071609856.0000, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 74857964254527488.0000, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 58383139721969664.0000, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 48512647745765376.0000, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 74706257419698176.0000, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 70099801915523072.0000, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 332522.28125\n",
      "Final parameters: [[ 84.53091   83.46353   79.701775  65.80607   82.73815  129.2053\n",
      "   99.99969   85.89128   84.35818   90.16891 ]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(f\"Quadratic/Discount/Train_2\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(f\"Quadratic/Discount/Test_2\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Discount/Train_2\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=50, max_horizon=50, discount=2, writer=writer)\n",
    "writer = SummaryWriter(f\"Quadratic/Discount/Test_2\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34bba52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaddde13da9b4e1295ee8b1c2274d8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 18863.7344, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0161, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 9.6801, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0483, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0059, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0231, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0157, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0169, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0197, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0495, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.025036029517650604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec3cda9c714417c8749f007517dc564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 60361.3008, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.4575, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.7181, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 4.3570, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 3.8612, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 6.1974, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 7.0903, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 2.7111, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 3.8155, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 5.6945, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 4.595547676086426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc31fdb1aa0c43338c66e901d9aa1ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.2998, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1184, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0919, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0944, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.1672, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1896, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0955, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0597, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1520, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1376, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.21306996047496796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d63ab2b3f00437aba577035fd7d5c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 32230.9883, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 5.9552, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 2.8149, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.9725, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6378, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0001, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6976, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.9508, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.7085, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.5810, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 2.0627806186676025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa25ea0cce54e2b83c5e90b08b38839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.4621, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1114, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6370, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.8220, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7274, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.5828, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6032, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.3352, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.0814, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.8935, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6995306611061096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbbd542a2b24527829c09d68ca64087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 18899.3105, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 7.0937, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8159, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 2.2447, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 1.4058, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.8772, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.5287, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.8929, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.3095, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.9035, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.9510489702224731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910d2e3e62884fab8813e1af5b6becc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.7460, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1576, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0994, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0870, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0555, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0388, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0352, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0820, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0270, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0340, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10253313928842545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2a732a375d4cbd9bcf473ddacc82f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.7460, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1576, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0994, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0870, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0555, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0388, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0352, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0820, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0270, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0340, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10253313928842545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11faa759c284ae68e01a174a99586c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8.0472, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 5.3325, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 3.7232, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 3.0543, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 2.3280, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 2.6844, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 4.1541, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 2.5754, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.5171, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 5.7459, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.1009637117385864\n",
      "Times_Train [56.72163724899292, 81.90180659294128, 106.4897084236145, 135.35498309135437, 155.31620001792908, 180.24078154563904, 210.21417117118835, 212.05891036987305, 228.83933758735657]\n",
      "Times_Test [0.8150696754455566, 0.954148530960083, 1.4411990642547607, 1.7495684623718262, 2.428520441055298, 2.3750107288360596, 2.845968723297119, 3.0905141830444336, 2.404212236404419]\n"
     ]
    }
   ],
   "source": [
    "times_train, times_test = [], []\n",
    "for count in range(1, 10):\n",
    "    shutil.rmtree(f\"Quadratic/Multi/Train_{count}_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Quadratic/Multi/Test_{count}_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": list(np.arange(0.01, (count+1)*0.01, 0.01))}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    t0 = time.time()\n",
    "    writer = SummaryWriter(f\"Quadratic/Multi/Train_{count}_optim\") \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=200, max_horizon=500, discount=0, writer=writer)\n",
    "    t1 = time.time()\n",
    "    writer = SummaryWriter(f\"Quadratic/Multi/Test_{count}_optim\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)\n",
    "    t2 = time.time()\n",
    "\n",
    "    times_train.append(t1-t0)\n",
    "    times_test.append(t2-t1)\n",
    "\n",
    "print(\"Times_Train\", times_train)\n",
    "print(\"Times_Test\", times_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626dad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad909349ae584b009968a4c0caccc7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 11530.6094, LR: 1.000e-01\n",
      "Epoch [40/200], Cumulative Loss: 0.4383, LR: 1.000e-01\n",
      "Epoch [60/200], Cumulative Loss: 0.6987, LR: 1.000e-01\n",
      "Epoch [80/200], Cumulative Loss: 0.8164, LR: 1.000e-01\n",
      "Epoch [100/200], Cumulative Loss: 2.5323, LR: 1.000e-01\n",
      "Epoch [120/200], Cumulative Loss: 0.5414, LR: 1.000e-01\n",
      "Epoch [140/200], Cumulative Loss: 0.4951, LR: 1.000e-01\n",
      "Epoch [160/200], Cumulative Loss: 0.2133, LR: 1.000e-01\n",
      "Epoch [180/200], Cumulative Loss: 0.6893, LR: 1.000e-01\n",
      "Epoch [200/200], Cumulative Loss: 0.6405, LR: 1.000e-01\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.2033077478408813\n",
      "Final parameters: [[-1.993716   4.672194  -2.7682881  3.217623   2.6500154  1.6340405\n",
      "   7.3981104 -0.326854   1.8212482  2.4724944]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d717d1a0aaee4ba89a6b50ba87500fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 3.0271, LR: 1.000e-02\n",
      "Epoch [40/200], Cumulative Loss: 23.6708, LR: 1.000e-02\n",
      "Epoch [60/200], Cumulative Loss: 5.9904, LR: 1.000e-02\n",
      "Epoch [80/200], Cumulative Loss: 9.8778, LR: 1.000e-02\n",
      "Epoch [100/200], Cumulative Loss: 4.2154, LR: 1.000e-02\n",
      "Epoch [120/200], Cumulative Loss: 5.8195, LR: 1.000e-02\n",
      "Epoch [140/200], Cumulative Loss: 3.6592, LR: 1.000e-02\n",
      "Epoch [160/200], Cumulative Loss: 4.2677, LR: 1.000e-02\n",
      "Epoch [180/200], Cumulative Loss: 2.6112, LR: 1.000e-02\n",
      "Epoch [200/200], Cumulative Loss: 2.8854, LR: 1.000e-02\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.7332725524902344\n",
      "Final parameters: [[1.5985692  0.06193918 0.80853343 0.86137754 0.84611106 0.70785636\n",
      "  0.38167152 1.3204658  0.24488646 1.0086565 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b824965e1b45ac8ab19323945e8188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 1679.9042, LR: 1.000e-03\n",
      "Epoch [40/200], Cumulative Loss: 0.0149, LR: 1.000e-03\n",
      "Epoch [60/200], Cumulative Loss: 0.1046, LR: 1.000e-03\n",
      "Epoch [80/200], Cumulative Loss: 0.3084, LR: 1.000e-03\n",
      "Epoch [100/200], Cumulative Loss: 0.1007, LR: 1.000e-03\n",
      "Epoch [120/200], Cumulative Loss: 0.1111, LR: 1.000e-03\n",
      "Epoch [140/200], Cumulative Loss: 0.1002, LR: 1.000e-03\n",
      "Epoch [160/200], Cumulative Loss: 0.1151, LR: 1.000e-03\n",
      "Epoch [180/200], Cumulative Loss: 0.1102, LR: 1.000e-03\n",
      "Epoch [200/200], Cumulative Loss: 0.1093, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.07103900611400604\n",
      "Final parameters: [[ 1.5654256   0.30250198  1.5598316   0.60075116  0.7088844   0.81796616\n",
      "  -0.05125184  1.1672177   0.7796621   0.8026926 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc7c1d3c8654778bd29216679f58328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 151222.4062, LR: 1.000e-04\n",
      "Epoch [40/200], Cumulative Loss: 104470.3438, LR: 1.000e-04\n",
      "Epoch [60/200], Cumulative Loss: 65506.3711, LR: 1.000e-04\n",
      "Epoch [80/200], Cumulative Loss: 36242.8086, LR: 1.000e-04\n",
      "Epoch [100/200], Cumulative Loss: 17056.4102, LR: 1.000e-04\n",
      "Epoch [120/200], Cumulative Loss: 7074.2148, LR: 1.000e-04\n",
      "Epoch [140/200], Cumulative Loss: 2590.7512, LR: 1.000e-04\n",
      "Epoch [160/200], Cumulative Loss: 719.8875, LR: 1.000e-04\n",
      "Epoch [180/200], Cumulative Loss: 240.4924, LR: 1.000e-04\n",
      "Epoch [200/200], Cumulative Loss: 77.0269, LR: 1.000e-04\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 95.14395904541016\n",
      "Final parameters: [[ 1.1561422  -3.5963435  -1.160581   -2.333385    0.44913736 -2.902987\n",
      "  -1.7678744  -0.38950413 -2.892629    0.25393853]]...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "\n",
    "    shutil.rmtree(f\"Quadratic/Lr/Train_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Quadratic/Lr/Test_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    lr = 10**(-i)\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=lr)\n",
    "\n",
    "    writer = SummaryWriter(f\"Quadratic/Lr/Train_e{i}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(f\"Quadratic/Lr/Test_e{i}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cb60fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef603e35f38e48df804caff90c6e50b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 3.0597, LR: 8.179e-03\n",
      "Epoch [40/200], Cumulative Loss: 20.0837, LR: 6.690e-03\n",
      "Epoch [60/200], Cumulative Loss: 4.8919, LR: 5.472e-03\n",
      "Epoch [80/200], Cumulative Loss: 8.1621, LR: 4.475e-03\n",
      "Epoch [100/200], Cumulative Loss: 3.3011, LR: 3.660e-03\n",
      "Epoch [120/200], Cumulative Loss: 5.1842, LR: 2.994e-03\n",
      "Epoch [140/200], Cumulative Loss: 6.6976, LR: 2.449e-03\n",
      "Epoch [160/200], Cumulative Loss: 2.6100, LR: 2.003e-03\n",
      "Epoch [180/200], Cumulative Loss: 4.3614, LR: 1.638e-03\n",
      "Epoch [200/200], Cumulative Loss: 3.0451, LR: 1.340e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.4719452857971191\n",
      "Final parameters: [[ 2.814364   -1.4750608   3.383005   -0.14283155 -0.34546745  0.6081619\n",
      "  -3.4897394   1.8527266   0.3712343  -0.02309484]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Lr/Train_Exp\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"Quadratic/Lr/Test_Exp\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(meta_optimizer, gamma=0.99)\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Lr/Train_Exp\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer, scheduler=scheduler)\n",
    "writer = SummaryWriter(f\"Quadratic/Lr/Test_Exp\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e7fbd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533ff77bc92c40d2b444c2fe708faaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 43108.0234, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0079, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0405, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0206, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0381, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0365, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0380, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0392, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0617, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0276, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.051029056310653687\n",
      "Final parameters: [[1.4419096  0.47065443 1.5104656  0.66466796 0.77859986 0.8647847\n",
      "  0.10427374 1.1317858  0.9094761  0.7959955 ]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Comparison/Train_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"Quadratic/Comparison/Test_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Comparison/Train_Right_Horizon\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=800, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"Quadratic/Comparison/Test_Right_Horizon\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49795c8e",
   "metadata": {},
   "source": [
    "## NN Optimizee - Single Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "477fbfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c1d082257e43d58b623ecbcefa6889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.3986, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.5936, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.3766, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5789, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6115, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.5456, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.4613, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.5994, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.4522, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.9941, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.2365554571151733\n",
      "Final parameters: [[-3.7129465e-01  1.0120838e+00  7.5938427e-01 -1.3668164e+00\n",
      "  -5.1037580e-01 -1.0658029e+00  1.4971182e+00  4.0634751e-02\n",
      "   3.8262188e-01  8.8855398e-01 -3.4394583e-01 -7.6368004e-01\n",
      "  -2.7626874e+00 -1.1818930e+00  2.6072996e+00 -4.8864582e-01\n",
      "  -2.7946612e-01 -1.0717512e+00 -2.9759047e+00  1.2299753e+00\n",
      "   7.7837074e-01 -8.5570151e-01 -6.1777079e-01  2.6675978e-01\n",
      "  -8.8365299e-01  7.6057577e-01  1.4695446e+00 -3.5335405e+00\n",
      "  -1.6337606e+00 -2.2283158e+00 -4.0368002e-02 -2.4613020e+00\n",
      "  -1.8428631e+00 -2.7594326e+00  1.7435602e+00 -9.9119765e-01\n",
      "  -1.3761363e+00 -6.9555122e-01 -1.4564085e+00  1.4147767e+00\n",
      "  -2.3020036e+00 -2.1466680e+00 -8.5524380e-01 -3.2766256e+00\n",
      "  -2.1297431e+00 -1.6453563e+00  1.6637623e-01 -2.9189057e+00\n",
      "   3.2251352e-01  1.6453542e-01 -2.3180203e+00  1.1728580e+00\n",
      "  -2.5849156e+00 -2.0901742e+00 -1.6693915e-01 -9.3077198e-02\n",
      "  -3.5649564e+00 -1.7675178e+00 -1.6340851e+00  2.2128115e+00\n",
      "   1.7570481e+00 -1.9332983e+00  1.2279413e+00 -1.3495567e+00\n",
      "  -2.0639312e+00 -1.6047255e+00 -1.3996056e+00 -1.7916061e+00\n",
      "  -8.1336683e-01  1.5607430e+00 -9.3007553e-01  5.6245822e-01\n",
      "  -1.1905439e+00  6.6429198e-01 -9.0891564e-01 -1.5575610e-01\n",
      "  -2.6317093e+00  3.2728508e-01 -1.9183134e+00  2.0951049e+00\n",
      "   2.5853834e-01 -1.4731686e+00 -2.0934184e+00 -1.6806492e-01\n",
      "  -5.6289941e-01 -5.6515139e-01  1.5648942e+00  3.3601439e-01\n",
      "  -2.9395361e+00  3.0032604e+00 -2.1218219e+00 -5.6996113e-01\n",
      "  -8.0700076e-01 -1.8218598e+00 -2.4438467e+00 -1.9473478e+00\n",
      "  -8.9192361e-01  5.5586171e-01 -1.2542737e+00  2.2308929e-01\n",
      "   6.9208086e-01 -5.6347764e-01 -1.3063623e+00 -3.1080883e+00\n",
      "  -1.3711908e+00 -1.1897432e+00  1.7265391e-01 -1.4570749e+00\n",
      "  -3.7451303e-01 -2.1145885e+00 -6.8982136e-01  8.1584495e-01\n",
      "  -3.7239687e+00 -1.6169573e-01  4.8554030e-01  6.6559500e-01\n",
      "  -3.2967708e+00 -2.7053709e+00 -1.4397868e+00  1.3012207e-01\n",
      "  -2.8369884e+00 -1.2202692e+00 -2.4702690e+00  1.4080536e-02\n",
      "  -9.6839350e-01 -1.1050129e+00  1.0956286e-01 -1.4678537e+00\n",
      "  -4.4654718e-01  2.8394878e+00 -1.0080588e+00 -8.1141853e-01\n",
      "  -3.3975203e+00 -1.4237387e+00  8.2956415e-01  9.3292117e-01\n",
      "   2.1884406e+00  9.1113895e-01 -2.1461165e-01  9.4278294e-01\n",
      "   3.5862129e+00 -1.9999858e+00 -4.0092620e-01 -3.2421162e+00\n",
      "  -2.3847904e+00 -1.0684987e+00 -3.1926399e-01  1.6414714e-01\n",
      "   3.6108699e-03  3.2906204e-01  3.5021830e-01  6.4850152e-01\n",
      "  -1.1812999e+00 -1.2886819e+00  1.4348199e+00  1.7614932e+00\n",
      "  -2.8528030e+00 -4.9437580e-01 -6.5902686e-01 -1.2347034e+00\n",
      "   3.4416609e+00  4.0575421e-01 -3.3068471e+00 -1.6998093e+00\n",
      "   3.2884610e-01 -8.3926022e-01 -1.3990382e-02 -6.0220724e-01\n",
      "  -9.9863076e-01  1.0373816e+00 -1.2262503e+00  2.3078799e+00\n",
      "  -3.7148394e-02  5.3519160e-01  6.1170143e-01  1.2698027e+00\n",
      "  -2.1711459e+00 -1.7474065e+00 -1.3881389e+00  1.8234389e+00\n",
      "   1.3191867e+00 -7.4411333e-01 -6.5721172e-01 -1.2980587e+00\n",
      "  -1.3059517e+00 -1.1514738e+00  8.8832635e-01 -1.9493029e+00\n",
      "  -9.2993701e-01  5.0669038e-01 -1.1358422e+00 -2.7001601e-01\n",
      "  -1.2111455e+00 -1.7069277e-01  2.8976865e+00 -1.9038957e-01\n",
      "  -2.7064707e+00  5.1635790e-01 -2.5216789e+00  7.7695745e-01\n",
      "   8.0900103e-01 -2.1036317e+00 -3.1602793e+00  8.6893553e-01\n",
      "  -1.5621006e+00 -1.3808997e+00  2.1989901e+00 -1.7557195e+00\n",
      "   9.2395234e-01  1.6925433e+00 -1.7979804e+00  1.0735635e+00\n",
      "  -2.4908509e+00  3.8706991e-01  1.6682906e-01  1.3175347e+00\n",
      "   9.7015160e-01  2.7078884e+00 -1.7349292e+00  1.9403142e-01\n",
      "  -3.5726050e-01 -1.1903328e+00 -3.6864965e+00 -2.1996830e-01\n",
      "   3.2903171e-01  6.7954540e-02 -1.2045919e+00  2.0904005e-01\n",
      "   3.1883311e-01 -2.4361195e-01 -1.9533123e+00  9.6908009e-01\n",
      "  -2.5033636e+00 -5.4282527e-03  9.8083913e-01  2.0926075e+00\n",
      "  -1.1190651e-02 -9.0308803e-01  1.4971185e-01  1.9297376e+00\n",
      "   5.9889483e-01]]...\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "writer = SummaryWriter(\"NNDiabetes/Train_LSTM\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(\"NNDiabetes/Test_LSTM\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34928ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa61e6fdf51488ea9ed11be3f206bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 7480974.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 7.5716, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6830, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5253, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6185, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8197, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.4618, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.8909, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.5758, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.8395, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5506489276885986\n",
      "Final parameters: [[ 7.599983    2.7843318   6.6394277   2.9566762   1.338065    6.441568\n",
      "   3.521296    2.644813    5.299309    2.0529118   4.108005    7.732799\n",
      "   1.5953798   6.5078998   3.3165114   2.5736332   5.880645    3.0566843\n",
      "   2.4319198   3.0965295   6.1721354   2.8461027   7.4915605   2.5822473\n",
      "   0.628514    6.292873    0.37704724  3.027811    7.548646    1.0867473\n",
      "   3.901279    6.2071195   2.4978766   6.6804295   2.6655128   1.3545585\n",
      "   7.1188927   2.6216297   1.3878142   3.4946406   5.196762    2.9568295\n",
      "   5.2386603   1.9410197   2.2833366   4.8067513   2.916135    5.07099\n",
      "   4.788921    3.981904    2.2653706   6.361101    2.6383216   3.5637407\n",
      "   2.786752    2.6245809   6.282469    2.5733337   3.8980963   1.1235176\n",
      "   5.721139    2.7306046   6.514317    1.1110716   3.7226417   4.8003697\n",
      "   3.0668433   1.7984407   5.9138203   1.1124257   0.78428936  6.9490514\n",
      "   2.2901208   5.1059885   1.5288197   3.200829    3.992846   -0.49204046\n",
      "   1.9040022   3.8482563   5.6919346   1.7047423   8.66863     2.0009303\n",
      "   1.7513175   6.509296    1.8470999   1.5162542   6.014646    2.6436589\n",
      "   0.47104228  9.044347    0.30121922  5.597439   -1.5931109   1.5005535\n",
      "   6.3477135   2.5262022   0.04237706  2.2945015   4.097134    1.3417498\n",
      "   5.7977753   0.3688382   2.643049    4.426043    4.586683    3.9425726\n",
      "   5.1552176   1.3780342   3.0935724   7.6724763   4.0626554   5.501635\n",
      "   3.1580563   2.69695     5.972234    2.3720398   1.093015    2.6351418\n",
      "   6.724571    0.6562282   5.501505    6.090827    2.9055755   4.913275\n",
      "   3.9197714   4.4186015   7.050238    6.728093    3.6015708   4.888302\n",
      "   3.219909    6.564013    7.0115085   5.791482    5.047979    6.7692227\n",
      "   1.1252755   4.7532105   6.1382346   2.3831334   4.3431067   0.96993256\n",
      "   2.2817438   3.8144639   5.3181863   3.082766    5.4973736   2.1345553\n",
      "   2.2044263   6.690487    0.99051857  5.9468327   3.0649586   1.5243125\n",
      "   6.298171    3.3616493   4.3416395   1.0787802   4.1742105   2.8888402\n",
      "   4.9240203   3.1858768   1.4732853   5.5265684   2.6745427   3.7622287\n",
      "   4.2308064   3.1054373   3.8588688   7.971022    4.2645154   5.0007577\n",
      "   2.961308    2.8376472   4.456857    1.9463602   1.6941262   3.5899348\n",
      "   6.175564    2.9312165   7.3124027   2.6443315   2.133915    4.976049\n",
      "   1.7846047   3.3137712   7.0765696   1.9302571   2.4288354   6.5027742\n",
      "   2.5484798   5.0517015   2.1604497   2.128576    6.9648123   3.3264859\n",
      "   1.5971448   2.803419    3.3898895   1.4497815   2.6385882  -0.6165055\n",
      "   0.68598735  4.1371007   1.6531522   2.5992334   3.3796268  -1.0051123\n",
      "   0.9644002   2.6423643   1.1941952   4.1990337  -0.91264725 -1.0509259\n",
      "   4.3981895  -0.26685336  1.0002316   0.80808264 -0.73010707  1.5995554\n",
      "  -0.5280569   1.9399626   1.6619558  -0.7196432   0.5872846   0.5889765\n",
      "  -0.27096522  2.1505404   0.9406722  -0.10761826  1.0312479  -0.51579595\n",
      "   0.6385701   0.88974226 -0.3677519   0.11371606  1.3568012   0.8352425\n",
      "   1.3813143 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88e31e2a7224f4290c2ad86b40fe715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 7487125.5000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 8.9984, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.5436, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5380, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.5964, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.2576, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.5960, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6481, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.5448, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.4570, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5230700969696045\n",
      "Final parameters: [[ 7.4017563e+00  2.8264959e+00  6.3616233e+00  2.8975861e+00\n",
      "   1.2441117e+00  6.1409235e+00  3.6253059e+00  2.8402042e+00\n",
      "   5.3419895e+00  1.4945668e+00  4.5759072e+00  7.9320498e+00\n",
      "   1.9294580e+00  6.3124814e+00  3.1238980e+00  2.8946066e+00\n",
      "   5.7403312e+00  4.8980441e+00  2.3405907e+00  2.0481553e+00\n",
      "   6.1992817e+00  2.6260600e+00  7.1386933e+00  2.8571475e+00\n",
      "   1.5863402e+00  6.2391295e+00  2.6260856e-01  2.9462981e+00\n",
      "   7.8133607e+00  1.0453809e+00  3.9963589e+00  6.4323521e+00\n",
      "   2.4653625e+00  6.7453742e+00  2.7292287e+00  8.5792917e-01\n",
      "   7.2390571e+00  4.3445292e+00  1.4262092e+00  2.4893045e+00\n",
      "   5.1304998e+00  2.9483993e+00  4.8349199e+00  1.8688985e+00\n",
      "   3.2380466e+00  4.6171346e+00  2.8410087e+00  5.2240968e+00\n",
      "   4.9545693e+00  2.9308631e+00  2.5135047e+00  6.4226985e+00\n",
      "   2.7656753e+00  3.4909487e+00  2.6614673e+00  2.1821325e+00\n",
      "   6.2544966e+00  3.5701141e+00  4.1188865e+00  1.1054887e+00\n",
      "   5.5777149e+00  2.8494067e+00  6.1269031e+00  1.3215711e+00\n",
      "   3.5502007e+00  4.5431485e+00  3.0800602e+00  2.0627673e+00\n",
      "   6.0009747e+00  6.0300857e-01  9.7587132e-01  7.0319371e+00\n",
      "   2.5378377e+00  4.9685707e+00  1.5417436e+00  3.3567076e+00\n",
      "   3.8945081e+00  2.1657908e+00  2.1963000e+00  3.3771214e+00\n",
      "   5.7088666e+00  1.5473627e+00  8.2320347e+00  1.8999032e+00\n",
      "   1.7414838e+00  6.4407535e+00  1.6448420e+00  1.5454828e+00\n",
      "   6.2905488e+00  2.6827557e+00  4.1629486e-03  8.6962347e+00\n",
      "   4.8891017e-01  5.6471930e+00 -1.4448614e+00  1.5574101e+00\n",
      "   6.4672656e+00  4.4773507e+00  5.9550416e-01  2.0646923e+00\n",
      "   4.0310478e+00  1.2326564e+00  5.3333616e+00  6.5887505e-01\n",
      "   2.3340642e+00  4.2608995e+00  4.6076856e+00  3.9546695e+00\n",
      "   5.3625145e+00  6.6695744e-01  2.6954207e+00  7.6534867e+00\n",
      "   4.2593536e+00  5.4696803e+00  3.1902907e+00  2.4635386e+00\n",
      "   6.0107255e+00  4.0257964e+00  1.6205263e+00  2.9009993e+00\n",
      "   6.5430098e+00  8.5166651e-01  5.2409334e+00  5.9600172e+00\n",
      "   3.1494634e+00  4.6478806e+00  4.0147171e+00  4.6516757e+00\n",
      "   7.1875768e+00  5.6210041e+00  3.6913161e+00  5.0923643e+00\n",
      "   3.5747344e+00  6.4143105e+00  6.9997530e+00  5.6617723e+00\n",
      "   4.9946809e+00  6.5004539e+00  1.0332752e+00  4.3091621e+00\n",
      "   5.8811722e+00  2.5408442e+00  4.0814371e+00  7.5783759e-01\n",
      "   1.9487729e+00  3.4726236e+00  5.4055257e+00  3.3697228e+00\n",
      "   5.5773983e+00  1.6804714e+00  2.3141146e+00  6.9723139e+00\n",
      "   1.2854139e+00  5.7428002e+00  3.1185870e+00  1.1039194e+00\n",
      "   6.1910777e+00  5.0023603e+00  4.5557780e+00  9.0324157e-01\n",
      "   3.7985179e+00  3.0252440e+00  4.4545827e+00  3.4515495e+00\n",
      "   1.1855248e+00  5.0594907e+00  2.6215882e+00  4.1255331e+00\n",
      "   4.1996689e+00  2.5554936e+00  3.7196417e+00  8.2189913e+00\n",
      "   4.5418334e+00  4.6698227e+00  2.9448185e+00  2.6322048e+00\n",
      "   4.2232175e+00  3.5961094e+00  1.8231165e+00  3.6941016e+00\n",
      "   6.0284381e+00  2.9915330e+00  6.9166274e+00  2.5334201e+00\n",
      "   2.2356501e+00  4.7279582e+00  1.7171968e+00  3.4591603e+00\n",
      "   7.2248631e+00  9.5535499e-01  2.5831854e+00  6.8234310e+00\n",
      "   2.5719681e+00  4.9482098e+00  1.9476466e+00  2.5684531e+00\n",
      "   6.9416547e+00  4.6017723e+00  1.7735732e+00  1.9867723e+00\n",
      "   3.2013738e+00  1.3677305e+00  2.3174419e+00 -6.7416757e-01\n",
      "   4.5685342e-01  3.8410907e+00  1.5889195e+00  2.6651866e+00\n",
      "   3.5419111e+00 -8.7022758e-01  8.1479585e-01  2.4968963e+00\n",
      "   1.2922571e+00  4.0535398e+00 -9.3666929e-01 -9.0553910e-01\n",
      "   4.3362885e+00  1.2838998e+00  1.1313292e+00  1.1037192e+00\n",
      "  -6.8667084e-01  1.6783208e+00 -5.5994129e-01  1.6421254e+00\n",
      "   1.4386016e+00 -7.8542918e-01  6.6159368e-01  4.6357590e-01\n",
      "  -1.9051817e-01  1.6712315e+00  9.7518057e-01 -7.1315356e-02\n",
      "   7.8570002e-01 -4.7406036e-01  6.9384390e-01  5.0416857e-01\n",
      "  -3.3782774e-01  8.0122747e-02  1.0766541e+00  8.2082540e-01\n",
      "   1.3290024e+00]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c78452dfe94bb18dbaa9c84e81c253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8168709.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 5.2414, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.0063, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.6117, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 1.3982, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0383, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.5248, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.9562, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6784, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5868, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6965892314910889\n",
      "Final parameters: [[ 7.515696    2.6425323   6.287308    2.9793954   1.4373722   6.324554\n",
      "   3.9301708   2.6439548   5.3209424   2.1967535   4.825604    7.4151664\n",
      "   1.7905827   6.5118165   3.0862513   2.8021495   5.7307243   5.0213137\n",
      "   2.6143022   2.9585412   6.091894    2.6876707   7.2104597   2.759556\n",
      "   0.9074222   6.223353    0.24268888  2.9798388   7.6849513   0.7686545\n",
      "   4.073522    5.433632    2.3786879   6.776283    2.6953957   0.9907466\n",
      "   7.0880117   4.3530655   1.7238466   2.6207297   5.2172      2.7176774\n",
      "   5.002707    1.3741993   2.3936534   4.8030806   2.828555    4.9756403\n",
      "   4.985878    3.8336756   2.8411758   6.051674    2.5689394   3.7154634\n",
      "   2.6245985   2.217195    6.2644734   4.01081     4.290853    1.7460244\n",
      "   5.4429336   2.706589    6.369613    1.2136196   3.2157133   4.5298247\n",
      "   2.9933922   1.9559414   5.8535666   1.0614039   1.1479237   6.749362\n",
      "   2.3883748   4.984707    1.5343566   3.5392478   3.7346957   2.140052\n",
      "   2.0484204   3.864901    5.388532    1.6316531   8.14112     2.0411937\n",
      "   1.7630558   6.210278    1.9836804   1.5716931   5.923917    2.7835238\n",
      "   0.21424991  8.442931    0.504717    5.450936   -1.69725     1.7351907\n",
      "   6.080758    4.9524846   0.70672745  2.1626565   3.9802241   1.1678407\n",
      "   5.622898    0.45552695  2.6056178   4.3138113   4.719474    3.884856\n",
      "   5.236497    1.2957593   2.9247406   7.2214627   4.163933    5.549201\n",
      "   3.2358983   2.2124922   5.8755174   3.9474056   1.4136912   2.483768\n",
      "   6.532845    0.57469004  5.24281     5.783733    3.083417    4.7338715\n",
      "   3.837257    4.462705    7.110787    6.321295    3.712257    4.6114697\n",
      "   3.3058836   6.5545115   7.0659046   5.1722617   4.9298787   7.57645\n",
      "   0.9502843   4.194547    5.793822    2.437943    4.216352    0.5920841\n",
      "   2.2751155   3.4817214   5.453768    3.2490544   5.386667    1.8343827\n",
      "   2.218079    6.4854074   1.1743988   5.7705336   3.0561054   1.0937972\n",
      "   5.9858074   4.9529686   4.536674    1.4598548   3.8600855   3.0002859\n",
      "   4.6564937   3.207112    0.8691151   5.2240734   2.6097019   3.9965353\n",
      "   4.213724    2.934008    3.7186508   7.8912234   4.5367227   4.8825183\n",
      "   2.9110332   2.2816916   4.208803    3.7291021   1.8309016   3.7714345\n",
      "   5.9169664   2.87464     6.945781    2.4030118   1.7762697   4.716484\n",
      "   1.7951075   3.4057965   7.02351     1.5641959   2.7071753   6.510612\n",
      "   2.556679    4.9545875   2.10764     2.370311    6.741968    4.613442\n",
      "   1.9852105   2.8528361   3.13454     1.3676097   2.2232907  -0.73206335\n",
      "   0.66605246  3.9248693   1.7163163   2.649223    3.41723    -1.0477707\n",
      "   0.81054264  2.3125322   1.2973356   4.1768627  -0.9448585  -0.9140496\n",
      "   4.2047944   1.0905662   1.2912933   0.7863281  -0.7973469   1.6100801\n",
      "  -0.45638797  1.8157482   1.5809449  -0.74476373  0.5630717   0.49540016\n",
      "  -0.2558472   2.6254606   1.0504712  -0.14043185  0.8945005  -0.4295389\n",
      "   0.6622628   0.7158323  -0.34198937  0.02348564  1.028475    1.0868422\n",
      "   1.2977536 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5e8724360049d9a71ba7293a8622e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 68082568.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 55.6187, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 8.3572, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 6.5806, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 6.5378, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 6.9133, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 5.5608, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 5.8308, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 5.7950, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 6.4680, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6517543196678162\n",
      "Final parameters: [[ 7.189363    2.7568908   6.063767    3.0507228   1.5156734   5.9500446\n",
      "   4.0548544   2.8029153   5.040096    2.2124088   3.786973    7.11362\n",
      "   1.8314431   6.185864    3.3171985   3.2770855   5.4077654   3.7382424\n",
      "   2.742783    2.1922784   5.785195    2.8863106   7.0033555   3.063198\n",
      "   1.4143693   5.867461    0.48246998  3.1832933   7.421374    0.582828\n",
      "   2.8778555   5.342172    2.4242144   6.4526477   2.6937017   1.0120215\n",
      "   6.7640967   3.3700278   1.339061    1.9598069   4.9198976   2.8490636\n",
      "   4.7333264   1.7547727   2.7943094   4.479303    2.970021    5.139809\n",
      "   4.744365    3.8278937   2.5667045   5.73068     2.6080823   3.4436188\n",
      "   2.5905876   2.5939765   6.003321    2.4185784   4.2313995   1.0282645\n",
      "   5.3946548   2.6441917   6.085603    1.3711227   3.8499541   4.4304976\n",
      "   3.2278528   1.8580539   5.8513904   0.91286397  0.5924452   6.4716296\n",
      "   2.2731144   4.937499    1.5968143   3.064606    3.7032025   0.45216912\n",
      "   2.1046925   3.388334    5.5340557   1.479647    8.269699    1.9975055\n",
      "   2.2770638   6.310889    1.7059442   1.4324902   6.126682    2.721682\n",
      "   0.7145959   8.262812    0.15578139  5.5967965  -1.5532566   1.8576219\n",
      "   6.234057    2.763547    0.2666965   2.3185158   4.1072774   0.97597295\n",
      "   5.417633    0.32473803  2.9485414   4.3875165   4.646936    3.6857374\n",
      "   5.381988    1.5769521   3.4266558   7.0654826   3.882036    5.6685157\n",
      "   3.3172832   2.9137053   5.993401    2.2566643   1.300339    2.9248915\n",
      "   6.228121    0.8527373   4.9894013   5.8881335   2.973759    4.3577843\n",
      "   4.1247296   4.6975307   6.83689     6.3887644   3.1529722   4.7594056\n",
      "   3.4855685   6.224485    7.0518055   5.5696235   4.6038117   6.1936255\n",
      "   1.0032412   3.3160996   5.6760387   2.5099566   4.0583806   0.7890589\n",
      "   2.0112693   3.2754474   5.348507    3.351685    5.328651    1.6144311\n",
      "   2.9049132   6.517801    1.0665278   5.6376905   3.054613    1.2971548\n",
      "   5.8725624   3.3597264   4.512298    1.5837406   3.9356103   2.7773297\n",
      "   4.6064425   3.3118632   1.2632184   5.240837    2.5335324   3.8054163\n",
      "   4.231342    3.0087044   4.0965853   7.6249895   4.3964252   4.9101553\n",
      "   2.9562874   2.7603784   4.229024    2.3295834   1.6919217   3.9727132\n",
      "   5.727412    2.9544904   6.530301    2.2884815   2.0499637   4.4883485\n",
      "   2.0704033   3.4867837   6.8568044   1.466081    1.9595044   6.2390313\n",
      "   2.5781782   4.759555    2.0504403   2.4629536   6.552926    3.5061753\n",
      "   1.8811746   1.9384053   3.0957758   1.3773695   2.337693   -0.65249825\n",
      "   0.64943516  3.7874508   1.7219543   2.6766627   3.3807752  -1.0135814\n",
      "   1.2441669   2.4212966   1.1562922   4.091445   -0.94176173 -1.103025\n",
      "   4.124008    0.16675451  1.0247246   1.0437927  -0.8174345   1.6631831\n",
      "  -0.6493782   1.8809891   1.5886436  -0.8370427   0.46359172  0.44563496\n",
      "  -0.2446388   2.6453147   0.8392407  -0.11421064  0.9898037  -0.45309576\n",
      "   0.5845743   1.0036346  -0.4143171   0.5357214   1.3122039   1.0194558\n",
      "   1.3283966 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6a9a7d472f44aba2b261a82a1950ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 785689280.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1280.2855, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 417.8753, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 364.1897, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 289.3607, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 280.8538, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 313.5740, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 330.5112, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 275.7343, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 399.3818, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.7447274923324585\n",
      "Final parameters: [[ 6.40132     2.690162    5.1082163   2.8454876   1.414958    5.052476\n",
      "   3.6692212   2.9221797   4.232763    2.053313    3.308219    6.600107\n",
      "   1.534249    5.1878505   2.9008489   2.715915    4.4235325   5.616676\n",
      "   2.5995748   2.6071124   5.208144    2.6284754   5.978781    4.470692\n",
      "   0.7509929   5.1999445   0.33402306  3.0171452   6.725374    0.47580078\n",
      "   2.9915848   4.9575024   2.004116    5.664089    2.5855365   0.7042172\n",
      "   5.9899015   4.8574266   0.8974075   3.0129745   4.1915126   2.688369\n",
      "   4.431792    1.9354085   2.4054515   3.746236    2.7822394   5.117948\n",
      "   3.9746156   3.8936527   2.27023     4.4475584   2.1042058   2.5699303\n",
      "   2.442206    2.7302635   5.1188984   4.8420677   4.0296254   1.5275606\n",
      "   4.654034    2.6154475   5.278124    1.4159956   3.3952773   3.643707\n",
      "   3.0051486   1.9501024   5.0712686   0.93241155  0.5071334   5.267682\n",
      "   2.2332177   4.0351844   1.4619471   3.282494    2.792712    2.7993355\n",
      "   1.936405    3.8518832   4.393733    1.819534    7.45466     0.8759068\n",
      "   2.089593    5.082487    1.989865    1.8660489   4.929027    2.8258069\n",
      "   0.5588846   6.9112964   0.32101104  4.2465463  -1.5786504   1.8786705\n",
      "   4.894311    5.593638    0.4454543   2.422896    3.343272    0.87960464\n",
      "   5.015791   -0.6161816   2.564237    3.46597     4.4395957   3.829431\n",
      "   4.4755826   1.4482363   3.350414    5.901262    3.644748    4.6206646\n",
      "   2.985966    2.6354983   4.9695396   4.489553    1.1785736   2.4442441\n",
      "   5.745115    0.50177574  4.345177    4.8710456   2.3113942   3.908268\n",
      "   3.8167586   4.3070874   6.3513274   6.0538216   2.8812575   4.5823097\n",
      "   3.101972    5.633336    6.6037536   5.4599566   4.0161715   6.4729424\n",
      "   0.53149605  4.6657987   4.8521485   2.4968343   3.4363978   0.23264398\n",
      "   1.9936637   2.3917053   5.467173    3.4781415   4.471079    1.4983295\n",
      "   2.5377746   5.1743226   1.0388341   4.6078687   2.9990118   1.0776639\n",
      "   4.8392086   5.4855905   4.4191732   0.5706036   3.0636475   2.7718065\n",
      "   3.9230473   2.4690416   1.1340338   4.3533096   2.242509    3.833042\n",
      "   3.4086587   3.1385071   4.0506315   6.704425    4.07337     3.9217296\n",
      "   2.7938578   2.65098     3.2527392   4.2471533   1.5394319   2.9526172\n",
      "   4.8800893   2.909405    6.0382843   2.7623892   1.598122    3.5369444\n",
      "   1.931983    3.580479    6.0090613   1.2663772   1.8952302   5.122411\n",
      "   2.2684474   3.7125542   1.8259308   2.17164     5.5198827   5.1437063\n",
      "   1.5972847   2.102085    2.3541274   1.2119402   1.6536839  -0.3063438\n",
      "   0.5758082   3.10415     1.5144113   2.490293    2.7009602  -0.8909323\n",
      "   0.9856785   1.4406314   1.0025525   3.2554502  -0.8641863  -0.9020468\n",
      "   3.2967956   1.2928551   0.8865577   0.5847688  -0.7979642   1.5606433\n",
      "  -0.5927875   1.460665    1.4338018  -0.82385594  0.5139935   0.40639827\n",
      "  -0.25013027  2.8937716   0.989646   -0.19157952  1.0055147  -0.58398706\n",
      "   0.83187747  0.5126817  -0.41236785 -0.09165955  1.1797491   0.804939\n",
      "   1.1002645 ]]...\n"
     ]
    }
   ],
   "source": [
    "for discount in [0, 1e-3, 0.1, 0.9, 1]:\n",
    "    \n",
    "    shutil.rmtree(f\"NN_Diabetes/Discount/Train_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NN_Diabetes/Discount/Test_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "    initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Discount/Train_{discount}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=discount, writer=writer)\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Discount/Test_{discount}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26d3bfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93493679eaf64f46bd86627471b3c80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 6249670.0000, LR: 1.000e-01\n",
      "Epoch [40/200], Cumulative Loss: 6372057.5000, LR: 1.000e-01\n",
      "Epoch [60/200], Cumulative Loss: 5327980.5000, LR: 1.000e-01\n",
      "Epoch [80/200], Cumulative Loss: 5257824.0000, LR: 1.000e-01\n",
      "Epoch [100/200], Cumulative Loss: 5160894.5000, LR: 1.000e-01\n",
      "Epoch [120/200], Cumulative Loss: 5063762.0000, LR: 1.000e-01\n",
      "Epoch [140/200], Cumulative Loss: 4940141.0000, LR: 1.000e-01\n",
      "Epoch [160/200], Cumulative Loss: 4815151.0000, LR: 1.000e-01\n",
      "Epoch [180/200], Cumulative Loss: 4694249.5000, LR: 1.000e-01\n",
      "Epoch [200/200], Cumulative Loss: 4556052.5000, LR: 1.000e-01\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 4577862.5\n",
      "Final parameters: [[-2139.6338 -2139.5386 -2139.3198 -2138.8416 -2137.8494 -2135.9512\n",
      "  -2138.3613 -2137.4475 -2138.4233 -2136.694  -2139.2332 -2139.5715\n",
      "  -2139.9653 -2136.1748 -2138.9194 -2138.7114 -2138.079  -2138.4326\n",
      "  -2137.951  -2137.8035 -2139.09   -2140.0803 -2136.8918 -2139.1582\n",
      "  -2139.0652 -2138.2954 -2137.416  -2137.475  -2138.5325 -2138.4048\n",
      "  -2136.6843 -2139.1792 -2137.7727 -2139.5076 -2138.3376 -2137.661\n",
      "  -2139.7961 -2137.1643 -2136.9226 -2139.2373 -2138.5125 -2138.8596\n",
      "  -2138.9495 -2139.087  -2137.6628 -2138.4995 -2136.0198 -2137.8816\n",
      "  -2140.302  -2138.7007 -2136.6929 -2139.1409 -2138.4094 -2137.0571\n",
      "  -2138.0676 -2137.999  -2138.1133 -2138.6233 -2138.4175 -2138.7485\n",
      "  -2136.4326 -2138.0864 -2137.5466 -2138.1042 -2137.0232 -2138.5903\n",
      "  -2138.0293 -2138.7468 -2137.7017 -2138.5369 -2138.4253 -2139.3154\n",
      "  -2139.9087 -2137.509  -2138.1326 -2137.633  -2139.471  -2136.1257\n",
      "  -2137.3599 -2138.2466 -2138.7034 -2138.062  -2136.48   -2139.6943\n",
      "  -2138.415  -2140.9182 -2138.3538 -2136.3113 -2138.8074 -2138.5686\n",
      "  -2138.2517 -2137.3462 -2139.8176 -2137.438  -2138.1748 -2138.5435\n",
      "  -2140.4177 -2136.9822 -2138.5498 -2138.8362 -2139.7944 -2138.5723\n",
      "  -2136.708  -2137.0056 -2138.495  -2136.6912 -2137.8696 -2137.3213\n",
      "  -2139.9563 -2139.294  -2138.0962 -2136.4932 -2138.808  -2137.9314\n",
      "  -2137.4817 -2138.6143 -2139.232  -2137.6436 -2137.519  -2138.7336\n",
      "  -2138.6821 -2138.3696 -2138.8193 -2138.1624 -2138.445  -2139.1184\n",
      "  -2138.1755 -2139.332  -2138.212  -2136.944  -2139.4246 -2136.1624\n",
      "  -2139.0503 -2137.34   -2139.0405 -2139.4844 -2137.4514 -2138.737\n",
      "  -2137.4749 -2139.1475 -2139.5757 -2137.4753 -2137.9563 -2140.3416\n",
      "  -2137.6348 -2138.8699 -2137.6526 -2139.2056 -2138.5078 -2138.5032\n",
      "  -2138.2861 -2139.0793 -2137.6995 -2138.5154 -2137.3    -2136.3076\n",
      "  -2137.8103 -2137.5435 -2137.561  -2137.4895 -2137.6016 -2138.5671\n",
      "  -2138.3308 -2138.3262 -2137.3054 -2137.5603 -2137.375  -2138.5325\n",
      "  -2138.628  -2138.998  -2137.5261 -2139.2437 -2138.4011 -2137.6548\n",
      "  -2139.149  -2138.3777 -2138.9775 -2139.354  -2136.2798 -2137.7275\n",
      "  -2139.4568 -2137.8438 -2139.8452 -2138.8142 -2137.9893 -2137.8352\n",
      "  -2139.1287 -2138.547  -2138.0852 -2140.724  -2137.8096 -2138.405\n",
      "  -2139.3853 -2139.215  -2137.3958 -2137.6833 -2141.1118 -2137.3894\n",
      "  -2136.4514 -2137.5625 -2137.2288 -2137.4988 -2138.664  -2137.538\n",
      "  -2138.87   -2139.6973 -2138.9888 -2137.4482 -2136.9473 -2138.1958\n",
      "  -2137.1326 -2138.3132 -2137.0237 -2139.021  -2139.0693 -2135.8472\n",
      "  -2138.11   -2136.6797 -2138.447  -2138.5405 -2138.721  -2139.3818\n",
      "  -2137.3264 -2139.644  -2138.6377 -2138.868  -2137.1553 -2137.1084\n",
      "  -2139.486  -2139.3306 -2137.9236 -2140.0684 -2138.8755 -2137.3738\n",
      "  -2138.5115 -2138.4521 -2139.3394 -2137.9365 -2138.8665 -2138.6252\n",
      "  -2139.6682]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9155fcc38e4f329cb8a8018394a031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 14194.9180, LR: 1.000e-02\n",
      "Epoch [40/200], Cumulative Loss: 21041.3184, LR: 1.000e-02\n",
      "Epoch [60/200], Cumulative Loss: 21892.2852, LR: 1.000e-02\n",
      "Epoch [80/200], Cumulative Loss: 21427.7305, LR: 1.000e-02\n",
      "Epoch [100/200], Cumulative Loss: 21263.6230, LR: 1.000e-02\n",
      "Epoch [120/200], Cumulative Loss: 21721.2500, LR: 1.000e-02\n",
      "Epoch [140/200], Cumulative Loss: 20845.8867, LR: 1.000e-02\n",
      "Epoch [160/200], Cumulative Loss: 20414.2422, LR: 1.000e-02\n",
      "Epoch [180/200], Cumulative Loss: 20626.1328, LR: 1.000e-02\n",
      "Epoch [200/200], Cumulative Loss: 19850.9141, LR: 1.000e-02\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 20628.640625\n",
      "Final parameters: [[-144.21327 -144.17206 -144.6734  -143.44218 -142.86865 -140.9704\n",
      "  -143.38077 -142.70139 -143.10115 -142.27995 -144.42044 -144.14645\n",
      "  -144.47533 -141.77798 -143.93872 -143.94342 -142.59561 -143.60626\n",
      "  -142.97029 -142.82271 -143.40091 -144.40218 -142.87471 -143.45876\n",
      "  -144.08455 -143.3148  -142.4353  -143.44862 -142.75139 -142.9768\n",
      "  -142.25635 -143.75302 -141.96863 -144.0316  -143.35692 -143.0329\n",
      "  -144.15567 -142.75731 -141.94196 -144.25676 -143.11012 -143.50934\n",
      "  -144.29202 -143.71204 -142.68204 -143.51888 -141.03908 -143.34653\n",
      "  -144.85228 -143.27383 -141.90791 -143.71838 -142.93672 -141.57957\n",
      "  -143.08702 -143.26495 -142.62967 -143.83377 -143.4368  -143.76782\n",
      "  -141.0197  -142.73349 -142.92894 -142.72171 -142.04245 -143.60971\n",
      "  -143.04861 -144.18744 -142.25081 -143.11253 -143.47383 -143.89139\n",
      "  -144.43492 -143.05978 -143.15201 -142.85497 -143.98988 -141.16576\n",
      "  -142.37909 -143.26598 -143.14937 -142.54872 -141.99759 -144.1516\n",
      "  -143.43445 -145.93758 -143.37318 -141.98615 -143.20052 -143.13788\n",
      "  -143.52504 -142.55739 -144.19409 -141.95827 -143.19403 -143.60277\n",
      "  -144.93895 -142.2433  -143.56909 -143.85558 -144.23718 -143.05547\n",
      "  -142.31705 -141.45738 -143.51457 -141.71054 -142.88876 -143.02786\n",
      "  -144.34677 -143.86342 -143.40712 -141.9051  -143.1786  -142.45169\n",
      "  -142.50085 -143.70752 -143.75137 -142.94182 -142.53818 -143.75298\n",
      "  -144.49701 -143.91142 -143.3476  -143.63948 -143.46452 -144.13788\n",
      "  -143.19484 -144.19939 -144.05907 -142.30441 -144.28394 -141.40167\n",
      "  -144.40315 -141.89888 -144.0598  -144.37349 -143.03171 -143.7186\n",
      "  -142.49403 -144.16676 -144.00519 -141.95291 -143.32147 -144.7883\n",
      "  -142.65407 -143.88919 -142.67175 -144.7818  -142.89021 -143.07426\n",
      "  -143.45424 -143.65672 -142.05527 -143.03807 -142.31932 -141.39119\n",
      "  -142.32948 -142.69318 -142.58018 -142.50876 -142.18808 -143.21657\n",
      "  -143.65176 -142.94635 -142.32454 -142.57959 -142.39429 -143.75952\n",
      "  -143.34415 -143.57239 -142.66966 -143.81822 -142.92255 -142.17865\n",
      "  -144.16824 -143.85924 -143.49734 -144.62227 -141.29918 -142.74683\n",
      "  -144.05489 -142.49622 -145.18481 -143.43852 -143.00859 -142.85432\n",
      "  -144.14793 -143.98619 -142.64012 -145.30544 -142.8518  -142.97241\n",
      "  -143.91829 -143.73778 -142.4149  -142.90277 -145.639   -142.56898\n",
      "  -141.47069 -142.58173 -143.14284 -142.95235 -143.2587  -143.12766\n",
      "  -143.8894  -144.71654 -144.0082  -142.42613 -142.90479 -143.27254\n",
      "  -142.67172 -143.29637 -143.05702 -143.53856 -144.08868 -142.31967\n",
      "  -143.39594 -142.40482 -143.4664  -143.55989 -143.23642 -143.9375\n",
      "  -141.8688  -144.16422 -143.65701 -143.88736 -142.17459 -141.58281\n",
      "  -143.98938 -143.91035 -142.40683 -144.64008 -143.36255 -141.90219\n",
      "  -143.53091 -142.9154  -143.88647 -142.42406 -143.88591 -143.64468\n",
      "  -143.69785]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51548d7dd1af45688ef6d1022edf952e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 7.5716, LR: 1.000e-03\n",
      "Epoch [40/200], Cumulative Loss: 0.5253, LR: 1.000e-03\n",
      "Epoch [60/200], Cumulative Loss: 0.8197, LR: 1.000e-03\n",
      "Epoch [80/200], Cumulative Loss: 0.8909, LR: 1.000e-03\n",
      "Epoch [100/200], Cumulative Loss: 0.8395, LR: 1.000e-03\n",
      "Epoch [120/200], Cumulative Loss: 0.4654, LR: 1.000e-03\n",
      "Epoch [140/200], Cumulative Loss: 0.7502, LR: 1.000e-03\n",
      "Epoch [160/200], Cumulative Loss: 0.5141, LR: 1.000e-03\n",
      "Epoch [180/200], Cumulative Loss: 0.6688, LR: 1.000e-03\n",
      "Epoch [200/200], Cumulative Loss: 0.9374, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5737376809120178\n",
      "Final parameters: [[ 2.9833179   3.8622794  -0.14068198  4.3857355   3.7798533   5.3756747\n",
      "   3.5110707   3.1401856   4.829986    6.0724025   1.8579866   3.8900626\n",
      "   4.009493    4.063535    2.226865    3.2519507   5.548273    2.7012928\n",
      "   5.7696447   3.7616239   3.0303867   4.399092    1.5447174   5.156945\n",
      "   2.9124982   4.1088667   2.3230379   1.7117053   5.8028502   6.3108554\n",
      "   2.9037166   4.7738843   7.218111    3.0861804   3.3332653   2.8653045\n",
      "   4.8156023   2.4354537   7.0653067   2.6609013   4.387425    3.9325497\n",
      "   0.89347357  3.5290716   3.83759     1.949758    4.3576956   3.2839718\n",
      "   2.3409214   4.5261874   4.9143033   3.8762648   4.979506    3.7063744\n",
      "   2.7167385   4.4561815   4.8690743   3.0209825   5.8837056   2.2031088\n",
      "   6.0937223   5.0301914   2.173312    4.8391094   4.3458376   2.124522\n",
      "   1.5386246   2.2012117   5.268431    5.3659134   2.8786476   4.0128694\n",
      "   3.7797322   2.943897    2.7518315   4.518693    3.8553221   5.2093387\n",
      "   6.888393    2.370473    2.890795    6.158195    2.2448163   4.3562036\n",
      "   2.3325722  -2.6282241   1.200713    3.2301042   5.2672153   5.9915156\n",
      "   1.6603407   6.3648305   4.932696    1.0557357   1.7352475   2.3325803\n",
      "   3.88789     2.96933     6.854594    0.7371184   3.3620892   4.5984087\n",
      "   2.8873014   5.9809785   3.2806811   2.6973019   2.465906    3.3193157\n",
      "   3.0531778   4.2259855   2.9072168   6.06988     4.944807    1.3811487\n",
      "   3.0597193   3.2221835   4.023553    3.3906631   7.121735    2.116032\n",
      "   2.5428395   4.760855    2.9865205   4.8391676   4.057009    4.5758624\n",
      "   6.7508283   1.5565015   4.8223658   6.281743    1.9382148   6.345533\n",
      "   4.509158    4.257961    4.541891    2.9205406   5.8827043   2.6856604\n",
      "   4.9653053   2.4724796   3.5834227   5.173832    1.9224242   2.1178777\n",
      "   4.036575    0.7291186   2.0942302   2.1034088   3.9792907   4.595758\n",
      "   3.4056149   3.7450106   5.576422    1.5738432   3.319553    6.199581\n",
      "   5.042405    4.1868405   6.4808292   3.5270662   5.473539    3.909841\n",
      "   1.5687833   3.9644604   4.402929    2.717198    2.9030037   2.9949253\n",
      "   3.6914372   3.709014    4.35642     3.4333956   4.679858    2.431642\n",
      "   1.7642552   4.3688655   3.6687734   2.5665393   8.062141    3.1989136\n",
      "   3.0387018   5.3821206   0.331918    4.22394     3.6037421   2.978351\n",
      "   0.41142613  2.3133137   4.9810348   2.7919533   3.361756    4.583048\n",
      "   4.44586     2.150892    4.1038365   4.3543787   2.383157    3.7941084\n",
      "   7.128867    3.3845756   2.4472344   2.8410106   0.6117716   2.6140475\n",
      "   1.8473978  -0.6130223  -1.0401691   1.5090415   3.226358    2.1238437\n",
      "   2.3255382   1.5591537   3.9110928   0.4791184   0.81885266  4.779049\n",
      "   2.4414227   2.8434954   1.947263    1.1641368   0.18422621 -0.6179991\n",
      "   2.2858565  -0.7980415   0.27221653  1.349043    2.9822955   1.904329\n",
      "  -0.8798691  -0.3243514   0.79396445 -0.9243812  -0.2787299   2.0482552\n",
      "   0.45387176  0.06153198 -0.3469083   0.7207379  -0.28808427  0.8927337\n",
      "  -0.33927578]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377a191c60304f29b496e519ebbf8cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 823756864.0000, LR: 1.000e-04\n",
      "Epoch [40/200], Cumulative Loss: 317365088.0000, LR: 1.000e-04\n",
      "Epoch [60/200], Cumulative Loss: 154674672.0000, LR: 1.000e-04\n",
      "Epoch [80/200], Cumulative Loss: 85397240.0000, LR: 1.000e-04\n",
      "Epoch [100/200], Cumulative Loss: 51343612.0000, LR: 1.000e-04\n",
      "Epoch [120/200], Cumulative Loss: 35554128.0000, LR: 1.000e-04\n",
      "Epoch [140/200], Cumulative Loss: 23031468.0000, LR: 1.000e-04\n",
      "Epoch [160/200], Cumulative Loss: 16873882.0000, LR: 1.000e-04\n",
      "Epoch [180/200], Cumulative Loss: 10926652.0000, LR: 1.000e-04\n",
      "Epoch [200/200], Cumulative Loss: 9881110.0000, LR: 1.000e-04\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 9630429.0\n",
      "Final parameters: [[14.98499   15.269319  14.378924  16.08163   16.045311  17.987875\n",
      "  15.2940855 16.236506  16.450212  16.892769  14.446253  15.641608\n",
      "  14.664139  16.71841   14.998168  14.882536  16.688196  15.23445\n",
      "  16.40212   16.365612  17.0328    16.229595  17.818953  17.301712\n",
      "  16.433405  17.172335  17.83573   17.489338  17.883286  17.911379\n",
      "  18.264004  17.456264  18.397532  16.400976  17.096066  17.17477\n",
      "  16.501318  17.738607  19.029387  16.461773  16.387943  16.130537\n",
      "  15.042349  16.103569  16.523588  15.693499  17.920176  16.058725\n",
      "  14.755642  16.223083  17.241316  16.35807   16.481653  17.384436\n",
      "  16.045755  15.850873  16.81929   15.294646  16.263405  15.659293\n",
      "  17.892147  16.374832  15.81821   16.602785  16.67412   15.092715\n",
      "  15.40415   14.642344  16.870926  15.860143  14.961566  15.787177\n",
      "  14.4019375 15.256267  15.530553  15.672303  14.931891  17.237486\n",
      "  16.76095   15.60216   15.011135  16.061825  16.098028  14.68081\n",
      "  14.815409  12.376038  14.601062  16.386808  15.429643  15.459508\n",
      "  14.443482  16.573475  13.941225  15.981404  14.9499035 14.070536\n",
      "  13.624815  15.695173  15.104863  14.499325  14.263325  15.757012\n",
      "  16.1475    17.528982  14.940081  16.829361  15.312624  15.698059\n",
      "  14.4426985 14.980538  14.916636  17.45137   15.291735  15.720232\n",
      "  15.86645   14.311607  15.055846  15.349858  16.384243  14.882188\n",
      "  23.645441  24.512451  24.610855  24.678734  24.802156  24.36336\n",
      "  25.035137  24.11381   24.68953   25.887964  24.079912  26.815794\n",
      "  23.60438   26.093988  24.266922  24.06698   25.38203   24.770052\n",
      "  25.5319    23.84526   14.804613  16.945383  15.495557  14.28043\n",
      "  15.954946  14.7198715 15.632056  14.252243  15.981624  15.876143\n",
      "  15.168573  15.74372   16.670172  15.327108  16.176292  17.065557\n",
      "  16.598377  15.897737  16.540907  16.427908  16.498848  15.709935\n",
      "  14.867106  16.154053  16.134743  15.892853  15.793895  14.677817\n",
      "  15.760416  15.252615  15.685317  15.568273  15.687079  15.953098\n",
      "  14.195131  14.760301  15.282786  13.842158  17.629566  15.910803\n",
      "  14.878543  16.624043  13.584392  15.808638  15.547947  15.693487\n",
      "  14.165088  14.867104  16.492588  13.676938  15.602008  16.526794\n",
      "  14.938985  14.557889  16.062054  15.649121  13.298208  15.982933\n",
      "  17.65989   16.271605  12.508459  13.217208  11.708561  13.4778805\n",
      "  11.356207  10.717722  10.999693  12.910717  13.909719  12.311173\n",
      "  13.318851  13.084664  13.082337  11.358158  11.098908  14.683884\n",
      "  12.510457  13.775948  12.186383  11.762456  11.685726  10.955115\n",
      "  13.178152  10.674613  11.5023575 11.635082  13.053876  13.243814\n",
      "  10.789675  11.111747  12.393245  10.30539   11.471237  13.163934\n",
      "  11.70761   11.753902  11.082065  12.337896  11.00503   11.373255\n",
      "  11.8843355]]...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "\n",
    "    shutil.rmtree(f\"NN_Diabetes/Lr/Train_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NN_Diabetes/Lr/Test_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    lr = 10**(-i)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "    initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=lr)\n",
    "\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Lr/Train_e{i}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Lr/Test_e{i}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e417c23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a280a3ea121f40f094801ced72b563e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/2000], Cumulative Loss: 9881110.0000, LR: 1.000e-04\n",
      "Epoch [400/2000], Cumulative Loss: 1246417.8750, LR: 1.000e-04\n",
      "Epoch [600/2000], Cumulative Loss: 382936.7188, LR: 1.000e-04\n",
      "Epoch [800/2000], Cumulative Loss: 148605.0156, LR: 1.000e-04\n",
      "Epoch [1000/2000], Cumulative Loss: 72151.5234, LR: 1.000e-04\n",
      "Epoch [1200/2000], Cumulative Loss: 32419.0684, LR: 1.000e-04\n",
      "Epoch [1400/2000], Cumulative Loss: 22371.1641, LR: 1.000e-04\n",
      "Epoch [1600/2000], Cumulative Loss: 4474.1758, LR: 1.000e-04\n",
      "Epoch [1800/2000], Cumulative Loss: 2441.9375, LR: 1.000e-04\n",
      "Epoch [2000/2000], Cumulative Loss: 1084.0387, LR: 1.000e-04\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1392.8455810546875\n",
      "Final parameters: [[ 4.4197307   9.92952     5.375772    7.0339346  20.885983    4.8713417\n",
      "   2.7107177   4.1928186   4.885844    8.907608    4.54261    21.969252\n",
      "   4.549441    8.206536    5.313111   20.189985    5.13701     6.5987716\n",
      "   6.24627     5.765709    5.3153024  12.793395    5.91939     6.311839\n",
      "  17.92428     6.3287153   5.7556505   5.735578    6.522535   12.861653\n",
      "   4.1830645  19.373384    4.9254303   9.535274    7.191734   19.263844\n",
      "   6.2421875   5.8997555   6.922664    5.9850597   6.2865353   8.851097\n",
      "   2.7731006   6.86293    20.307165    3.7585325   5.5090637   3.565444\n",
      "   6.852719    8.287176    4.9737477  18.595816    6.328521    8.619517\n",
      "   5.906531   19.362478    5.9640317   6.400286    4.9273353   6.324533\n",
      "   6.6769934   9.596638    6.4010396   5.4251094  21.87996     4.5992255\n",
      "   4.613698    4.6637855   5.68208    11.616599    5.2786574  20.10108\n",
      "   5.2331576   5.1902523   5.827081   20.381353    6.121332    5.146327\n",
      "   7.192719    3.9408972   5.6316743  10.464844    5.895172    3.4678073\n",
      "  21.153683    3.433861    4.1244864   5.55609     3.2546387   8.15448\n",
      "   3.7838843  22.262999    5.0214915   7.633179    3.34       20.913404\n",
      "   5.7590914   4.2271223   6.04464     3.8141646   4.4527135  10.253082\n",
      "   5.490072    3.1719415  19.93142     4.5465994   4.0515      4.6089697\n",
      "   4.8735094   8.146483    5.3204956  20.580097    4.7066064   7.6313376\n",
      "   3.6919997  20.364548    6.0914373   3.4203687   6.900019    5.5562935\n",
      "  11.97043    15.145327   12.057724   12.459736   13.691461   12.602442\n",
      "  11.659591   11.341086   13.361637   12.608922   13.565283   12.787704\n",
      "  11.706997   14.016109   12.2348995  13.205321   12.501382   11.712908\n",
      "  13.662142   10.512092    3.77322    10.176089    5.6958447   5.2463665\n",
      "  20.357779    5.254192    5.094125    3.353041    6.8365755   8.138586\n",
      "   5.182269   18.107222    6.89368    10.437743    5.820027   20.983334\n",
      "   5.2550406   4.264535    9.36476     6.271902    3.3101592   9.211728\n",
      "   7.5302835   4.4727387  20.318241    3.752058    4.447556    6.0585165\n",
      "   4.956522    9.528886    4.8214793  20.282593    5.10809     8.920586\n",
      "   5.5936885  20.916779    5.5338936   6.294393    5.3142033   6.2289076\n",
      "   3.125778    9.279411    5.810116    4.532481   19.611126    4.995643\n",
      "   3.3018186   2.2959998   4.1558423  11.395317    4.604146   21.090939\n",
      "   3.1364903   8.663986    7.530197   20.276976    4.0915904   6.215032\n",
      "   5.787273    4.721078    1.3957248   7.9526224   2.613111    2.8260813\n",
      "  19.764067    1.4653832   1.335665    2.3557858   2.6232352   5.8989058\n",
      "   1.4201665  20.527548    4.646786    7.0595694   2.6009946  20.933495\n",
      "   3.0133855   2.2300477   5.6964126   3.2473862   2.0717688   0.25982964\n",
      "   1.6800889   2.2794666  -2.0334225   2.9891813   3.211999    2.7046928\n",
      "   1.5497298   0.4201785   2.0209496  -0.15437861  1.1630772   0.40863457\n",
      "   1.3237605  -0.22805208  1.7022496   2.3690343   0.60024464  2.1112733\n",
      "   2.7929246 ]]...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.0001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "writer = SummaryWriter(\"NN_Diabetes/Lr/Train_e4_Long\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=2000, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(\"NN_Diabetes/Lr/Test_e4_Long\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f763fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b65b5ad5fc4d6bbbf4711401488f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 7.5716, LR: 1.000e-03\n",
      "Epoch [40/200], Cumulative Loss: 0.5253, LR: 3.162e-04\n",
      "Epoch [60/200], Cumulative Loss: 0.5635, LR: 3.162e-04\n",
      "Epoch [80/200], Cumulative Loss: 0.6683, LR: 1.000e-04\n",
      "Epoch [100/200], Cumulative Loss: 0.4577, LR: 1.000e-04\n",
      "Epoch [120/200], Cumulative Loss: 0.5452, LR: 3.162e-05\n",
      "Epoch [140/200], Cumulative Loss: 0.5339, LR: 3.162e-05\n",
      "Epoch [160/200], Cumulative Loss: 0.7056, LR: 1.000e-05\n",
      "Epoch [180/200], Cumulative Loss: 1.8736, LR: 1.000e-05\n",
      "Epoch [200/200], Cumulative Loss: 0.3772, LR: 3.162e-06\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6363527774810791\n",
      "Final parameters: [[ 2.8847816   5.201524    0.12562682  5.690243    3.7675788   5.326632\n",
      "   3.0322957   3.411771    6.1306696   7.198477    1.9685651   5.1859236\n",
      "   5.302169    4.1409636   1.8312805   3.2916188   6.9305086   2.8173516\n",
      "   7.182851    3.118078    3.0049963   5.6659985   1.7903786   6.3867702\n",
      "   3.0520074   4.2430196   3.684196    2.0288446   7.0280037   7.519516\n",
      "   3.136249    6.3169765   8.489915    2.8443139   1.3786868   2.9365046\n",
      "   6.215981    2.6715107   8.550735    1.9952551   4.051559    5.550188\n",
      "   1.3762227   5.1211777   3.7941482   2.0323396   4.327264    3.1674554\n",
      "   3.929958    5.8744864   4.661954    5.1790614   6.5261755   3.9856493\n",
      "   2.8899639   4.1811852   6.519982    2.7778413   7.3866034   1.6374062\n",
      "   5.765613    6.6322317   2.039308    6.410855    4.412134    2.2626548\n",
      "   1.8510617   2.0483294   6.836633    6.39923     2.6801908   5.185065\n",
      "   5.319801    3.5245488   1.8729872   4.2997084   5.5269427   5.018071\n",
      "   8.52463     2.2654305   2.601165    7.8215075   1.7778846   5.9950266\n",
      "   1.5222542  -2.3143942  -0.45706022  3.0749326   6.9008164   7.5335464\n",
      "   1.4710028   7.626655    6.5676      1.4989312   1.6206326   2.0293782\n",
      "   5.6497316   2.7871332   8.570977    0.21640868  3.4016585   5.905758\n",
      "   3.006273    7.2680187   3.0225239   2.7873542   1.5028135   3.598006\n",
      "   4.3324842   5.167543    3.1427922   7.195017    6.231094    2.0876572\n",
      "   2.733875    3.3779225   5.4104404   3.6358333   8.261106    2.1787298\n",
      "   2.7769933   5.966949    2.9804132   6.0241814   4.428572    5.155393\n",
      "   6.544729    1.9585867   6.000519    7.534758    2.2712617   7.8526397\n",
      "   5.7385025   4.943253    7.1739564   3.1290567   7.170444    3.0252092\n",
      "   6.722217    2.8645642   3.5168478   6.632989    2.2873936   3.5426576\n",
      "   3.829647    1.0089954   2.1266289   2.2696648   5.3996925   5.923236\n",
      "   3.4859307   5.049355    6.9915557   1.6643659   2.849626    6.1929655\n",
      "   6.5694723   4.2721543   7.8706875   3.6405272   5.259101    5.4696774\n",
      "   1.8015226   5.484941    4.0664697   3.0676117   2.0878768   3.0384371\n",
      "   5.209461    5.359711    4.330631    4.509534    6.213873    2.7984233\n",
      "   1.9952669   4.217833    5.3824534   2.5451183   9.522435    3.256952\n",
      "   3.0049822   6.7685075   0.11119577  5.5895224   3.9362786   3.11802\n",
      "   1.0894897   2.3979185   6.343269    4.202535    3.4401176   5.7735806\n",
      "   5.777725    2.5880487   2.9473898   4.369895    3.7728832   3.8828\n",
      "   8.48854     3.3178256   2.3838637   4.2764993   0.8468301   4.0018435\n",
      "   1.7574973  -0.53179836 -0.92364025  1.6820475   4.608758    3.7252192\n",
      "   2.4410074   2.9160156   5.3248744   0.60935163 -0.7573351   4.769636\n",
      "   3.992531    2.9655352   3.0448346   1.1627594   0.46653485 -0.5600327\n",
      "   2.3939066  -0.8206838   0.4275008   1.2206572   1.8213115   2.080086\n",
      "  -0.8170775  -0.32008398  1.1121477  -0.9646491  -0.23110852  2.0246298\n",
      "   0.8391494   0.1434464  -0.2736122   0.936594   -0.29114574  1.2038246\n",
      "  -0.11994503]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes/Lr/Train_Step\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes/Lr/Test_Step\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(meta_optimizer, step_size=40, gamma=np.sqrt(0.1))\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Lr/Train_Step\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer, scheduler=scheduler)\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Lr/Test_Step\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c267a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f264d11b3e374cc7be0d6dbe312a54aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 49678488.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 64455.5508, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 3.5796, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.0915, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 1.1040, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 2.4050, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.4086, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.7440, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.8023, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.3376, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.8964078426361084\n",
      "Final parameters: [[ 4.333922   15.034754   13.66389    14.237004    3.0719428   2.0987694\n",
      "   3.0590067   3.9937625  14.728879    1.8864397   5.748864    1.6719899\n",
      "   3.6101408   3.6434765   3.923642    3.968352   15.279895    5.018868\n",
      "   2.9828792   3.8431978   3.0129983  14.562385   14.268546   13.784624\n",
      "   2.5398457   1.4271225   3.5619824   6.438787   12.611475    2.3104882\n",
      "   6.3358693   6.262538    3.8532155   3.267823    3.9552207   3.3307333\n",
      "  15.616351    5.784325    2.2612224   2.6658309   2.8070412  13.625981\n",
      "  12.273794   15.098562    4.41223     3.3481722   7.765068    4.3105793\n",
      "  15.572577    2.078752    4.724       7.0605593   3.2835307   3.6716807\n",
      "   3.8167672   4.597835   14.662445    3.384881    4.451996    2.78635\n",
      "   3.7865522  12.720732   14.652348   16.099234    3.4199548   1.4784786\n",
      "   4.5249543   4.59589    14.645419    4.8054724   1.4422783   4.832049\n",
      "   3.858333    4.8223424   4.1791434   3.1739824  13.742615    4.648925\n",
      "   2.7275958   3.5596135   0.2205348  14.968422   13.777108   14.863632\n",
      "   2.4227958   0.3335142   0.4725182   3.0505943  15.439736    0.41596603\n",
      "   1.8926758   3.6267047   2.694456    2.8714054   1.6292721   0.7079338\n",
      "  16.765348    3.3409498   1.8155378   0.8341959   4.0005755  13.207624\n",
      "  14.870413   12.883663    4.3913627   3.4765058   4.8712087   4.7279882\n",
      "  12.246937    2.8745813   2.1222217   4.1109133   4.0434284   6.138626\n",
      "   3.542505    3.7661436  14.657409    4.2335224   3.1761115   4.7304854\n",
      "   4.2663827  12.767529   12.424916   11.776242    4.949262    2.5341425\n",
      "   4.870744    6.6943836  11.880258    4.734645    5.7369475   7.214332\n",
      "   6.7580023   6.4750543   6.2350183  12.005497   13.492491    7.295415\n",
      "   4.9866796   4.787972    6.0270147  11.439343   11.895659   11.691507\n",
      "   5.013104    2.8116446   5.20218     5.310001   13.374496    4.977497\n",
      "   2.0760288   5.8889756   3.3865852   5.307256    4.3642783   4.731903\n",
      "  11.838233    3.2389834   4.2686      4.81003     4.8558135   9.927202\n",
      "  13.40932    14.193243    3.4282367   2.204739    3.407484    3.6593199\n",
      "  11.807404    3.6593575   2.793933    4.5887785   5.094972    4.3759427\n",
      "   3.4385114   4.261754   14.812556    2.845943    3.1896656   4.340284\n",
      "   3.2684047  12.527743   15.73892    12.64037     3.5597157   0.57877755\n",
      "   4.241694    4.091629   13.42207     3.1180024   5.825806    6.2769475\n",
      "   2.4308462   2.668991    2.2628067   3.542661   15.600931    2.4052122\n",
      "   2.8924968   2.8455365   2.3778906  11.34965    12.372046   11.444302\n",
      "   2.4083695   1.9933385   3.1766303   3.6905165  10.531372    3.091468\n",
      "   0.84951437  4.0084987   1.8833352   3.9777112   1.9366171  -1.3627754\n",
      "  12.410322    0.8997606   2.424569    2.4374242   1.3637978  -1.5632923\n",
      "  -0.7115727  -0.77156556  1.6365582   2.4218774   0.8619645   0.6242376\n",
      "  -0.3425696   2.2994487   1.6334168   0.43317932  0.49377352  0.6193102\n",
      "   0.76809746  2.2432497  -0.32197225  0.48294964  1.8356622   1.6110898\n",
      "   1.5726053 ]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes/Comparison/Train_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes/Comparison/Test_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Comparison/Train_Right_Horizon\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=800, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Comparison/Test_Right_Horizon\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a43a5",
   "metadata": {},
   "source": [
    "## NN Optimizee - Multi Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e125cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa6b1c156ae4c3db1caed24da209bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 11.6563, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0274, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0227, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0496, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0429, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0224, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0228, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0287, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0519, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0221, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.055267542600631714\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NNDiabetes_Multi/Train_Dir_11\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NNDiabetes_Multi/Test_Dir_11\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "\n",
    "initializer = Data_Initializer(XYNNOptimizee, kwargs, num_optims=10, subset_size=40)\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(\"NNDiabetes_Multi/Train_Dir_11\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(\"NNDiabetes_Multi/Test_Dir_11\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2d43a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1022d03e149f4b518d6b61fd1fd5d04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 9.3712, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.6942, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.7742, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.0797, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.9257, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8957, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6777, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.8461, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.2062, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.3523, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.2804855108261108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12cb0cacdd34c9587705d1d7c18e40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1.8720, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.0809, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8563, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5012, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7889, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0272, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6731, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6559, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.9176, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5971, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5302165746688843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3edd1303fa48a88ce399df60415e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.8952, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.2487, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8769, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.5273, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8627, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7031, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8705, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6246, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6137, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5438, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.8982424139976501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa48d5f64fd44846b5e341f42b0c9e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.7295, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.2545, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.9127, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.0445, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8711, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.5838, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.4326, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.7885, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.3083, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.0391, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.7224278450012207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f729e321bedc412cbdc2d5eb2515395c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.6271, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 3.6102, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.9125, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.6189, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8111, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.6105, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.9432, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.4965, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6080, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5315, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6149126887321472\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(40, 140, 20):\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/SubSize/Train_{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/SubSize/Test_{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "\n",
    "    initializer = Data_Initializer(XYNNOptimizee, kwargs, num_optims=5, subset_size=i)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"NNDiabetes_Multi/SubSize/Train_{i}\") \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(f\"NNDiabetes_Multi/SubSize/Test_{i}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1afdce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527c732b197846569a916380c3a09e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.4605, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.4844, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.9941, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.6039, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.4922, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.6912, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8025, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.5898, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.0929, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.0338, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.0842245817184448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4421d18e524b738d186130118024da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 21.8032, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.4232, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.9033, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5923, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.5541, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7923, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 9.1531, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 4.8190, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.7431, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 4.5322, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.9238519072532654\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.6,0.8,0.2):\n",
    "    i = float(str(f\"{i:.2f}\"))\n",
    "    i_str = str(i).replace(\".\", \"\")\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet/Train_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet/Test_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "    dist = Dirichlet(torch.tensor([i, i]))\n",
    "\n",
    "    initializer = Data_Initializer(XYNNOptimizee, kwargs, distribution=dist, num_optims=5, subset_size=60)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet/Train_Dir_\" + i_str) \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet/Test_Dir_\" + i_str)\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0eda44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343cb129f07f4859bb6bd229012a2b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 3.8885, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 3.1569, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6032, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.3146, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.5266, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8746, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8030, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.4714, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.5087, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 2.7344, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6261980533599854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56edfaaf893345c9a1913b4ad34b94d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.9358, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.0775, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6241, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 2.1537, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7499, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7232, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3988, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.9394, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.2381, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.5663, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.9663375020027161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddd0b10e86f43d1aec24636871ade55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 5.0868, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 2.9498, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.1802, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.7046, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.4853, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.7676, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.6014, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6284, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 2.6566, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.9823, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.8411714434623718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1243e674aa384ff9b681f44cd8077770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1.8720, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.0809, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8563, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5012, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7889, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0272, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6731, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6559, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.9176, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5971, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5302165746688843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2396ec1de44c4132b68ea17f2e079219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 28.4856, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.6673, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.1037, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 2.5722, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 6.1662, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 7.6745, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.2455, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 4.9773, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 6.8608, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.6939, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 3.2333266735076904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488c47bb049542ec9a5edbd33208ba79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1.9181, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.2097, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.2240, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.4632, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6835, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7340, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.5765, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.8840, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.8503, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.4778, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5290380120277405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7025531e1bb549418faaea1c156908a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.0031, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 13.8098, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 4.0023, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.3252, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 3.0893, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0997, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.9132, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.5231, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.6686, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 2.2863, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.0342798233032227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a395f4e6fb045728a600335b42ac88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 4.2231, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.5239, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8191, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.1897, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8875, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8233, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8568, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.3659, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.3039, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.9797, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.1990711688995361\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.25,2.25,0.25):\n",
    "    i = float(str(f\"{i:.3f}\"))\n",
    "    i_str = str(i).replace(\".\", \"\")\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet2/Train_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet2/Test_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "    dist = Dirichlet(torch.tensor([i, i]))\n",
    "\n",
    "    initializer = Data_Initializer(XYNNOptimizee, kwargs, distribution=dist, num_optims=5, subset_size=60)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet2/Train_Dir_\" + i_str) \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet2/Test_Dir_\" + i_str)\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
