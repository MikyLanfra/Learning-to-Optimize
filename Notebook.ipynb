{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "from src.optimizee import *\n",
    "from src.torch_utils import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d42487b3803441aaef698a4f858c27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Final parameters: [[1.0655398  1.6188935  0.6811273  1.0631032  0.6347599  0.7418846\n",
      "  0.98628175 0.6697677  0.81006145 1.5054967 ]]\n",
      "Final loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 10\n",
    "W = np.random.randn(n, n)\n",
    "theta0 = np.ones((n, 1))\n",
    "\n",
    "# Create the quadratic function optimizee\n",
    "optimizee = QuadraticOptimizee(W, theta0)\n",
    "params = optimizee.get_initial_params()\n",
    "\n",
    "optimizer = optim.Adam([params], lr=0.1)\n",
    "writer = SummaryWriter(\"runs/Adam\")\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "    optimizer.zero_grad()\n",
    "    loss = optimizee.compute_loss(params)\n",
    "    writer.add_scalar(\"Loss\", loss,epoch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Final parameters and loss\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final parameters: {params.detach().numpy().T}\")\n",
    "print(f\"Final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMOptimizer(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based optimizer as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=20, preproc=True, preproc_factor=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preproc = preproc\n",
    "        self.preproc_factor = torch.tensor(preproc_factor)\n",
    "        self.preproc_threshold = float(torch.exp(self.preproc_factor))\n",
    "        \n",
    "        self.input_size = 2 if preproc else 1\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, 2, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM optimizer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "            hidden_state (tuple): Hidden state of the LSTM (h, c).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output updates of shape (batch_size, sequence_length, 1).\n",
    "            tuple: Updated hidden state.\n",
    "        \"\"\"\n",
    "        if self.preproc: x = self.preprocess_gradients(x)\n",
    "\n",
    "        # print(\"Preprocess Shape\", x.shape)\n",
    "        out, hidden_state = self.lstm(x, hidden_state)\n",
    "        out = self.output_layer(out)\n",
    "        # print(\"Output Shape\", out.shape)\n",
    "        return out, hidden_state\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess_gradients(self, gradients):\n",
    "        \"\"\" Applies log transformation & sign extraction to gradients, moving to CUDA if available. \"\"\"\n",
    "\n",
    "        gradients = gradients.data  # Extract raw gradient data\n",
    "        \n",
    "        param_size = gradients.size(0)\n",
    "        preprocessed = torch.zeros(param_size, 2)\n",
    "\n",
    "        # Identify large gradients (above threshold)\n",
    "        keep_grads = (torch.abs(gradients) >= self.preproc_threshold).squeeze()\n",
    "        \n",
    "        # Log transformation for large gradients\n",
    "        preprocessed[:, 0][keep_grads] = (torch.log(torch.abs(gradients[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
    "        preprocessed[:, 1][keep_grads] = torch.sign(gradients[keep_grads]).squeeze()\n",
    "\n",
    "        # Direct scaling for small gradients\n",
    "        preprocessed[:, 0][~keep_grads] = -1\n",
    "        preprocessed[:, 1][~keep_grads] = (float(torch.exp(self.preproc_factor)) * gradients[~keep_grads]).squeeze()\n",
    "\n",
    "        return torch.tensor(preprocessed)\n",
    "    \n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # Initialize hidden & cell states for LSTM (one per parameter)\n",
    "        self.h0 = to_cuda(torch.zeros(2, self.hidden_size))\n",
    "        self.c0 = to_cuda(torch.zeros(2, self.hidden_size))\n",
    "        return (self.h0, self.c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(lstm_optimizer, meta_optimizer, optimizee_class, optimizee_kwargs, num_epochs=500, time_horizon=200, discount=1, writer=None, scheduler=None):\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(meta_optimizer, factor=1.0, total_iters=num_epochs)\n",
    "    \n",
    "    with tqdm(range(num_epochs), desc=\"Training Progress\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            # Initialize optimizee parameters\n",
    "            optimizee = optimizee_class(**optimizee_kwargs)\n",
    "            params = optimizee.get_initial_params()\n",
    "            hidden_state = lstm_optimizer.initialize_hidden_state()\n",
    "\n",
    "            cumulative_loss = None\n",
    "            for t in range(time_horizon):\n",
    "                loss = optimizee.compute_loss(params)\n",
    "                cumulative_loss = loss*discount**(time_horizon) if cumulative_loss is None else cumulative_loss + loss*discount**(time_horizon-t)\n",
    "\n",
    "                # Compute gradients of the loss w.r.t. the parameters\n",
    "                grad_params = torch.autograd.grad(loss, params, create_graph=True)[0]\n",
    "                grad_params = grad_params.detach()\n",
    "                # print(\"Grads\", grad_params.shape)\n",
    "                updates, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "                params = params - updates  # Update parameters\n",
    "\n",
    "            # Backpropagation through time (BPTT)\n",
    "            # cumulative_loss = loss\n",
    "\n",
    "            if writer: writer.add_scalar(\"Loss\", cumulative_loss, epoch)\n",
    "            meta_optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(lstm_optimizer.parameters(), 1)\n",
    "            meta_optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=cumulative_loss.item())\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Cumulative Loss: {cumulative_loss.item():.4f}, True Loss: {np.linalg.norm(params.detach().numpy() - theta0):.4f}\")\n",
    "                print(f\"Final parameters: {params.detach().numpy().T}\")\n",
    "                \n",
    "    print(\"\\nTraining complete!\")\n",
    "    return lstm_optimizer\n",
    "\n",
    "\n",
    "def test_LSTM(lstm_optimizer, optimizee_cls, optimizee_kwargs, time_horizon=200, writer=None):\n",
    "    optimizee = optimizee_cls(**optimizee_kwargs)\n",
    "    params = optimizee.get_initial_params()\n",
    "    hidden_state = lstm_optimizer.initialize_hidden_state()\n",
    "    for t in range(time_horizon):\n",
    "        loss = optimizee.compute_loss(params)\n",
    "        if writer: writer.add_scalar(\"Loss\", loss, t)\n",
    "\n",
    "        grad_params = torch.autograd.grad(loss, params, create_graph=True)[0]\n",
    "        grad_params = grad_params.detach()\n",
    "\n",
    "        updates, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "        params = params - updates \n",
    "\n",
    "    print(f\"Final parameters: {params.detach().numpy().T}\")\n",
    "    print(f\"Final loss: {optimizee.compute_loss(params).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W [[ 1.76405235  0.40015721  0.97873798  2.2408932   1.86755799 -0.97727788\n",
      "   0.95008842 -0.15135721 -0.10321885  0.4105985 ]\n",
      " [ 0.14404357  1.45427351  0.76103773  0.12167502  0.44386323  0.33367433\n",
      "   1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502  2.26975462 -1.45436567\n",
      "   0.04575852 -0.18718385  1.53277921  1.46935877]\n",
      " [ 0.15494743  0.37816252 -0.88778575 -1.98079647 -0.34791215  0.15634897\n",
      "   1.23029068  1.20237985 -0.38732682 -0.30230275]\n",
      " [-1.04855297 -1.42001794 -1.70627019  1.9507754  -0.50965218 -0.4380743\n",
      "  -1.25279536  0.77749036 -1.61389785 -0.21274028]\n",
      " [-0.89546656  0.3869025  -0.51080514 -1.18063218 -0.02818223  0.42833187\n",
      "   0.06651722  0.3024719  -0.63432209 -0.36274117]\n",
      " [-0.67246045 -0.35955316 -0.81314628 -1.7262826   0.17742614 -0.40178094\n",
      "  -1.63019835  0.46278226 -0.90729836  0.0519454 ]\n",
      " [ 0.72909056  0.12898291  1.13940068 -1.23482582  0.40234164 -0.68481009\n",
      "  -0.87079715 -0.57884966 -0.31155253  0.05616534]\n",
      " [-1.16514984  0.90082649  0.46566244 -1.53624369  1.48825219  1.89588918\n",
      "   1.17877957 -0.17992484 -1.07075262  1.05445173]\n",
      " [-0.40317695  1.22244507  0.20827498  0.97663904  0.3563664   0.70657317\n",
      "   0.01050002  1.78587049  0.12691209  0.40198936]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dde06c5d93f409699566d16bbc6ff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\AppData\\Local\\Temp\\ipykernel_2248\\3110683188.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(preprocessed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], Cumulative Loss: 678.0087, True Loss: 4.4608\n",
      "Final parameters: [[-1.7408055  -0.6848107   1.9327322   1.7622056   1.1565609   2.144124\n",
      "  -0.38439995 -0.5949832  -0.39030692  0.3918745 ]]\n",
      "Epoch [100/500], Cumulative Loss: 1.7167, True Loss: 0.6350\n",
      "Final parameters: [[1.2065135  1.3939402  0.7177888  1.0273954  1.0677747  1.0888687\n",
      "  0.8006659  0.76449645 0.997844   1.1313748 ]]\n",
      "Epoch [150/500], Cumulative Loss: 40.3085, True Loss: 2.4126\n",
      "Final parameters: [[1.3239648  0.49238402 0.9104134  0.80154693 0.49345618 0.6382917\n",
      "  1.8699192  1.1657548  0.9001383  3.0564482 ]]\n",
      "Epoch [200/500], Cumulative Loss: 1.7670, True Loss: 1.2464\n",
      "Final parameters: [[1.1339531  0.5654306  0.88947594 1.0246433  1.7561078  1.4821682\n",
      "  0.830778   1.1263865  1.392892   0.4249072 ]]\n",
      "Epoch [250/500], Cumulative Loss: 42.9599, True Loss: 2.8247\n",
      "Final parameters: [[ 1.148145    0.8831059   0.8568987   1.078455   -0.02480925  0.47527987\n",
      "   1.9699063   0.9989253   0.73728865  3.3624678 ]]\n",
      "Epoch [300/500], Cumulative Loss: 1.6532, True Loss: 1.2365\n",
      "Final parameters: [[1.2040861  1.7358679  0.25035948 1.1009665  1.0724372  1.0168008\n",
      "  0.81635094 0.52912074 1.075464   1.3271636 ]]\n",
      "Epoch [350/500], Cumulative Loss: 2.4389, True Loss: 1.0478\n",
      "Final parameters: [[0.7671151  0.58188146 1.7064842  0.99447924 0.75707734 0.8339428\n",
      "  1.3266811  1.3822538  0.8431624  0.924767  ]]\n",
      "Epoch [400/500], Cumulative Loss: 0.9478, True Loss: 0.9290\n",
      "Final parameters: [[1.1387553  1.621276   0.5598213  1.1153634  0.9157734  0.9331936\n",
      "  0.8115746  0.60395217 0.9674532  1.2138604 ]]\n",
      "Epoch [450/500], Cumulative Loss: 1.2858, True Loss: 1.8581\n",
      "Final parameters: [[1.141921   2.1840243  0.18043968 1.0990053  0.52600914 0.645149\n",
      "  0.79185253 0.29637244 0.8387598  1.6588409 ]]\n",
      "Epoch [500/500], Cumulative Loss: 3.0273, True Loss: 1.6647\n",
      "Final parameters: [[1.2862097  0.82296365 0.6166592  1.0065498  1.924469   1.5674181\n",
      "  0.5554389  1.0611396  1.6604403  0.16531172]]\n",
      "\n",
      "Training complete!\n",
      "Final parameters: [[0.87988883 0.04210973 1.4925071  0.843223   1.370276   1.2592444\n",
      "  1.0072143  1.470411   1.2052342  0.2788937 ]]\n",
      "Final loss: 0.2202\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 10\n",
    "W = np.random.randn(n, n)\n",
    "theta0 = np.ones((n, 1))\n",
    "print(\"W\", W)\n",
    "\n",
    "lstm_optimizer = LSTMOptimizer()\n",
    "writer = SummaryWriter(\"train/LSTM\")\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.0001)\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, QuadraticOptimizee, {\"W\": W, \"theta0\": theta0}, num_epochs=500, time_horizon=500, discount=0.9, writer=writer)\n",
    "writer = SummaryWriter(\"runs/LSTM\")\n",
    "test_LSTM(lstm_optimizer, QuadraticOptimizee, {\"W\": W, \"theta0\": theta0}, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f3a7cdbb494b308cadae1750eaf595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\AppData\\Local\\Temp\\ipykernel_29216\\3110683188.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(preprocessed)\n",
      "c:\\Users\\miche\\anaconda3\\envs\\MLAI\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], Cumulative Loss: 1.0409, True Loss: 1.4085\n",
      "Epoch [100/500], Cumulative Loss: 0.2729, True Loss: 0.3145\n",
      "Epoch [150/500], Cumulative Loss: 0.1629, True Loss: 0.6578\n",
      "Epoch [200/500], Cumulative Loss: 0.2133, True Loss: 0.6015\n",
      "Epoch [250/500], Cumulative Loss: 0.3481, True Loss: 0.5417\n",
      "Epoch [300/500], Cumulative Loss: 0.5037, True Loss: 0.9331\n",
      "Epoch [350/500], Cumulative Loss: 0.1810, True Loss: 0.5045\n",
      "Epoch [400/500], Cumulative Loss: 0.8420, True Loss: 3.0661\n",
      "Epoch [450/500], Cumulative Loss: 1.3890, True Loss: 0.8003\n",
      "Epoch [500/500], Cumulative Loss: 1.8909, True Loss: 1.4473\n",
      "\n",
      "Training complete!\n",
      "Final parameters: [[0.9348897  1.0122341  0.98700523 1.0252625  0.8976317  1.0598589\n",
      "  1.0697854  0.88492054 1.0005735  0.9322422 ]]\n",
      "Final loss: 0.0214\n"
     ]
    }
   ],
   "source": [
    "lstm_optimizer = LSTMOptimizer()\n",
    "writer = SummaryWriter(\"train/LSTM_Scheduled\")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(meta_optimizer, gamma=0.99)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.0001)\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, QuadraticOptimizee, {\"W\": W, \"theta0\": theta0}, num_epochs=500, time_horizon=200, discount=0.9,scheduler=scheduler, writer=writer)\n",
    "writer = SummaryWriter(\"runs/LSTM_Scheduled\")\n",
    "test_LSTM(lstm_optimizer, QuadraticOptimizee, {\"W\": W, \"theta0\": theta0}, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\AppData\\Local\\Temp\\ipykernel_29216\\3110683188.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(preprocessed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [[2.1140873 2.2084498 1.439697  2.131821  2.1317496 2.820525  1.6001282\n",
      "  2.0870872 2.185955  2.0423636]]\n",
      "Final loss: 0.0271\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(n, n)\n",
    "theta0 = np.ones((n, 1)) * 2\n",
    "writer = SummaryWriter(\"runs/LSTM_on_Different_Theta0\")\n",
    "test_LSTM(lstm_optimizer, QuadraticOptimizee, {\"W\": W, \"theta0\": theta0}, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miche\\AppData\\Local\\Temp\\ipykernel_14356\\3110683188.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(preprocessed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters: [[ 1.4611137   0.24016447  1.6320393   2.376492    1.0171142   0.8914471\n",
      "   0.7716255   1.0323058  -0.4345507   0.08845386  2.3286564   0.8356896\n",
      "   1.0963686   1.5145445   0.10581642  1.2890223   0.61576074  1.3796409\n",
      "   1.2943015   1.0529432 ]]\n",
      "Final loss: 0.3115\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "n = 20  # Dimension of theta\n",
    "W = np.random.randn(n, n)\n",
    "theta0 = np.ones((n, 1))*1\n",
    "\n",
    "writer = SummaryWriter(\"runs/LSTM_Different_n\")\n",
    "test_LSTM(lstm_optimizer, QuadraticOptimizee, {\"W\": W, \"theta0\": theta0}, time_horizon=1000, writer=writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
