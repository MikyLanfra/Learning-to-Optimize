{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49f7bc7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdbf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "\n",
    "from src.optimizee import *\n",
    "from src.optimizer import *\n",
    "from src.initializer import *\n",
    "\n",
    "from src.train_lstm import *\n",
    "from src.test_optimizer import *\n",
    "\n",
    "from src.interpretable_train_lstm import *\n",
    "from src.preprocessed_train_lstm import *\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f1ee7",
   "metadata": {},
   "source": [
    "## Sanity Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94e596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619bce20d49d4a1e8f4f726569cafea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Cumulative Loss: 334928559128707072.0000, LR: 1.000e-03\n",
      "Epoch [2/10], Cumulative Loss: 223858282492592128.0000, LR: 1.000e-03\n",
      "Epoch [3/10], Cumulative Loss: 124885124843044864.0000, LR: 1.000e-03\n",
      "Epoch [4/10], Cumulative Loss: 97719551185125376.0000, LR: 1.000e-03\n",
      "Epoch [5/10], Cumulative Loss: 97970171116781568.0000, LR: 1.000e-03\n",
      "Epoch [6/10], Cumulative Loss: 115990866048319488.0000, LR: 1.000e-03\n",
      "Epoch [7/10], Cumulative Loss: 63587291464990720.0000, LR: 1.000e-03\n",
      "Epoch [8/10], Cumulative Loss: 176612559904833536.0000, LR: 1.000e-03\n",
      "Epoch [9/10], Cumulative Loss: 138666910391730176.0000, LR: 1.000e-03\n",
      "Epoch [10/10], Cumulative Loss: 97805201422942208.0000, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 848.5136108398438\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.02]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=10, min_horizon=50, max_horizon=50, discount=2)\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec303b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8075ad582ab54d2fa737d983b639ad89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Cumulative Loss: 3201211.7500, LR: 1.000e-03\n",
      "Epoch [2/10], Cumulative Loss: 2120136.2500, LR: 1.000e-03\n",
      "Epoch [3/10], Cumulative Loss: 1421280.3750, LR: 1.000e-03\n",
      "Epoch [4/10], Cumulative Loss: 1140170.0000, LR: 1.000e-03\n",
      "Epoch [5/10], Cumulative Loss: 871803.2500, LR: 1.000e-03\n",
      "Epoch [6/10], Cumulative Loss: 617131.6250, LR: 1.000e-03\n",
      "Epoch [7/10], Cumulative Loss: 469041.4688, LR: 1.000e-03\n",
      "Epoch [8/10], Cumulative Loss: 395526.3125, LR: 1.000e-03\n",
      "Epoch [9/10], Cumulative Loss: 310691.6875, LR: 1.000e-03\n",
      "Epoch [10/10], Cumulative Loss: 223572.9375, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 136564.59375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "y = (y-np.mean(y))/np.std(y)\n",
    "X.shape, y.shape\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss(), nn.L1Loss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=2, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=10, min_horizon=50, max_horizon=50, discount=0)\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dce75",
   "metadata": {},
   "source": [
    "## Quadratic Optimizee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36f3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427765aa11b740dc8ff69f0845ef51c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?time step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.3852],\n",
       "        [0.5164],\n",
       "        [1.5571],\n",
       "        [0.7063],\n",
       "        [0.7442],\n",
       "        [0.9559],\n",
       "        [0.1205],\n",
       "        [1.2055],\n",
       "        [0.9906],\n",
       "        [0.7883]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Optimizers/Adam\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "optimizee = QuadraticOptimizee(W, theta0)\n",
    "optimizee.set_params()\n",
    "optimizer_cls = optim.Adam\n",
    "optimizer_kwargs = {\"lr\":0.05}\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Optimizers/Adam\")\n",
    "test_optimizer(optimizer_cls, optimizee,optimizer_kwargs, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73de3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63240ef617240a185adc379c7bc1ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 18863.7344, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0161, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 9.6801, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0483, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0059, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0231, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0157, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0169, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0197, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0495, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.025036029517650604\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Optimizers/LSTM\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=200, max_horizon=500, discount=0)\n",
    "writer = SummaryWriter(f\"Quadratic/Optimizers/LSTM\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c88dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54c716394944fbebd5f3e4e79cbe25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 87100.5781, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.3382, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1890, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0665, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.3615, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.3239, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1530, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0884, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.3373, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.2057, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.16304242610931396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f661bf5fe3bb413288ed1f1915ad7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 82497.3750, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.2789, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1875, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.2674, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.2013, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.3278, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.2189, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0411, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1276, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5433, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.49988266825675964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f3a931dacb4ae087d7f874261642f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 136763.3594, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.2124, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1286, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.1214, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.1698, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1458, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3471, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.1434, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1870, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1961, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.2404858022928238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fe927745d64365a3c1882cd8d0eebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 38981.0156, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 754.7361, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.1826, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.1219, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.1317, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0988, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3531, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0957, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0760, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1229, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.1588994711637497\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    th = i * 100\n",
    "    shutil.rmtree(f\"Quadratic/Horizon/{th}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500-th, max_horizon=500+th, discount=0)\n",
    "    writer = SummaryWriter(f\"Quadratic/Horizon/{th}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=1000, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b56d749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cdedea587349ea9bfeaaf9bebaea1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 19161.7090, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1679.9042, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0424, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0149, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0937, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1046, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1442, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.3084, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1321, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1007, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.14113982021808624\n",
      "Final parameters: [[ 1.8663387  -0.18337986  2.13553     0.29716218  0.44743246  0.79809546\n",
      "  -0.9425076   1.343382    0.7459527   0.5683814 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1515ce073ad472884a28b8e325bedac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 19180.8203, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1743.2588, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0289, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0382, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0766, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1223, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1372, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.2658, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1685, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0767, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10835688561201096\n",
      "Final parameters: [[ 1.9424584  -0.22430722  2.132454    0.29362375  0.4429594   0.79527706\n",
      "  -0.936571    1.3651272   0.74557936  0.5520531 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12959396f67b4ca09d675ce5f691dc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 21282.2812, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1922.0547, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0245, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0248, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0975, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1307, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.1743, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.2652, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1349, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1239, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.15897709131240845\n",
      "Final parameters: [[ 2.0037286  -0.35737175  2.2502873   0.20876706  0.43233424  0.74677634\n",
      "  -1.177658    1.3873047   0.8225865   0.50722057]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a5d047893e4765942aa4046ee632f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 185512.4844, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 16820.3613, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.3394, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.2994, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6529, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.1399, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.7025, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 2.2102, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.1801, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.6545, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.1259104311466217\n",
      "Final parameters: [[ 2.0240297  -0.30898386  2.3234987   0.21957426  0.3159482   0.7972845\n",
      "  -1.1508235   1.3413458   0.83222246  0.5255313 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105f6c8542c64a6f8588fd7ade581ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 3495373.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 299706.8438, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 14753.2412, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 3852.6289, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 1498.2699, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1612.4880, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1393.4733, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1211.7590, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1708.2153, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1392.5122, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.06501167267560959\n",
      "Final parameters: [[ 1.6859447   0.13510853  1.7694694   0.54262686  0.61068016  0.82488966\n",
      "  -0.34569275  1.2157702   0.7859552   0.71407807]]...\n"
     ]
    }
   ],
   "source": [
    "for discount in [0, 1e-3, 0.1, 0.9, 1]:\n",
    "    \n",
    "    shutil.rmtree(f\"Quadratic/Discount/Train_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Quadratic/Discount/Test_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"Quadratic/Discount/Train_{discount}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=discount, writer=writer)\n",
    "    writer = SummaryWriter(f\"Quadratic/Discount/Test_{discount}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6edcdfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f7ae9ca2024f4a8f4c9a644cdac2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 91058821413208064.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 122737228878184448.0000, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 105265430767075328.0000, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 188573854845632512.0000, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 76045557071609856.0000, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 74857964254527488.0000, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 58383139721969664.0000, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 48512647745765376.0000, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 74706257419698176.0000, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 70099801915523072.0000, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 332522.28125\n",
      "Final parameters: [[ 84.53091   83.46353   79.701775  65.80607   82.73815  129.2053\n",
      "   99.99969   85.89128   84.35818   90.16891 ]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(f\"Quadratic/Discount/Train_2\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(f\"Quadratic/Discount/Test_2\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Discount/Train_2\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=50, max_horizon=50, discount=2, writer=writer)\n",
    "writer = SummaryWriter(f\"Quadratic/Discount/Test_2\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34bba52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaddde13da9b4e1295ee8b1c2274d8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 18863.7344, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0161, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 9.6801, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0483, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0059, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0231, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0157, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0169, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0197, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0495, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.025036029517650604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec3cda9c714417c8749f007517dc564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 60361.3008, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.4575, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.7181, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 4.3570, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 3.8612, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 6.1974, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 7.0903, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 2.7111, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 3.8155, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 5.6945, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 4.595547676086426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc31fdb1aa0c43338c66e901d9aa1ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.2998, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1184, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0919, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0944, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.1672, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.1896, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0955, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0597, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.1520, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.1376, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.21306996047496796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d63ab2b3f00437aba577035fd7d5c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 32230.9883, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 5.9552, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 2.8149, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.9725, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6378, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0001, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6976, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.9508, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.7085, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.5810, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 2.0627806186676025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa25ea0cce54e2b83c5e90b08b38839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.4621, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1114, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6370, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.8220, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7274, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.5828, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6032, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.3352, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.0814, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.8935, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6995306611061096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbbd542a2b24527829c09d68ca64087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 18899.3105, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 7.0937, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8159, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 2.2447, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 1.4058, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.8772, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.5287, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.8929, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.3095, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.9035, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.9510489702224731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910d2e3e62884fab8813e1af5b6becc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.7460, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1576, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0994, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0870, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0555, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0388, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0352, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0820, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0270, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0340, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10253313928842545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2a732a375d4cbd9bcf473ddacc82f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.7460, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.1576, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0994, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0870, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0555, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0388, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0352, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0820, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0270, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0340, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.10253313928842545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11faa759c284ae68e01a174a99586c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8.0472, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 5.3325, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 3.7232, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 3.0543, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 2.3280, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 2.6844, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 4.1541, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 2.5754, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.5171, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 5.7459, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.1009637117385864\n",
      "Times_Train [56.72163724899292, 81.90180659294128, 106.4897084236145, 135.35498309135437, 155.31620001792908, 180.24078154563904, 210.21417117118835, 212.05891036987305, 228.83933758735657]\n",
      "Times_Test [0.8150696754455566, 0.954148530960083, 1.4411990642547607, 1.7495684623718262, 2.428520441055298, 2.3750107288360596, 2.845968723297119, 3.0905141830444336, 2.404212236404419]\n"
     ]
    }
   ],
   "source": [
    "times_train, times_test = [], []\n",
    "for count in range(1, 10):\n",
    "    shutil.rmtree(f\"Quadratic/Multi/Train_{count}_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Quadratic/Multi/Test_{count}_optim\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": list(np.arange(0.01, (count+1)*0.01, 0.01))}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    t0 = time.time()\n",
    "    writer = SummaryWriter(f\"Quadratic/Multi/Train_{count}_optim\") \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=200, max_horizon=500, discount=0, writer=writer)\n",
    "    t1 = time.time()\n",
    "    writer = SummaryWriter(f\"Quadratic/Multi/Test_{count}_optim\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)\n",
    "    t2 = time.time()\n",
    "\n",
    "    times_train.append(t1-t0)\n",
    "    times_test.append(t2-t1)\n",
    "\n",
    "print(\"Times_Train\", times_train)\n",
    "print(\"Times_Test\", times_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626dad57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad909349ae584b009968a4c0caccc7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 11530.6094, LR: 1.000e-01\n",
      "Epoch [40/200], Cumulative Loss: 0.4383, LR: 1.000e-01\n",
      "Epoch [60/200], Cumulative Loss: 0.6987, LR: 1.000e-01\n",
      "Epoch [80/200], Cumulative Loss: 0.8164, LR: 1.000e-01\n",
      "Epoch [100/200], Cumulative Loss: 2.5323, LR: 1.000e-01\n",
      "Epoch [120/200], Cumulative Loss: 0.5414, LR: 1.000e-01\n",
      "Epoch [140/200], Cumulative Loss: 0.4951, LR: 1.000e-01\n",
      "Epoch [160/200], Cumulative Loss: 0.2133, LR: 1.000e-01\n",
      "Epoch [180/200], Cumulative Loss: 0.6893, LR: 1.000e-01\n",
      "Epoch [200/200], Cumulative Loss: 0.6405, LR: 1.000e-01\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.2033077478408813\n",
      "Final parameters: [[-1.993716   4.672194  -2.7682881  3.217623   2.6500154  1.6340405\n",
      "   7.3981104 -0.326854   1.8212482  2.4724944]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d717d1a0aaee4ba89a6b50ba87500fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 3.0271, LR: 1.000e-02\n",
      "Epoch [40/200], Cumulative Loss: 23.6708, LR: 1.000e-02\n",
      "Epoch [60/200], Cumulative Loss: 5.9904, LR: 1.000e-02\n",
      "Epoch [80/200], Cumulative Loss: 9.8778, LR: 1.000e-02\n",
      "Epoch [100/200], Cumulative Loss: 4.2154, LR: 1.000e-02\n",
      "Epoch [120/200], Cumulative Loss: 5.8195, LR: 1.000e-02\n",
      "Epoch [140/200], Cumulative Loss: 3.6592, LR: 1.000e-02\n",
      "Epoch [160/200], Cumulative Loss: 4.2677, LR: 1.000e-02\n",
      "Epoch [180/200], Cumulative Loss: 2.6112, LR: 1.000e-02\n",
      "Epoch [200/200], Cumulative Loss: 2.8854, LR: 1.000e-02\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.7332725524902344\n",
      "Final parameters: [[1.5985692  0.06193918 0.80853343 0.86137754 0.84611106 0.70785636\n",
      "  0.38167152 1.3204658  0.24488646 1.0086565 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b824965e1b45ac8ab19323945e8188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 1679.9042, LR: 1.000e-03\n",
      "Epoch [40/200], Cumulative Loss: 0.0149, LR: 1.000e-03\n",
      "Epoch [60/200], Cumulative Loss: 0.1046, LR: 1.000e-03\n",
      "Epoch [80/200], Cumulative Loss: 0.3084, LR: 1.000e-03\n",
      "Epoch [100/200], Cumulative Loss: 0.1007, LR: 1.000e-03\n",
      "Epoch [120/200], Cumulative Loss: 0.1111, LR: 1.000e-03\n",
      "Epoch [140/200], Cumulative Loss: 0.1002, LR: 1.000e-03\n",
      "Epoch [160/200], Cumulative Loss: 0.1151, LR: 1.000e-03\n",
      "Epoch [180/200], Cumulative Loss: 0.1102, LR: 1.000e-03\n",
      "Epoch [200/200], Cumulative Loss: 0.1093, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.07103900611400604\n",
      "Final parameters: [[ 1.5654256   0.30250198  1.5598316   0.60075116  0.7088844   0.81796616\n",
      "  -0.05125184  1.1672177   0.7796621   0.8026926 ]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc7c1d3c8654778bd29216679f58328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 151222.4062, LR: 1.000e-04\n",
      "Epoch [40/200], Cumulative Loss: 104470.3438, LR: 1.000e-04\n",
      "Epoch [60/200], Cumulative Loss: 65506.3711, LR: 1.000e-04\n",
      "Epoch [80/200], Cumulative Loss: 36242.8086, LR: 1.000e-04\n",
      "Epoch [100/200], Cumulative Loss: 17056.4102, LR: 1.000e-04\n",
      "Epoch [120/200], Cumulative Loss: 7074.2148, LR: 1.000e-04\n",
      "Epoch [140/200], Cumulative Loss: 2590.7512, LR: 1.000e-04\n",
      "Epoch [160/200], Cumulative Loss: 719.8875, LR: 1.000e-04\n",
      "Epoch [180/200], Cumulative Loss: 240.4924, LR: 1.000e-04\n",
      "Epoch [200/200], Cumulative Loss: 77.0269, LR: 1.000e-04\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 95.14395904541016\n",
      "Final parameters: [[ 1.1561422  -3.5963435  -1.160581   -2.333385    0.44913736 -2.902987\n",
      "  -1.7678744  -0.38950413 -2.892629    0.25393853]]...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "\n",
    "    shutil.rmtree(f\"Quadratic/Lr/Train_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Quadratic/Lr/Test_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    lr = 10**(-i)\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=lr)\n",
    "\n",
    "    writer = SummaryWriter(f\"Quadratic/Lr/Train_e{i}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(f\"Quadratic/Lr/Test_e{i}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cb60fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef603e35f38e48df804caff90c6e50b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 3.0597, LR: 8.179e-03\n",
      "Epoch [40/200], Cumulative Loss: 20.0837, LR: 6.690e-03\n",
      "Epoch [60/200], Cumulative Loss: 4.8919, LR: 5.472e-03\n",
      "Epoch [80/200], Cumulative Loss: 8.1621, LR: 4.475e-03\n",
      "Epoch [100/200], Cumulative Loss: 3.3011, LR: 3.660e-03\n",
      "Epoch [120/200], Cumulative Loss: 5.1842, LR: 2.994e-03\n",
      "Epoch [140/200], Cumulative Loss: 6.6976, LR: 2.449e-03\n",
      "Epoch [160/200], Cumulative Loss: 2.6100, LR: 2.003e-03\n",
      "Epoch [180/200], Cumulative Loss: 4.3614, LR: 1.638e-03\n",
      "Epoch [200/200], Cumulative Loss: 3.0451, LR: 1.340e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.4719452857971191\n",
      "Final parameters: [[ 2.814364   -1.4750608   3.383005   -0.14283155 -0.34546745  0.6081619\n",
      "  -3.4897394   1.8527266   0.3712343  -0.02309484]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Lr/Train_Exp\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"Quadratic/Lr/Test_Exp\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(meta_optimizer, gamma=0.99)\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Lr/Train_Exp\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer, scheduler=scheduler)\n",
    "writer = SummaryWriter(f\"Quadratic/Lr/Test_Exp\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e7fbd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533ff77bc92c40d2b444c2fe708faaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 43108.0234, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0079, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0405, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0206, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0381, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0365, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0380, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0392, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0617, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0276, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.051029056310653687\n",
      "Final parameters: [[1.4419096  0.47065443 1.5104656  0.66466796 0.77859986 0.8647847\n",
      "  0.10427374 1.1317858  0.9094761  0.7959955 ]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"Quadratic/Comparison/Train_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"Quadratic/Comparison/Test_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"Quadratic/Comparison/Train_Right_Horizon\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=800, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"Quadratic/Comparison/Test_Right_Horizon\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1) # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.1, 0.2, 0.3, 0.4]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent_Intepretable(num_optims=initializer.get_num_optims(), preproc=False)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "lstm_optimizer, lambdas_ = train_LSTM_Intepret(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=50, discount=0.9)\n",
    "params = test_LSTM_Interpret(lstm_optimizer, initializer, time_horizon=50)\n",
    "\n",
    "v = nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters())\n",
    "print(nn.functional.softmax(v, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in [3, 5, 7]:\n",
    "    shutil.rmtree(f\"Preproc/Quadratic/Train_{count}_Classic\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Preproc/Quadratic/Test_{count}_Classic\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": list(np.arange(0.01, (count+1)*0.01, 0.01))}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"Preproc/Quadratic/Train_{count}_Classic\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=200, max_horizon=500, discount=0, writer=writer)\n",
    "\n",
    "    writer = SummaryWriter(f\"Preproc/Quadratic/Test_{count}_Classic\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa913e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in [3, 5, 7]:\n",
    "    shutil.rmtree(f\"Preproc/Quadratic/Train_{count}_Alt1\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Preproc/Quadratic/Test_{count}_Alt1\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": list(np.arange(0.01, (count+1)*0.01, 0.01))}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"Preproc/Quadratic/Train_{count}_Alt1\")\n",
    "    lstm_optimizer = train_LSTM_alt_preproc(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=200, max_horizon=500, discount=0, writer=writer)\n",
    "\n",
    "    writer = SummaryWriter(f\"Preproc/Quadratic/Test_{count}_Alt1\")\n",
    "    params = test_LSTM_alt_preproc(lstm_optimizer, initializer, time_horizon=500, writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count in [3, 5, 7]:\n",
    "    shutil.rmtree(f\"Preproc/Quadratic/Train_{count}_Alt2\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"Preproc/Quadratic/Test_{count}_Alt2\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(0)  # Set random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n=10\n",
    "    W = torch.randn(n, n)  # Random weights for the linear model\n",
    "    theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "    kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": list(np.arange(0.01, (count+1)*0.01, 0.01))}\n",
    "\n",
    "    initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent_Preproc(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"Preproc/Quadratic/Train_{count}_Alt2\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=200, max_horizon=500, discount=0, writer=writer)\n",
    "\n",
    "    writer = SummaryWriter(f\"Preproc/Quadratic/Test_{count}_Alt2\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49795c8e",
   "metadata": {},
   "source": [
    "## NN Optimizee - Single Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477fbfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413de5b038d74d7d92e14b1b683d3001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8006432.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 9.7166, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 2.0988, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.7331, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7267, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0989, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.4511, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.5665, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6034, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.1383, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5529805421829224\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NNDiabetes/Train_LSTM\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NNDiabetes/Test_LSTM\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hiddaen_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "writer = SummaryWriter(\"NNDiabetes/Train_LSTM\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(\"NNDiabetes/Test_LSTM\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34928ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff00fc2903214570b3bbc9ae44a001ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8006432.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 9.7166, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 2.0988, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.7331, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7267, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0989, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.4511, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.5665, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6034, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.1383, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5529805421829224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c9b12a815b4c0285cc31b0c3ba8992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8014993.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 9.9052, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6903, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5635, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.4246, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8011, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.5069, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.7418, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6496, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.4590, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.48407047986984253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17283b30610449928e816d823640e9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8940307.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 2.8788, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.7551, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5931, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6959, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7834, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.7119, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.7548, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.9785, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.6719, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6535630822181702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a923081d8934ca8a41695bddb147000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 75164032.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 60.7986, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 12.4487, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 6.1888, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 6.6519, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 7.0342, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 5.9492, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 5.9315, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 8.2504, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 5.9288, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.7123647332191467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c28a35f45e44c0e8ec728147279d392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 859471296.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1441.3616, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 436.0175, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 318.4882, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 291.7909, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 349.0607, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 384.4711, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 466.8342, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 308.0726, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 659.2723, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.4745386242866516\n"
     ]
    }
   ],
   "source": [
    "for discount in [0, 1e-3, 0.1, 0.9, 1]:\n",
    "    \n",
    "    shutil.rmtree(f\"NN_Diabetes/Discount_v2/Train_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NN_Diabetes/Discount_v2/Test_{discount}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "    initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Discount_v2/Train_{discount}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=discount, writer=writer)\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Discount_v2/Test_{discount}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26d3bfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93493679eaf64f46bd86627471b3c80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 6249670.0000, LR: 1.000e-01\n",
      "Epoch [40/200], Cumulative Loss: 6372057.5000, LR: 1.000e-01\n",
      "Epoch [60/200], Cumulative Loss: 5327980.5000, LR: 1.000e-01\n",
      "Epoch [80/200], Cumulative Loss: 5257824.0000, LR: 1.000e-01\n",
      "Epoch [100/200], Cumulative Loss: 5160894.5000, LR: 1.000e-01\n",
      "Epoch [120/200], Cumulative Loss: 5063762.0000, LR: 1.000e-01\n",
      "Epoch [140/200], Cumulative Loss: 4940141.0000, LR: 1.000e-01\n",
      "Epoch [160/200], Cumulative Loss: 4815151.0000, LR: 1.000e-01\n",
      "Epoch [180/200], Cumulative Loss: 4694249.5000, LR: 1.000e-01\n",
      "Epoch [200/200], Cumulative Loss: 4556052.5000, LR: 1.000e-01\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 4577862.5\n",
      "Final parameters: [[-2139.6338 -2139.5386 -2139.3198 -2138.8416 -2137.8494 -2135.9512\n",
      "  -2138.3613 -2137.4475 -2138.4233 -2136.694  -2139.2332 -2139.5715\n",
      "  -2139.9653 -2136.1748 -2138.9194 -2138.7114 -2138.079  -2138.4326\n",
      "  -2137.951  -2137.8035 -2139.09   -2140.0803 -2136.8918 -2139.1582\n",
      "  -2139.0652 -2138.2954 -2137.416  -2137.475  -2138.5325 -2138.4048\n",
      "  -2136.6843 -2139.1792 -2137.7727 -2139.5076 -2138.3376 -2137.661\n",
      "  -2139.7961 -2137.1643 -2136.9226 -2139.2373 -2138.5125 -2138.8596\n",
      "  -2138.9495 -2139.087  -2137.6628 -2138.4995 -2136.0198 -2137.8816\n",
      "  -2140.302  -2138.7007 -2136.6929 -2139.1409 -2138.4094 -2137.0571\n",
      "  -2138.0676 -2137.999  -2138.1133 -2138.6233 -2138.4175 -2138.7485\n",
      "  -2136.4326 -2138.0864 -2137.5466 -2138.1042 -2137.0232 -2138.5903\n",
      "  -2138.0293 -2138.7468 -2137.7017 -2138.5369 -2138.4253 -2139.3154\n",
      "  -2139.9087 -2137.509  -2138.1326 -2137.633  -2139.471  -2136.1257\n",
      "  -2137.3599 -2138.2466 -2138.7034 -2138.062  -2136.48   -2139.6943\n",
      "  -2138.415  -2140.9182 -2138.3538 -2136.3113 -2138.8074 -2138.5686\n",
      "  -2138.2517 -2137.3462 -2139.8176 -2137.438  -2138.1748 -2138.5435\n",
      "  -2140.4177 -2136.9822 -2138.5498 -2138.8362 -2139.7944 -2138.5723\n",
      "  -2136.708  -2137.0056 -2138.495  -2136.6912 -2137.8696 -2137.3213\n",
      "  -2139.9563 -2139.294  -2138.0962 -2136.4932 -2138.808  -2137.9314\n",
      "  -2137.4817 -2138.6143 -2139.232  -2137.6436 -2137.519  -2138.7336\n",
      "  -2138.6821 -2138.3696 -2138.8193 -2138.1624 -2138.445  -2139.1184\n",
      "  -2138.1755 -2139.332  -2138.212  -2136.944  -2139.4246 -2136.1624\n",
      "  -2139.0503 -2137.34   -2139.0405 -2139.4844 -2137.4514 -2138.737\n",
      "  -2137.4749 -2139.1475 -2139.5757 -2137.4753 -2137.9563 -2140.3416\n",
      "  -2137.6348 -2138.8699 -2137.6526 -2139.2056 -2138.5078 -2138.5032\n",
      "  -2138.2861 -2139.0793 -2137.6995 -2138.5154 -2137.3    -2136.3076\n",
      "  -2137.8103 -2137.5435 -2137.561  -2137.4895 -2137.6016 -2138.5671\n",
      "  -2138.3308 -2138.3262 -2137.3054 -2137.5603 -2137.375  -2138.5325\n",
      "  -2138.628  -2138.998  -2137.5261 -2139.2437 -2138.4011 -2137.6548\n",
      "  -2139.149  -2138.3777 -2138.9775 -2139.354  -2136.2798 -2137.7275\n",
      "  -2139.4568 -2137.8438 -2139.8452 -2138.8142 -2137.9893 -2137.8352\n",
      "  -2139.1287 -2138.547  -2138.0852 -2140.724  -2137.8096 -2138.405\n",
      "  -2139.3853 -2139.215  -2137.3958 -2137.6833 -2141.1118 -2137.3894\n",
      "  -2136.4514 -2137.5625 -2137.2288 -2137.4988 -2138.664  -2137.538\n",
      "  -2138.87   -2139.6973 -2138.9888 -2137.4482 -2136.9473 -2138.1958\n",
      "  -2137.1326 -2138.3132 -2137.0237 -2139.021  -2139.0693 -2135.8472\n",
      "  -2138.11   -2136.6797 -2138.447  -2138.5405 -2138.721  -2139.3818\n",
      "  -2137.3264 -2139.644  -2138.6377 -2138.868  -2137.1553 -2137.1084\n",
      "  -2139.486  -2139.3306 -2137.9236 -2140.0684 -2138.8755 -2137.3738\n",
      "  -2138.5115 -2138.4521 -2139.3394 -2137.9365 -2138.8665 -2138.6252\n",
      "  -2139.6682]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9155fcc38e4f329cb8a8018394a031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 14194.9180, LR: 1.000e-02\n",
      "Epoch [40/200], Cumulative Loss: 21041.3184, LR: 1.000e-02\n",
      "Epoch [60/200], Cumulative Loss: 21892.2852, LR: 1.000e-02\n",
      "Epoch [80/200], Cumulative Loss: 21427.7305, LR: 1.000e-02\n",
      "Epoch [100/200], Cumulative Loss: 21263.6230, LR: 1.000e-02\n",
      "Epoch [120/200], Cumulative Loss: 21721.2500, LR: 1.000e-02\n",
      "Epoch [140/200], Cumulative Loss: 20845.8867, LR: 1.000e-02\n",
      "Epoch [160/200], Cumulative Loss: 20414.2422, LR: 1.000e-02\n",
      "Epoch [180/200], Cumulative Loss: 20626.1328, LR: 1.000e-02\n",
      "Epoch [200/200], Cumulative Loss: 19850.9141, LR: 1.000e-02\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 20628.640625\n",
      "Final parameters: [[-144.21327 -144.17206 -144.6734  -143.44218 -142.86865 -140.9704\n",
      "  -143.38077 -142.70139 -143.10115 -142.27995 -144.42044 -144.14645\n",
      "  -144.47533 -141.77798 -143.93872 -143.94342 -142.59561 -143.60626\n",
      "  -142.97029 -142.82271 -143.40091 -144.40218 -142.87471 -143.45876\n",
      "  -144.08455 -143.3148  -142.4353  -143.44862 -142.75139 -142.9768\n",
      "  -142.25635 -143.75302 -141.96863 -144.0316  -143.35692 -143.0329\n",
      "  -144.15567 -142.75731 -141.94196 -144.25676 -143.11012 -143.50934\n",
      "  -144.29202 -143.71204 -142.68204 -143.51888 -141.03908 -143.34653\n",
      "  -144.85228 -143.27383 -141.90791 -143.71838 -142.93672 -141.57957\n",
      "  -143.08702 -143.26495 -142.62967 -143.83377 -143.4368  -143.76782\n",
      "  -141.0197  -142.73349 -142.92894 -142.72171 -142.04245 -143.60971\n",
      "  -143.04861 -144.18744 -142.25081 -143.11253 -143.47383 -143.89139\n",
      "  -144.43492 -143.05978 -143.15201 -142.85497 -143.98988 -141.16576\n",
      "  -142.37909 -143.26598 -143.14937 -142.54872 -141.99759 -144.1516\n",
      "  -143.43445 -145.93758 -143.37318 -141.98615 -143.20052 -143.13788\n",
      "  -143.52504 -142.55739 -144.19409 -141.95827 -143.19403 -143.60277\n",
      "  -144.93895 -142.2433  -143.56909 -143.85558 -144.23718 -143.05547\n",
      "  -142.31705 -141.45738 -143.51457 -141.71054 -142.88876 -143.02786\n",
      "  -144.34677 -143.86342 -143.40712 -141.9051  -143.1786  -142.45169\n",
      "  -142.50085 -143.70752 -143.75137 -142.94182 -142.53818 -143.75298\n",
      "  -144.49701 -143.91142 -143.3476  -143.63948 -143.46452 -144.13788\n",
      "  -143.19484 -144.19939 -144.05907 -142.30441 -144.28394 -141.40167\n",
      "  -144.40315 -141.89888 -144.0598  -144.37349 -143.03171 -143.7186\n",
      "  -142.49403 -144.16676 -144.00519 -141.95291 -143.32147 -144.7883\n",
      "  -142.65407 -143.88919 -142.67175 -144.7818  -142.89021 -143.07426\n",
      "  -143.45424 -143.65672 -142.05527 -143.03807 -142.31932 -141.39119\n",
      "  -142.32948 -142.69318 -142.58018 -142.50876 -142.18808 -143.21657\n",
      "  -143.65176 -142.94635 -142.32454 -142.57959 -142.39429 -143.75952\n",
      "  -143.34415 -143.57239 -142.66966 -143.81822 -142.92255 -142.17865\n",
      "  -144.16824 -143.85924 -143.49734 -144.62227 -141.29918 -142.74683\n",
      "  -144.05489 -142.49622 -145.18481 -143.43852 -143.00859 -142.85432\n",
      "  -144.14793 -143.98619 -142.64012 -145.30544 -142.8518  -142.97241\n",
      "  -143.91829 -143.73778 -142.4149  -142.90277 -145.639   -142.56898\n",
      "  -141.47069 -142.58173 -143.14284 -142.95235 -143.2587  -143.12766\n",
      "  -143.8894  -144.71654 -144.0082  -142.42613 -142.90479 -143.27254\n",
      "  -142.67172 -143.29637 -143.05702 -143.53856 -144.08868 -142.31967\n",
      "  -143.39594 -142.40482 -143.4664  -143.55989 -143.23642 -143.9375\n",
      "  -141.8688  -144.16422 -143.65701 -143.88736 -142.17459 -141.58281\n",
      "  -143.98938 -143.91035 -142.40683 -144.64008 -143.36255 -141.90219\n",
      "  -143.53091 -142.9154  -143.88647 -142.42406 -143.88591 -143.64468\n",
      "  -143.69785]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51548d7dd1af45688ef6d1022edf952e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 7.5716, LR: 1.000e-03\n",
      "Epoch [40/200], Cumulative Loss: 0.5253, LR: 1.000e-03\n",
      "Epoch [60/200], Cumulative Loss: 0.8197, LR: 1.000e-03\n",
      "Epoch [80/200], Cumulative Loss: 0.8909, LR: 1.000e-03\n",
      "Epoch [100/200], Cumulative Loss: 0.8395, LR: 1.000e-03\n",
      "Epoch [120/200], Cumulative Loss: 0.4654, LR: 1.000e-03\n",
      "Epoch [140/200], Cumulative Loss: 0.7502, LR: 1.000e-03\n",
      "Epoch [160/200], Cumulative Loss: 0.5141, LR: 1.000e-03\n",
      "Epoch [180/200], Cumulative Loss: 0.6688, LR: 1.000e-03\n",
      "Epoch [200/200], Cumulative Loss: 0.9374, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5737376809120178\n",
      "Final parameters: [[ 2.9833179   3.8622794  -0.14068198  4.3857355   3.7798533   5.3756747\n",
      "   3.5110707   3.1401856   4.829986    6.0724025   1.8579866   3.8900626\n",
      "   4.009493    4.063535    2.226865    3.2519507   5.548273    2.7012928\n",
      "   5.7696447   3.7616239   3.0303867   4.399092    1.5447174   5.156945\n",
      "   2.9124982   4.1088667   2.3230379   1.7117053   5.8028502   6.3108554\n",
      "   2.9037166   4.7738843   7.218111    3.0861804   3.3332653   2.8653045\n",
      "   4.8156023   2.4354537   7.0653067   2.6609013   4.387425    3.9325497\n",
      "   0.89347357  3.5290716   3.83759     1.949758    4.3576956   3.2839718\n",
      "   2.3409214   4.5261874   4.9143033   3.8762648   4.979506    3.7063744\n",
      "   2.7167385   4.4561815   4.8690743   3.0209825   5.8837056   2.2031088\n",
      "   6.0937223   5.0301914   2.173312    4.8391094   4.3458376   2.124522\n",
      "   1.5386246   2.2012117   5.268431    5.3659134   2.8786476   4.0128694\n",
      "   3.7797322   2.943897    2.7518315   4.518693    3.8553221   5.2093387\n",
      "   6.888393    2.370473    2.890795    6.158195    2.2448163   4.3562036\n",
      "   2.3325722  -2.6282241   1.200713    3.2301042   5.2672153   5.9915156\n",
      "   1.6603407   6.3648305   4.932696    1.0557357   1.7352475   2.3325803\n",
      "   3.88789     2.96933     6.854594    0.7371184   3.3620892   4.5984087\n",
      "   2.8873014   5.9809785   3.2806811   2.6973019   2.465906    3.3193157\n",
      "   3.0531778   4.2259855   2.9072168   6.06988     4.944807    1.3811487\n",
      "   3.0597193   3.2221835   4.023553    3.3906631   7.121735    2.116032\n",
      "   2.5428395   4.760855    2.9865205   4.8391676   4.057009    4.5758624\n",
      "   6.7508283   1.5565015   4.8223658   6.281743    1.9382148   6.345533\n",
      "   4.509158    4.257961    4.541891    2.9205406   5.8827043   2.6856604\n",
      "   4.9653053   2.4724796   3.5834227   5.173832    1.9224242   2.1178777\n",
      "   4.036575    0.7291186   2.0942302   2.1034088   3.9792907   4.595758\n",
      "   3.4056149   3.7450106   5.576422    1.5738432   3.319553    6.199581\n",
      "   5.042405    4.1868405   6.4808292   3.5270662   5.473539    3.909841\n",
      "   1.5687833   3.9644604   4.402929    2.717198    2.9030037   2.9949253\n",
      "   3.6914372   3.709014    4.35642     3.4333956   4.679858    2.431642\n",
      "   1.7642552   4.3688655   3.6687734   2.5665393   8.062141    3.1989136\n",
      "   3.0387018   5.3821206   0.331918    4.22394     3.6037421   2.978351\n",
      "   0.41142613  2.3133137   4.9810348   2.7919533   3.361756    4.583048\n",
      "   4.44586     2.150892    4.1038365   4.3543787   2.383157    3.7941084\n",
      "   7.128867    3.3845756   2.4472344   2.8410106   0.6117716   2.6140475\n",
      "   1.8473978  -0.6130223  -1.0401691   1.5090415   3.226358    2.1238437\n",
      "   2.3255382   1.5591537   3.9110928   0.4791184   0.81885266  4.779049\n",
      "   2.4414227   2.8434954   1.947263    1.1641368   0.18422621 -0.6179991\n",
      "   2.2858565  -0.7980415   0.27221653  1.349043    2.9822955   1.904329\n",
      "  -0.8798691  -0.3243514   0.79396445 -0.9243812  -0.2787299   2.0482552\n",
      "   0.45387176  0.06153198 -0.3469083   0.7207379  -0.28808427  0.8927337\n",
      "  -0.33927578]]...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377a191c60304f29b496e519ebbf8cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 823756864.0000, LR: 1.000e-04\n",
      "Epoch [40/200], Cumulative Loss: 317365088.0000, LR: 1.000e-04\n",
      "Epoch [60/200], Cumulative Loss: 154674672.0000, LR: 1.000e-04\n",
      "Epoch [80/200], Cumulative Loss: 85397240.0000, LR: 1.000e-04\n",
      "Epoch [100/200], Cumulative Loss: 51343612.0000, LR: 1.000e-04\n",
      "Epoch [120/200], Cumulative Loss: 35554128.0000, LR: 1.000e-04\n",
      "Epoch [140/200], Cumulative Loss: 23031468.0000, LR: 1.000e-04\n",
      "Epoch [160/200], Cumulative Loss: 16873882.0000, LR: 1.000e-04\n",
      "Epoch [180/200], Cumulative Loss: 10926652.0000, LR: 1.000e-04\n",
      "Epoch [200/200], Cumulative Loss: 9881110.0000, LR: 1.000e-04\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 9630429.0\n",
      "Final parameters: [[14.98499   15.269319  14.378924  16.08163   16.045311  17.987875\n",
      "  15.2940855 16.236506  16.450212  16.892769  14.446253  15.641608\n",
      "  14.664139  16.71841   14.998168  14.882536  16.688196  15.23445\n",
      "  16.40212   16.365612  17.0328    16.229595  17.818953  17.301712\n",
      "  16.433405  17.172335  17.83573   17.489338  17.883286  17.911379\n",
      "  18.264004  17.456264  18.397532  16.400976  17.096066  17.17477\n",
      "  16.501318  17.738607  19.029387  16.461773  16.387943  16.130537\n",
      "  15.042349  16.103569  16.523588  15.693499  17.920176  16.058725\n",
      "  14.755642  16.223083  17.241316  16.35807   16.481653  17.384436\n",
      "  16.045755  15.850873  16.81929   15.294646  16.263405  15.659293\n",
      "  17.892147  16.374832  15.81821   16.602785  16.67412   15.092715\n",
      "  15.40415   14.642344  16.870926  15.860143  14.961566  15.787177\n",
      "  14.4019375 15.256267  15.530553  15.672303  14.931891  17.237486\n",
      "  16.76095   15.60216   15.011135  16.061825  16.098028  14.68081\n",
      "  14.815409  12.376038  14.601062  16.386808  15.429643  15.459508\n",
      "  14.443482  16.573475  13.941225  15.981404  14.9499035 14.070536\n",
      "  13.624815  15.695173  15.104863  14.499325  14.263325  15.757012\n",
      "  16.1475    17.528982  14.940081  16.829361  15.312624  15.698059\n",
      "  14.4426985 14.980538  14.916636  17.45137   15.291735  15.720232\n",
      "  15.86645   14.311607  15.055846  15.349858  16.384243  14.882188\n",
      "  23.645441  24.512451  24.610855  24.678734  24.802156  24.36336\n",
      "  25.035137  24.11381   24.68953   25.887964  24.079912  26.815794\n",
      "  23.60438   26.093988  24.266922  24.06698   25.38203   24.770052\n",
      "  25.5319    23.84526   14.804613  16.945383  15.495557  14.28043\n",
      "  15.954946  14.7198715 15.632056  14.252243  15.981624  15.876143\n",
      "  15.168573  15.74372   16.670172  15.327108  16.176292  17.065557\n",
      "  16.598377  15.897737  16.540907  16.427908  16.498848  15.709935\n",
      "  14.867106  16.154053  16.134743  15.892853  15.793895  14.677817\n",
      "  15.760416  15.252615  15.685317  15.568273  15.687079  15.953098\n",
      "  14.195131  14.760301  15.282786  13.842158  17.629566  15.910803\n",
      "  14.878543  16.624043  13.584392  15.808638  15.547947  15.693487\n",
      "  14.165088  14.867104  16.492588  13.676938  15.602008  16.526794\n",
      "  14.938985  14.557889  16.062054  15.649121  13.298208  15.982933\n",
      "  17.65989   16.271605  12.508459  13.217208  11.708561  13.4778805\n",
      "  11.356207  10.717722  10.999693  12.910717  13.909719  12.311173\n",
      "  13.318851  13.084664  13.082337  11.358158  11.098908  14.683884\n",
      "  12.510457  13.775948  12.186383  11.762456  11.685726  10.955115\n",
      "  13.178152  10.674613  11.5023575 11.635082  13.053876  13.243814\n",
      "  10.789675  11.111747  12.393245  10.30539   11.471237  13.163934\n",
      "  11.70761   11.753902  11.082065  12.337896  11.00503   11.373255\n",
      "  11.8843355]]...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "\n",
    "    shutil.rmtree(f\"NN_Diabetes/Lr/Train_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NN_Diabetes/Lr/Test_e{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    lr = 10**(-i)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "    initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=lr)\n",
    "\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Lr/Train_e{i}\")\n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(f\"NN_Diabetes/Lr/Test_e{i}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e417c23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a280a3ea121f40f094801ced72b563e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/2000], Cumulative Loss: 9881110.0000, LR: 1.000e-04\n",
      "Epoch [400/2000], Cumulative Loss: 1246417.8750, LR: 1.000e-04\n",
      "Epoch [600/2000], Cumulative Loss: 382936.7188, LR: 1.000e-04\n",
      "Epoch [800/2000], Cumulative Loss: 148605.0156, LR: 1.000e-04\n",
      "Epoch [1000/2000], Cumulative Loss: 72151.5234, LR: 1.000e-04\n",
      "Epoch [1200/2000], Cumulative Loss: 32419.0684, LR: 1.000e-04\n",
      "Epoch [1400/2000], Cumulative Loss: 22371.1641, LR: 1.000e-04\n",
      "Epoch [1600/2000], Cumulative Loss: 4474.1758, LR: 1.000e-04\n",
      "Epoch [1800/2000], Cumulative Loss: 2441.9375, LR: 1.000e-04\n",
      "Epoch [2000/2000], Cumulative Loss: 1084.0387, LR: 1.000e-04\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1392.8455810546875\n",
      "Final parameters: [[ 4.4197307   9.92952     5.375772    7.0339346  20.885983    4.8713417\n",
      "   2.7107177   4.1928186   4.885844    8.907608    4.54261    21.969252\n",
      "   4.549441    8.206536    5.313111   20.189985    5.13701     6.5987716\n",
      "   6.24627     5.765709    5.3153024  12.793395    5.91939     6.311839\n",
      "  17.92428     6.3287153   5.7556505   5.735578    6.522535   12.861653\n",
      "   4.1830645  19.373384    4.9254303   9.535274    7.191734   19.263844\n",
      "   6.2421875   5.8997555   6.922664    5.9850597   6.2865353   8.851097\n",
      "   2.7731006   6.86293    20.307165    3.7585325   5.5090637   3.565444\n",
      "   6.852719    8.287176    4.9737477  18.595816    6.328521    8.619517\n",
      "   5.906531   19.362478    5.9640317   6.400286    4.9273353   6.324533\n",
      "   6.6769934   9.596638    6.4010396   5.4251094  21.87996     4.5992255\n",
      "   4.613698    4.6637855   5.68208    11.616599    5.2786574  20.10108\n",
      "   5.2331576   5.1902523   5.827081   20.381353    6.121332    5.146327\n",
      "   7.192719    3.9408972   5.6316743  10.464844    5.895172    3.4678073\n",
      "  21.153683    3.433861    4.1244864   5.55609     3.2546387   8.15448\n",
      "   3.7838843  22.262999    5.0214915   7.633179    3.34       20.913404\n",
      "   5.7590914   4.2271223   6.04464     3.8141646   4.4527135  10.253082\n",
      "   5.490072    3.1719415  19.93142     4.5465994   4.0515      4.6089697\n",
      "   4.8735094   8.146483    5.3204956  20.580097    4.7066064   7.6313376\n",
      "   3.6919997  20.364548    6.0914373   3.4203687   6.900019    5.5562935\n",
      "  11.97043    15.145327   12.057724   12.459736   13.691461   12.602442\n",
      "  11.659591   11.341086   13.361637   12.608922   13.565283   12.787704\n",
      "  11.706997   14.016109   12.2348995  13.205321   12.501382   11.712908\n",
      "  13.662142   10.512092    3.77322    10.176089    5.6958447   5.2463665\n",
      "  20.357779    5.254192    5.094125    3.353041    6.8365755   8.138586\n",
      "   5.182269   18.107222    6.89368    10.437743    5.820027   20.983334\n",
      "   5.2550406   4.264535    9.36476     6.271902    3.3101592   9.211728\n",
      "   7.5302835   4.4727387  20.318241    3.752058    4.447556    6.0585165\n",
      "   4.956522    9.528886    4.8214793  20.282593    5.10809     8.920586\n",
      "   5.5936885  20.916779    5.5338936   6.294393    5.3142033   6.2289076\n",
      "   3.125778    9.279411    5.810116    4.532481   19.611126    4.995643\n",
      "   3.3018186   2.2959998   4.1558423  11.395317    4.604146   21.090939\n",
      "   3.1364903   8.663986    7.530197   20.276976    4.0915904   6.215032\n",
      "   5.787273    4.721078    1.3957248   7.9526224   2.613111    2.8260813\n",
      "  19.764067    1.4653832   1.335665    2.3557858   2.6232352   5.8989058\n",
      "   1.4201665  20.527548    4.646786    7.0595694   2.6009946  20.933495\n",
      "   3.0133855   2.2300477   5.6964126   3.2473862   2.0717688   0.25982964\n",
      "   1.6800889   2.2794666  -2.0334225   2.9891813   3.211999    2.7046928\n",
      "   1.5497298   0.4201785   2.0209496  -0.15437861  1.1630772   0.40863457\n",
      "   1.3237605  -0.22805208  1.7022496   2.3690343   0.60024464  2.1112733\n",
      "   2.7929246 ]]...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.0001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "writer = SummaryWriter(\"NN_Diabetes/Lr/Train_e4_Long\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=2000, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(\"NN_Diabetes/Lr/Test_e4_Long\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f763fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b65b5ad5fc4d6bbbf4711401488f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Cumulative Loss: 7.5716, LR: 1.000e-03\n",
      "Epoch [40/200], Cumulative Loss: 0.5253, LR: 3.162e-04\n",
      "Epoch [60/200], Cumulative Loss: 0.5635, LR: 3.162e-04\n",
      "Epoch [80/200], Cumulative Loss: 0.6683, LR: 1.000e-04\n",
      "Epoch [100/200], Cumulative Loss: 0.4577, LR: 1.000e-04\n",
      "Epoch [120/200], Cumulative Loss: 0.5452, LR: 3.162e-05\n",
      "Epoch [140/200], Cumulative Loss: 0.5339, LR: 3.162e-05\n",
      "Epoch [160/200], Cumulative Loss: 0.7056, LR: 1.000e-05\n",
      "Epoch [180/200], Cumulative Loss: 1.8736, LR: 1.000e-05\n",
      "Epoch [200/200], Cumulative Loss: 0.3772, LR: 3.162e-06\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6363527774810791\n",
      "Final parameters: [[ 2.8847816   5.201524    0.12562682  5.690243    3.7675788   5.326632\n",
      "   3.0322957   3.411771    6.1306696   7.198477    1.9685651   5.1859236\n",
      "   5.302169    4.1409636   1.8312805   3.2916188   6.9305086   2.8173516\n",
      "   7.182851    3.118078    3.0049963   5.6659985   1.7903786   6.3867702\n",
      "   3.0520074   4.2430196   3.684196    2.0288446   7.0280037   7.519516\n",
      "   3.136249    6.3169765   8.489915    2.8443139   1.3786868   2.9365046\n",
      "   6.215981    2.6715107   8.550735    1.9952551   4.051559    5.550188\n",
      "   1.3762227   5.1211777   3.7941482   2.0323396   4.327264    3.1674554\n",
      "   3.929958    5.8744864   4.661954    5.1790614   6.5261755   3.9856493\n",
      "   2.8899639   4.1811852   6.519982    2.7778413   7.3866034   1.6374062\n",
      "   5.765613    6.6322317   2.039308    6.410855    4.412134    2.2626548\n",
      "   1.8510617   2.0483294   6.836633    6.39923     2.6801908   5.185065\n",
      "   5.319801    3.5245488   1.8729872   4.2997084   5.5269427   5.018071\n",
      "   8.52463     2.2654305   2.601165    7.8215075   1.7778846   5.9950266\n",
      "   1.5222542  -2.3143942  -0.45706022  3.0749326   6.9008164   7.5335464\n",
      "   1.4710028   7.626655    6.5676      1.4989312   1.6206326   2.0293782\n",
      "   5.6497316   2.7871332   8.570977    0.21640868  3.4016585   5.905758\n",
      "   3.006273    7.2680187   3.0225239   2.7873542   1.5028135   3.598006\n",
      "   4.3324842   5.167543    3.1427922   7.195017    6.231094    2.0876572\n",
      "   2.733875    3.3779225   5.4104404   3.6358333   8.261106    2.1787298\n",
      "   2.7769933   5.966949    2.9804132   6.0241814   4.428572    5.155393\n",
      "   6.544729    1.9585867   6.000519    7.534758    2.2712617   7.8526397\n",
      "   5.7385025   4.943253    7.1739564   3.1290567   7.170444    3.0252092\n",
      "   6.722217    2.8645642   3.5168478   6.632989    2.2873936   3.5426576\n",
      "   3.829647    1.0089954   2.1266289   2.2696648   5.3996925   5.923236\n",
      "   3.4859307   5.049355    6.9915557   1.6643659   2.849626    6.1929655\n",
      "   6.5694723   4.2721543   7.8706875   3.6405272   5.259101    5.4696774\n",
      "   1.8015226   5.484941    4.0664697   3.0676117   2.0878768   3.0384371\n",
      "   5.209461    5.359711    4.330631    4.509534    6.213873    2.7984233\n",
      "   1.9952669   4.217833    5.3824534   2.5451183   9.522435    3.256952\n",
      "   3.0049822   6.7685075   0.11119577  5.5895224   3.9362786   3.11802\n",
      "   1.0894897   2.3979185   6.343269    4.202535    3.4401176   5.7735806\n",
      "   5.777725    2.5880487   2.9473898   4.369895    3.7728832   3.8828\n",
      "   8.48854     3.3178256   2.3838637   4.2764993   0.8468301   4.0018435\n",
      "   1.7574973  -0.53179836 -0.92364025  1.6820475   4.608758    3.7252192\n",
      "   2.4410074   2.9160156   5.3248744   0.60935163 -0.7573351   4.769636\n",
      "   3.992531    2.9655352   3.0448346   1.1627594   0.46653485 -0.5600327\n",
      "   2.3939066  -0.8206838   0.4275008   1.2206572   1.8213115   2.080086\n",
      "  -0.8170775  -0.32008398  1.1121477  -0.9646491  -0.23110852  2.0246298\n",
      "   0.8391494   0.1434464  -0.2736122   0.936594   -0.29114574  1.2038246\n",
      "  -0.11994503]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes/Lr/Train_Step\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes/Lr/Test_Step\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(meta_optimizer, step_size=40, gamma=np.sqrt(0.1))\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Lr/Train_Step\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=200, min_horizon=500, max_horizon=500, discount=0, writer=writer, scheduler=scheduler)\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Lr/Test_Step\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c267a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f264d11b3e374cc7be0d6dbe312a54aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 49678488.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 64455.5508, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 3.5796, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.0915, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 1.1040, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 2.4050, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.4086, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.7440, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.8023, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.3376, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.8964078426361084\n",
      "Final parameters: [[ 4.333922   15.034754   13.66389    14.237004    3.0719428   2.0987694\n",
      "   3.0590067   3.9937625  14.728879    1.8864397   5.748864    1.6719899\n",
      "   3.6101408   3.6434765   3.923642    3.968352   15.279895    5.018868\n",
      "   2.9828792   3.8431978   3.0129983  14.562385   14.268546   13.784624\n",
      "   2.5398457   1.4271225   3.5619824   6.438787   12.611475    2.3104882\n",
      "   6.3358693   6.262538    3.8532155   3.267823    3.9552207   3.3307333\n",
      "  15.616351    5.784325    2.2612224   2.6658309   2.8070412  13.625981\n",
      "  12.273794   15.098562    4.41223     3.3481722   7.765068    4.3105793\n",
      "  15.572577    2.078752    4.724       7.0605593   3.2835307   3.6716807\n",
      "   3.8167672   4.597835   14.662445    3.384881    4.451996    2.78635\n",
      "   3.7865522  12.720732   14.652348   16.099234    3.4199548   1.4784786\n",
      "   4.5249543   4.59589    14.645419    4.8054724   1.4422783   4.832049\n",
      "   3.858333    4.8223424   4.1791434   3.1739824  13.742615    4.648925\n",
      "   2.7275958   3.5596135   0.2205348  14.968422   13.777108   14.863632\n",
      "   2.4227958   0.3335142   0.4725182   3.0505943  15.439736    0.41596603\n",
      "   1.8926758   3.6267047   2.694456    2.8714054   1.6292721   0.7079338\n",
      "  16.765348    3.3409498   1.8155378   0.8341959   4.0005755  13.207624\n",
      "  14.870413   12.883663    4.3913627   3.4765058   4.8712087   4.7279882\n",
      "  12.246937    2.8745813   2.1222217   4.1109133   4.0434284   6.138626\n",
      "   3.542505    3.7661436  14.657409    4.2335224   3.1761115   4.7304854\n",
      "   4.2663827  12.767529   12.424916   11.776242    4.949262    2.5341425\n",
      "   4.870744    6.6943836  11.880258    4.734645    5.7369475   7.214332\n",
      "   6.7580023   6.4750543   6.2350183  12.005497   13.492491    7.295415\n",
      "   4.9866796   4.787972    6.0270147  11.439343   11.895659   11.691507\n",
      "   5.013104    2.8116446   5.20218     5.310001   13.374496    4.977497\n",
      "   2.0760288   5.8889756   3.3865852   5.307256    4.3642783   4.731903\n",
      "  11.838233    3.2389834   4.2686      4.81003     4.8558135   9.927202\n",
      "  13.40932    14.193243    3.4282367   2.204739    3.407484    3.6593199\n",
      "  11.807404    3.6593575   2.793933    4.5887785   5.094972    4.3759427\n",
      "   3.4385114   4.261754   14.812556    2.845943    3.1896656   4.340284\n",
      "   3.2684047  12.527743   15.73892    12.64037     3.5597157   0.57877755\n",
      "   4.241694    4.091629   13.42207     3.1180024   5.825806    6.2769475\n",
      "   2.4308462   2.668991    2.2628067   3.542661   15.600931    2.4052122\n",
      "   2.8924968   2.8455365   2.3778906  11.34965    12.372046   11.444302\n",
      "   2.4083695   1.9933385   3.1766303   3.6905165  10.531372    3.091468\n",
      "   0.84951437  4.0084987   1.8833352   3.9777112   1.9366171  -1.3627754\n",
      "  12.410322    0.8997606   2.424569    2.4374242   1.3637978  -1.5632923\n",
      "  -0.7115727  -0.77156556  1.6365582   2.4218774   0.8619645   0.6242376\n",
      "  -0.3425696   2.2994487   1.6334168   0.43317932  0.49377352  0.6193102\n",
      "   0.76809746  2.2432497  -0.32197225  0.48294964  1.8356622   1.6110898\n",
      "   1.5726053 ]]...\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes/Comparison/Train_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes/Comparison/Test_Right_Horizon\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Comparison/Train_Right_Horizon\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=800, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"NN_Diabetes/Comparison/Test_Right_Horizon\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a43a5",
   "metadata": {},
   "source": [
    "## NN Optimizee - Multi Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e125cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa6b1c156ae4c3db1caed24da209bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 11.6563, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.0274, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.0227, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.0496, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.0429, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.0224, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.0228, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.0287, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.0519, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.0221, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.055267542600631714\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NNDiabetes_Multi/Train_Dir_11\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NNDiabetes_Multi/Test_Dir_11\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "\n",
    "initializer = Data_Initializer(XYNNOptimizee, kwargs, num_optims=10, subset_size=40)\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(\"NNDiabetes_Multi/Train_Dir_11\") \n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(\"NNDiabetes_Multi/Test_Dir_11\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2d43a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1022d03e149f4b518d6b61fd1fd5d04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 9.3712, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.6942, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.7742, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.0797, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.9257, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8957, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6777, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.8461, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.2062, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.3523, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.2804855108261108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12cb0cacdd34c9587705d1d7c18e40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1.8720, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.0809, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8563, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5012, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7889, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0272, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6731, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6559, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.9176, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5971, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5302165746688843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3edd1303fa48a88ce399df60415e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.8952, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.2487, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8769, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.5273, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8627, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7031, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8705, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6246, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6137, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5438, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.8982424139976501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa48d5f64fd44846b5e341f42b0c9e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 0.7295, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.2545, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.9127, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.0445, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8711, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.5838, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.4326, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.7885, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.3083, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.0391, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.7224278450012207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f729e321bedc412cbdc2d5eb2515395c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.6271, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 3.6102, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.9125, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.6189, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8111, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.6105, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.9432, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.4965, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6080, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5315, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6149126887321472\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(40, 140, 20):\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/SubSize/Train_{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/SubSize/Test_{i}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "\n",
    "    initializer = Data_Initializer(XYNNOptimizee, kwargs, num_optims=5, subset_size=i)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(f\"NNDiabetes_Multi/SubSize/Train_{i}\") \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(f\"NNDiabetes_Multi/SubSize/Test_{i}\")\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1afdce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527c732b197846569a916380c3a09e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.4605, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.4844, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.9941, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.6039, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.4922, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.6912, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8025, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.5898, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.0929, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.0338, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.0842245817184448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4421d18e524b738d186130118024da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 21.8032, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.4232, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.9033, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5923, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.5541, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7923, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 9.1531, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 4.8190, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.7431, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 4.5322, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.9238519072532654\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.6,0.8,0.2):\n",
    "    i = float(str(f\"{i:.2f}\"))\n",
    "    i_str = str(i).replace(\".\", \"\")\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet/Train_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet/Test_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "    dist = Dirichlet(torch.tensor([i, i]))\n",
    "\n",
    "    initializer = Data_Initializer(XYNNOptimizee, kwargs, distribution=dist, num_optims=5, subset_size=60)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet/Train_Dir_\" + i_str) \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet/Test_Dir_\" + i_str)\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0eda44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343cb129f07f4859bb6bd229012a2b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 3.8885, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 3.1569, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6032, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.3146, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.5266, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8746, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8030, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.4714, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.5087, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 2.7344, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.6261980533599854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56edfaaf893345c9a1913b4ad34b94d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.9358, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.0775, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6241, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 2.1537, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7499, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7232, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3988, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.9394, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.2381, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.5663, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.9663375020027161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddd0b10e86f43d1aec24636871ade55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 5.0868, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 2.9498, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.1802, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.7046, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.4853, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.7676, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.6014, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6284, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 2.6566, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.9823, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.8411714434623718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1243e674aa384ff9b681f44cd8077770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1.8720, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.0809, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8563, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5012, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7889, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0272, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.6731, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.6559, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.9176, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.5971, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5302165746688843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2396ec1de44c4132b68ea17f2e079219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 28.4856, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.6673, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.1037, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 2.5722, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 6.1662, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 7.6745, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 1.2455, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 4.9773, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 6.8608, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.6939, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 3.2333266735076904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488c47bb049542ec9a5edbd33208ba79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1.9181, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 1.2097, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 1.2240, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.4632, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6835, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.7340, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.5765, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.8840, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.8503, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.4778, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5290380120277405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7025531e1bb549418faaea1c156908a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 2.0031, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 13.8098, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 4.0023, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.3252, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 3.0893, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0997, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.9132, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.5231, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.6686, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 2.2863, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.0342798233032227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a395f4e6fb045728a600335b42ac88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 4.2231, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.5239, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.8191, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 1.1897, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.8875, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8233, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.8568, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 1.3659, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.3039, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.9797, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 1.1990711688995361\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.25,2.25,0.25):\n",
    "    i = float(str(f\"{i:.3f}\"))\n",
    "    i_str = str(i).replace(\".\", \"\")\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet2/Train_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "    shutil.rmtree(f\"NNDiabetes_Multi/Dirichlet2/Test_Dir_{i_str}\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "    kwargs = {\"X\": X, \"y\": y, \"hidden_size\": 20, \"num_layers\": 2, \"num_samples\":100, \"loss_fn\":nn.MSELoss()}\n",
    "    dist = Dirichlet(torch.tensor([i, i]))\n",
    "\n",
    "    initializer = Data_Initializer(XYNNOptimizee, kwargs, distribution=dist, num_optims=5, subset_size=60)\n",
    "\n",
    "    torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "    meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet2/Train_Dir_\" + i_str) \n",
    "    lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "    writer = SummaryWriter(\"NNDiabetes_Multi/Dirichlet2/Test_Dir_\" + i_str)\n",
    "    params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12415463",
   "metadata": {},
   "source": [
    "## NN Diab Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25bcb730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ff08f055b84b7c98b26b44903f4218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 8006432.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 9.7166, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 2.0988, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.7331, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.7267, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 1.0989, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.4511, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.5665, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.6034, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 1.1383, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.5529805421829224\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Tr_Single\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Ts_Single\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Tr_Single\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Ts_Single\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f18900b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32030d0661da4471ac6612c2385a672b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 5844946.0000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 4.8254, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.4726, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.5150, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.4103, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.5669, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3464, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.3258, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.4290, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.3969, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.3394172787666321\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Tr_Loss\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Ts_Loss\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss(), nn.L1Loss(), nn.SmoothL1Loss(), nn.BCEWithLogitsLoss()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Tr_Loss\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Ts_Loss\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c59c9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e62b07fbe74549a87679576f85d782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 1334267.5000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 15.4812, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 6.8238, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.7594, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.6183, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.8165, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 5.4469, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 2.4528, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 1.7113, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 3.8405, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.892021119594574\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Tr_Activation\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Ts_Activation\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()], \"activation_fn\":[nn.ReLU(), nn.Tanh(), nn.Sigmoid(), nn.SiLU()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Tr_Activation\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Ts_Activation\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45cc0bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6167b393dc419d87d6d312127a5200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 6967467.5000, LR: 1.000e-03\n",
      "Epoch [20/100], Cumulative Loss: 0.8352, LR: 1.000e-03\n",
      "Epoch [30/100], Cumulative Loss: 0.6225, LR: 1.000e-03\n",
      "Epoch [40/100], Cumulative Loss: 0.3854, LR: 1.000e-03\n",
      "Epoch [50/100], Cumulative Loss: 0.3643, LR: 1.000e-03\n",
      "Epoch [60/100], Cumulative Loss: 0.5370, LR: 1.000e-03\n",
      "Epoch [70/100], Cumulative Loss: 0.3581, LR: 1.000e-03\n",
      "Epoch [80/100], Cumulative Loss: 0.3234, LR: 1.000e-03\n",
      "Epoch [90/100], Cumulative Loss: 0.3776, LR: 1.000e-03\n",
      "Epoch [100/100], Cumulative Loss: 0.4049, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.3693506121635437\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Tr_Both\", ignore_errors=True)  # Remove the directory if it exists\n",
    "shutil.rmtree(\"NN_Diabetes_Multi/Comparison/Ts_Both\", ignore_errors=True)  # Remove the directory if it exists\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss(), nn.L1Loss()], \"activation_fn\":[nn.ReLU(), nn.Sigmoid()]}\n",
    "\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "print(initializer.get_num_optims())\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Tr_Both\")\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, min_horizon=500, max_horizon=500, discount=0, writer=writer)\n",
    "writer = SummaryWriter(f\"NN_Diabetes_Multi/Comparison/Ts_Both\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4087816",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0964aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9658279ae9452383b1b1fba7572551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?time step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 0.23186102509498596\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Comparison/Adam\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "optimizee = XYNNOptimizee(X,y)\n",
    "optimizee.set_params()\n",
    "optimizer_cls = optim.Adam\n",
    "optimizer_kwargs = {\"lr\":0.01}\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Comparison/Adam\")\n",
    "optimizee.train_model(optimizer_cls, optimizer_kwargs, time_horizon=500, writer=writer)\n",
    "# optimizee.train_model(optimizer_cls, optimizer_kwargs, time_horizon=500, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488208b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad86ec4d50d347d1b75ac76811990310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?time step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 0.2659434378147125\n"
     ]
    }
   ],
   "source": [
    "# shutil.rmtree(\"NN_Comparison/SGD\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "optimizee = XYNNOptimizee(X,y)\n",
    "optimizee.set_params()\n",
    "optimizer_cls = optim.SGD\n",
    "optimizer_kwargs = {\"lr\":0.05}\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Comparison/SGD\")\n",
    "# optimizee.train_model(optimizer_cls, optimizer_kwargs, time_horizon=500, writer=writer)\n",
    "optimizee.train_model(optimizer_cls, optimizer_kwargs, time_horizon=500, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfa391c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feb6070f083457ebf26105ee228cf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/500 [00:00<?, ?time step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 0.2353810966014862\n"
     ]
    }
   ],
   "source": [
    "# shutil.rmtree(\"NN_Comparison/MSGD\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "optimizee = XYNNOptimizee(X,y)\n",
    "optimizee.set_params()\n",
    "optimizer_cls = optim.SGD\n",
    "optimizer_kwargs = {\"lr\":0.05, \"momentum\": 0.9}\n",
    "\n",
    "writer = SummaryWriter(f\"NN_Comparison/MSGD\")\n",
    "# optimizee.train_model(optimizer_cls, optimizer_kwargs, time_horizon=500, writer=writer)\n",
    "optimizee.train_model(optimizer_cls, optimizer_kwargs, time_horizon=500, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c72a286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec76fefabaf42d0856fcc7948d1614e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Cumulative Loss: 213020752.0000, LR: 1.000e-03\n",
      "Epoch [10/50], Cumulative Loss: 8006432.0000, LR: 1.000e-03\n",
      "Epoch [15/50], Cumulative Loss: 45590.1992, LR: 1.000e-03\n",
      "Epoch [20/50], Cumulative Loss: 9.7166, LR: 1.000e-03\n",
      "Epoch [25/50], Cumulative Loss: 0.8350, LR: 1.000e-03\n",
      "Epoch [30/50], Cumulative Loss: 2.0988, LR: 1.000e-03\n",
      "Epoch [35/50], Cumulative Loss: 0.6811, LR: 1.000e-03\n",
      "Epoch [40/50], Cumulative Loss: 0.7331, LR: 1.000e-03\n",
      "Epoch [45/50], Cumulative Loss: 0.4565, LR: 1.000e-03\n",
      "Epoch [50/50], Cumulative Loss: 0.7267, LR: 1.000e-03\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.9209152460098267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbd611edf684563a0b7a096885e16c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Cumulative Loss: 0.5450, LR: 1.000e-04\n",
      "Epoch [10/50], Cumulative Loss: 0.6523, LR: 1.000e-04\n",
      "Epoch [15/50], Cumulative Loss: 0.4074, LR: 1.000e-04\n",
      "Epoch [20/50], Cumulative Loss: 0.3281, LR: 1.000e-04\n",
      "Epoch [25/50], Cumulative Loss: 0.4269, LR: 1.000e-04\n",
      "Epoch [30/50], Cumulative Loss: 0.4663, LR: 1.000e-04\n",
      "Epoch [35/50], Cumulative Loss: 0.3490, LR: 1.000e-04\n",
      "Epoch [40/50], Cumulative Loss: 0.3610, LR: 1.000e-04\n",
      "Epoch [45/50], Cumulative Loss: 0.4363, LR: 1.000e-04\n",
      "Epoch [50/50], Cumulative Loss: 0.4247, LR: 1.000e-04\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.42720818519592285\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"NN_Comparison/LSTM\", ignore_errors=True)\n",
    "\n",
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "kwargs = {\"X\": [X], \"y\": [y], \"hidden_size\": [20], \"num_layers\": [2], \"num_samples\":[100], \"loss_fn\":[nn.MSELoss()]}\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent(num_optims=1, preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=50, min_horizon=500, max_horizon=500, discount=0)\n",
    "writer = SummaryWriter(\"NN_Comparison/LSTM_e3\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)\n",
    "\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.0001)\n",
    "initializer = Param_Initializer(XYNNOptimizee, kwargs)\n",
    "lstm_optimizer = train_LSTM(lstm_optimizer, meta_optimizer, initializer, num_epochs=50, min_horizon=500, max_horizon=500, discount=0)\n",
    "\n",
    "writer = SummaryWriter(\"NN_Comparison/LSTM_e4\")\n",
    "params = test_LSTM(lstm_optimizer, initializer, time_horizon=500, writer=writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
