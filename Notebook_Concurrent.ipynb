{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "\n",
    "from src.optimizee import *\n",
    "from src.initializer import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 - Output Convex Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMConcurrent_Post(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based optimizer as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_optims, hidden_size=20, preproc=True, preproc_factor=10, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preproc = preproc\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preproc_factor = torch.tensor(preproc_factor, device=self.device)\n",
    "        self.preproc_threshold = float(torch.exp(-self.preproc_factor))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.input_size = 2 * num_optims if preproc else 1 * num_optims\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, 2, batch_first=True).to(self.device)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_optims).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM optimizer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "            hidden_state (tuple): Hidden state of the LSTM (h, c).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output updates of shape (batch_size, sequence_length, 1).\n",
    "            tuple: Updated hidden state.\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        hidden_state = tuple(h.to(self.device) for h in hidden_state)\n",
    "\n",
    "        if self.preproc:\n",
    "            x = self.preprocess_gradients(x)\n",
    "\n",
    "        out, hidden_state = self.lstm(x, hidden_state)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out, hidden_state\n",
    "\n",
    "\n",
    "    def preprocess_gradients(self, gradients):\n",
    "        \"\"\" Applies log transformation & sign extraction to gradients. \"\"\"\n",
    "        gradients = gradients.data.to(self.device)\n",
    "        if len(gradients.size()) == 1:\n",
    "            gradients = gradients.unsqueeze(-1)\n",
    "\n",
    "        param_size = gradients.size(0)\n",
    "        num_optims = gradients.size(1)\n",
    "\n",
    "        preprocessed = torch.zeros(param_size, 2 * num_optims, device=self.device)\n",
    "\n",
    "        for i in range(num_optims):\n",
    "            gradient = gradients[:, i]\n",
    "            keep_grads = (torch.abs(gradient) >= self.preproc_threshold)\n",
    "\n",
    "            # Log transformation for large gradients\n",
    "            preprocessed[keep_grads, 2 * i] = (torch.log(torch.abs(gradient[keep_grads]) + 1e-8) / self.preproc_factor)\n",
    "            preprocessed[keep_grads, 2 * i + 1] = torch.sign(gradient[keep_grads])\n",
    "\n",
    "            # Direct scaling for small gradients\n",
    "            preprocessed[~keep_grads, 2 * i] = -1\n",
    "            preprocessed[~keep_grads, 2 * i + 1] = (float(torch.exp(self.preproc_factor)) * gradient[~keep_grads])\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # Initialize hidden & cell states for LSTM (one per parameter)\n",
    "        h0 = torch.zeros(2, self.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(2, self.hidden_size, device=self.device)\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_Post(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=200, discount=1, scheduler = None, writer=None):\n",
    "    \n",
    "    lstm_optimizer.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(meta_optimizer, factor=1.0, total_iters=num_epochs)\n",
    "\n",
    "\n",
    "    with tqdm(range(num_epochs), desc=\"Training Progress\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizees = initializer.initialize()\n",
    "            optimizees[0].set_params()\n",
    "            params = optimizees[0].all_parameters().to(device)\n",
    "            hidden_state = lstm_optimizer.initialize_hidden_state()\n",
    "            cumulative_loss = None\n",
    "            for t in range(time_horizon):\n",
    "                gradients = []\n",
    "                for i in range(len(optimizees)):\n",
    "                    optimizee = optimizees[i]\n",
    "                    loss, grad_params = optimizee.compute_loss(params, return_grad=True)\n",
    "                    if i == 0 and discount: cumulative_loss = loss*discount**(time_horizon-1) if cumulative_loss is None else cumulative_loss + loss*discount**(time_horizon-t-1)\n",
    "                    elif i==0: cumulative_loss = loss\n",
    "                    gradients.append(grad_params.squeeze().to(device))\n",
    "                    # if writer and i==0 and epoch==1: writer.add_scalar(\"Grad\", grad_params.squeeze().mean(), t)\n",
    "\n",
    "                grad_params = torch.stack(gradients).T\n",
    "                # print(grad_params.shape, len(optimizees))\n",
    "                output, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "                alpha = output.mean(dim=0)\n",
    "                lambda_ = nn.functional.softmax(alpha, dim=0)\n",
    "                update = grad_params @ lambda_\n",
    "                params = params + lstm_optimizer.lr * update.unsqueeze(-1)\n",
    "                # if writer and epoch==1: writer.add_scalar(\"Update\", update.mean(), t)\n",
    "                optimizees[0].set_params(params)\n",
    "\n",
    "\n",
    "            # Backpropagation through time (BPTT)\n",
    "            if writer: writer.add_scalar(\"Loss\", cumulative_loss, epoch)\n",
    "            meta_optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(lstm_optimizer.parameters(), 1)\n",
    "            meta_optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=cumulative_loss.item())\n",
    "\n",
    "            num_prints = num_epochs // 10\n",
    "            if (epoch + 1) % num_prints == 0:\n",
    "                current_lr = meta_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Cumulative Loss: {cumulative_loss.item():.4f}, LR: {current_lr:.3e}\")\n",
    "                print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return lstm_optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_LSTM_Post(lstm_optimizer, initializer, time_horizon=200, writer=None):\n",
    "    lstm_optimizer.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizees = initializer.initialize()\n",
    "    optimizees[0].set_params()\n",
    "    params = optimizees[0].all_parameters().to(device)\n",
    "    hidden_state = lstm_optimizer.initialize_hidden_state()\n",
    "\n",
    "    lambdas = []\n",
    "    for t in range(time_horizon):\n",
    "        gradients = []\n",
    "        for i in range(len(optimizees)):\n",
    "            optimizee = optimizees[i]\n",
    "            loss, grad_params = optimizee.compute_loss(params)\n",
    "            if writer and i==0: writer.add_scalar(\"Loss\", loss, t)\n",
    "            gradients.append(grad_params.squeeze().to(device))\n",
    "\n",
    "        grad_params = torch.stack(gradients).T\n",
    "        # if len(grad_params.shape)==1: grad_params = grad_params.unsqueeze(-1)\n",
    "\n",
    "        output, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "        alpha = output.sum(dim=0)\n",
    "        lambda_ = nn.functional.softmax(alpha, dim=0)\n",
    "        lambdas.append(lambda_)        \n",
    "        update = grad_params @ lambda_\n",
    "        params = params + lstm_optimizer.lr * update.unsqueeze(-1)\n",
    "        optimizees[0].set_params(params)\n",
    "\n",
    "    final_loss = optimizees[0].compute_loss(params, return_grad=False)\n",
    "    print(f\"Final Loss: {final_loss}\")\n",
    "    print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "    lambdas = torch.stack(lambdas).T\n",
    "    return params, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bce51f977746c0ac31a9ec9648803e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 39511.5156, LR: 1.000e-02\n",
      "Final parameters: [[-6.0194206   1.7483896  -8.478009   -3.5153334  -4.4033933  -5.215141\n",
      "   0.16786733  0.09905227 -3.0846622  -0.86051357]]...\n",
      "Epoch [20/100], Cumulative Loss: 11393.5645, LR: 1.000e-02\n",
      "Final parameters: [[ 1.0221771  -0.28389767 -4.0830293  -1.2902454  -2.0648732  -3.5640893\n",
      "  -2.3760672  -2.4591022   0.19383122 -0.7780546 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 33961.9570, LR: 1.000e-02\n",
      "Final parameters: [[-7.1104503   2.1884773  -6.0318775  -3.5795996  -4.606734   -4.7308726\n",
      "  -1.8622217   0.98911506 -2.1911707   0.9513819 ]]...\n",
      "Epoch [40/100], Cumulative Loss: 55718.1289, LR: 1.000e-02\n",
      "Final parameters: [[-10.690538     0.7943404   -7.4738164   -2.609048    -6.8816843\n",
      "   -3.9297535   -0.43910682  -0.64680797  -4.6128397   -1.8278034 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 102859.8125, LR: 1.000e-02\n",
      "Final parameters: [[-12.168209     1.5275154  -12.236728    -3.9083304  -11.193118\n",
      "   -7.2349505    0.41631803  -2.089656    -6.288732    -1.5971489 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 140285.7031, LR: 1.000e-02\n",
      "Final parameters: [[-16.592152    6.259378  -13.653502   -3.9687932  -7.9583454  -7.949267\n",
      "   -0.3503573   1.4146497  -6.700801   -5.2973123]]...\n",
      "Epoch [70/100], Cumulative Loss: 26855.7305, LR: 1.000e-02\n",
      "Final parameters: [[-5.73888    -3.829331   -2.680923   -5.250699   -0.38973567 -4.0348783\n",
      "  -3.1334786  -7.0839252  -2.4138277   0.67186445]]...\n",
      "Epoch [80/100], Cumulative Loss: 9906.6152, LR: 1.000e-02\n",
      "Final parameters: [[-0.73532814 -2.5571892  -3.99657    -3.503238   -0.16502063 -1.2514545\n",
      "  -1.0322825  -0.5002434  -2.7371101   1.9199228 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 32887.7266, LR: 1.000e-02\n",
      "Final parameters: [[-6.6359596  1.3084986 -5.6127944 -2.8420594 -3.5108783 -3.2093396\n",
      "  -1.684492  -1.6732314 -4.80557   -3.1661823]]...\n",
      "Epoch [100/100], Cumulative Loss: 49228.1953, LR: 1.000e-02\n",
      "Final parameters: [[-10.30286     -0.21284078  -8.390629    -1.7335075   -4.172199\n",
      "   -4.7852473   -1.3749797   -1.5029625   -2.507845    -2.5376194 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 240179.359375\n",
      "Final parameters: [[ 29.469275  -18.284155  -92.37174   -25.962055   -9.482152  -33.023468\n",
      "  -21.92011   -32.685688   -9.94907    -1.8947898]]...\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.03, 0.05, 0.02, 0.04]}\n",
    "\n",
    "initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent_Post(num_optims=initializer.get_num_optims(), preproc=True, learning_rate=1e-4)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "lstm_optimizer = train_LSTM_Post(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=0.9)\n",
    "params, lambdas = test_LSTM_Post(lstm_optimizer, initializer, time_horizon=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9690e-04, 1.1443e-04, 1.1356e-04,  ..., 1.2278e-04, 1.2277e-04,\n",
       "         1.2276e-04],\n",
       "        [1.1508e-07, 2.3006e-08, 2.2740e-08,  ..., 2.5501e-08, 2.5498e-08,\n",
       "         2.5495e-08],\n",
       "        [6.7602e-04, 4.0678e-04, 4.0516e-04,  ..., 4.3399e-04, 4.3398e-04,\n",
       "         4.3398e-04],\n",
       "        [3.7265e-04, 1.7844e-04, 1.7737e-04,  ..., 1.8482e-04, 1.8481e-04,\n",
       "         1.8480e-04],\n",
       "        [9.9865e-01, 9.9930e-01, 9.9930e-01,  ..., 9.9926e-01, 9.9926e-01,\n",
       "         9.9926e-01]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 - Input Convex Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMConcurrent_Pre(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based optimizer as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_optims, hidden_size=20, preproc=True, preproc_factor=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preproc = preproc\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preproc_factor = torch.tensor(preproc_factor, device=self.device)\n",
    "        self.preproc_threshold = float(torch.exp(-self.preproc_factor))\n",
    "        \n",
    "        self.input_layer = nn.Linear(num_optims, 1, bias=False).to(self.device)\n",
    "        self.input_size = 2 if preproc else 1\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, 2, batch_first=True).to(self.device)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM optimizer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "            hidden_state (tuple): Hidden state of the LSTM (h, c).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output updates of shape (batch_size, sequence_length, 1).\n",
    "            tuple: Updated hidden state.\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        hidden_state = tuple(h.to(self.device) for h in hidden_state)\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        if self.preproc:\n",
    "            x = self.preprocess_gradients(x)\n",
    "\n",
    "        out, hidden_state = self.lstm(x, hidden_state)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out, hidden_state\n",
    "\n",
    "\n",
    "    def preprocess_gradients(self, gradients):\n",
    "        \"\"\" Applies log transformation & sign extraction to gradients. \"\"\"\n",
    "        gradients = gradients.data.to(self.device)\n",
    "        if len(gradients.size()) == 1:\n",
    "            gradients = gradients.unsqueeze(-1)\n",
    "\n",
    "        param_size = gradients.size(0)\n",
    "        num_optims = gradients.size(1)\n",
    "\n",
    "        preprocessed = torch.zeros(param_size, 2 * num_optims, device=self.device)\n",
    "\n",
    "        for i in range(num_optims):\n",
    "            gradient = gradients[:, i]\n",
    "            keep_grads = (torch.abs(gradient) >= self.preproc_threshold)\n",
    "\n",
    "            # Log transformation for large gradients\n",
    "            preprocessed[keep_grads, 2 * i] = (torch.log(torch.abs(gradient[keep_grads]) + 1e-8) / self.preproc_factor)\n",
    "            preprocessed[keep_grads, 2 * i + 1] = torch.sign(gradient[keep_grads])\n",
    "\n",
    "            # Direct scaling for small gradients\n",
    "            preprocessed[~keep_grads, 2 * i] = -1\n",
    "            preprocessed[~keep_grads, 2 * i + 1] = (float(torch.exp(self.preproc_factor)) * gradient[~keep_grads])\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # Initialize hidden & cell states for LSTM (one per parameter)\n",
    "        h0 = torch.zeros(2, self.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(2, self.hidden_size, device=self.device)\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_Pre(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=200, discount=1, scheduler = None, writer=None):\n",
    "    \n",
    "    lstm_optimizer.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(meta_optimizer, factor=1.0, total_iters=num_epochs)\n",
    "\n",
    "\n",
    "    with tqdm(range(num_epochs), desc=\"Training Progress\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizees = initializer.initialize()\n",
    "            optimizees[0].set_params()\n",
    "            params = optimizees[0].all_parameters().to(device)\n",
    "            hidden_state = lstm_optimizer.initialize_hidden_state()\n",
    "            cumulative_loss = None\n",
    "            for t in range(time_horizon):\n",
    "                gradients = []\n",
    "                for i in range(len(optimizees)):\n",
    "                    optimizee = optimizees[i]\n",
    "                    loss, grad_params = optimizee.compute_loss(params, return_grad=True)\n",
    "                    if i == 0 and discount: cumulative_loss = loss*discount**(time_horizon-1) if cumulative_loss is None else cumulative_loss + loss*discount**(time_horizon-t-1)\n",
    "                    elif i==0: cumulative_loss = loss\n",
    "                    gradients.append(grad_params.squeeze().to(device))\n",
    "                    # if writer and i==0 and epoch==1: writer.add_scalar(\"Grad\", grad_params.squeeze().mean(), t)\n",
    "\n",
    "                grad_params = torch.stack(gradients).T\n",
    "                # print(grad_params.shape, len(optimizees))\n",
    "                update, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "                params = params + update\n",
    "                # if writer and epoch==1: writer.add_scalar(\"Update\", update.mean(), t)\n",
    "                optimizees[0].set_params(params)\n",
    "\n",
    "\n",
    "            # Backpropagation through time (BPTT)\n",
    "            if writer: writer.add_scalar(\"Loss\", cumulative_loss, epoch)\n",
    "            meta_optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(lstm_optimizer.parameters(), 1)\n",
    "            meta_optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=cumulative_loss.item())\n",
    "\n",
    "            num_prints = num_epochs // 10\n",
    "            if (epoch + 1) % num_prints == 0:\n",
    "                current_lr = meta_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Cumulative Loss: {cumulative_loss.item():.4f}, LR: {current_lr:.3e}\")\n",
    "                print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return lstm_optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_LSTM_Pre(lstm_optimizer, initializer, time_horizon=200, writer=None):\n",
    "    lstm_optimizer.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizees = initializer.initialize()\n",
    "    optimizees[0].set_params()\n",
    "    params = optimizees[0].all_parameters().to(device)\n",
    "    hidden_state = lstm_optimizer.initialize_hidden_state()\n",
    "\n",
    "    for t in range(time_horizon):\n",
    "        gradients = []\n",
    "        for i in range(len(optimizees)):\n",
    "            optimizee = optimizees[i]\n",
    "            loss, grad_params = optimizee.compute_loss(params)\n",
    "            if writer and i==0: writer.add_scalar(\"Loss\", loss, t)\n",
    "            gradients.append(grad_params.squeeze().to(device))\n",
    "\n",
    "        grad_params = torch.stack(gradients).T\n",
    "        # if len(grad_params.shape)==1: grad_params = grad_params.unsqueeze(-1)\n",
    "\n",
    "        update, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "        params = params + update\n",
    "        optimizees[0].set_params(params)\n",
    "\n",
    "    final_loss = optimizees[0].compute_loss(params, return_grad=False)\n",
    "    print(f\"Final Loss: {final_loss}\")\n",
    "    print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "    return params, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cf2dae8f774904b24b1c3146347ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 14.4609, LR: 1.000e-02\n",
      "Final parameters: [[1.2612435  0.76025265 1.4085405  1.5638216  0.44637197 0.6904131\n",
      "  0.6698243  1.4570692  0.1904761  2.0461488 ]]...\n",
      "Epoch [20/100], Cumulative Loss: 19.9983, LR: 1.000e-02\n",
      "Final parameters: [[1.0428268  1.4419596  0.9861763  0.52929705 1.2703655  0.9480345\n",
      "  0.61638314 1.4384967  1.4138949  1.1917597 ]]...\n",
      "Epoch [30/100], Cumulative Loss: 20.0624, LR: 1.000e-02\n",
      "Final parameters: [[1.017815  2.0292325 1.0270126 0.5879456 1.2848508 1.353821  1.0104824\n",
      "  0.9744402 1.4916581 1.2162242]]...\n",
      "Epoch [40/100], Cumulative Loss: 13.9848, LR: 1.000e-02\n",
      "Final parameters: [[0.9706728  0.98461735 0.91066253 0.9710672  1.0461549  0.9121424\n",
      "  0.6231784  1.3609481  1.2046943  1.1919951 ]]...\n",
      "Epoch [50/100], Cumulative Loss: 13.7419, LR: 1.000e-02\n",
      "Final parameters: [[ 1.1615869   0.14044592  1.0822572   1.9291571   0.4783302   0.5552066\n",
      "   0.93265265  1.3953425  -0.17320907  1.6908815 ]]...\n",
      "Epoch [60/100], Cumulative Loss: 25.5629, LR: 1.000e-02\n",
      "Final parameters: [[0.68163276 1.4293755  0.63747007 0.2984969  1.5923573  1.7131929\n",
      "  1.5354394  0.39384842 1.9537317  0.14690916]]...\n",
      "Epoch [70/100], Cumulative Loss: 14.1947, LR: 1.000e-02\n",
      "Final parameters: [[ 1.1965791   0.2667109   1.1609969   1.9193732   0.6148246   0.507307\n",
      "   0.7471279   1.277875   -0.27156168  2.096279  ]]...\n",
      "Epoch [80/100], Cumulative Loss: 34.3513, LR: 1.000e-02\n",
      "Final parameters: [[0.86317635 0.9845873  0.38483456 0.40604725 1.6690779  1.1608548\n",
      "  1.5925258  0.901151   2.1051068  0.6506668 ]]...\n",
      "Epoch [90/100], Cumulative Loss: 10.1944, LR: 1.000e-02\n",
      "Final parameters: [[1.1761724  1.6894945  0.9751102  0.7760237  0.99961245 1.2830046\n",
      "  1.0297737  1.3065469  1.0216265  1.5168486 ]]...\n",
      "Epoch [100/100], Cumulative Loss: 11.9968, LR: 1.000e-02\n",
      "Final parameters: [[0.94068706 1.6853999  0.79673916 1.0970237  0.9215299  1.0171893\n",
      "  1.1761726  1.0456691  1.3022448  1.1922485 ]]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 3.0800228118896484\n",
      "Final parameters: [[0.6563476  1.054751   0.8786566  0.29404813 1.328726   1.0609622\n",
      "  1.0262622  0.55889153 1.6257396  0.59389776]]...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2291,  0.1965, -0.2289, -0.2277, -0.1912,  0.0397,  0.3152, -0.1998,\n",
       "         0.1685, -0.1750], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": list(np.arange(0.01, 0.11, 0.01))}\n",
    "\n",
    "initializer = Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent_Pre(num_optims=initializer.get_num_optims(), preproc=True)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "lstm_optimizer = train_LSTM_Pre(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=500, discount=0.9)\n",
    "params, lambdas = test_LSTM_Pre(lstm_optimizer, initializer, time_horizon=1000)\n",
    "\n",
    "nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1216, 0.0894, 0.1213, 0.1201, 0.0847, 0.0037, 0.2301, 0.0924, 0.0658,\n",
       "        0.0710], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters())\n",
    "v**2 / torch.norm(v)**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
