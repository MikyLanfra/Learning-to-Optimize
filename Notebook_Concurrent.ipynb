{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "\n",
    "from src.optimizee import *\n",
    "from src.initializer import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 - Output Convex Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMConcurrent_Post(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based optimizer as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_optims, hidden_size=20, preproc=True, preproc_factor=10, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preproc = preproc\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preproc_factor = torch.tensor(preproc_factor, device=self.device)\n",
    "        self.preproc_threshold = float(torch.exp(-self.preproc_factor))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.input_size = 2 * num_optims if preproc else 1 * num_optims\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, 2, batch_first=True).to(self.device)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_optims).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        x: (num_params, 1, input_size)\n",
    "        hidden_state: tuple of (h, c) with shape (num_layers, num_params, hidden_size)\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        if self.preproc:\n",
    "            x = self.preprocess_gradients(x)  # shape: (num_params, input_size)\n",
    "        \n",
    "        x = x.unsqueeze(1)  # (num_params, 1, input_size) to match LSTM's (batch, seq_len, input_size)\n",
    "\n",
    "        out, new_hidden_state = self.lstm(x, hidden_state)  # Efficient batch LSTM call\n",
    "        out = self.output_layer(out).squeeze(1)  # (num_params, 1, 1) → (num_params, 1)\n",
    "        return out, new_hidden_state\n",
    "\n",
    "\n",
    "    def preprocess_gradients(self, gradients):\n",
    "        \"\"\" Applies log transformation & sign extraction to gradients. \"\"\"\n",
    "        gradients = gradients.data.to(self.device)\n",
    "        if len(gradients.size()) == 1:\n",
    "            gradients = gradients.unsqueeze(-1)\n",
    "\n",
    "        param_size = gradients.size(0)\n",
    "        num_optims = gradients.size(1)\n",
    "\n",
    "        preprocessed = torch.zeros(param_size, 2 * num_optims, device=self.device)\n",
    "\n",
    "        for i in range(num_optims):\n",
    "            gradient = gradients[:, i]\n",
    "            keep_grads = (torch.abs(gradient) >= self.preproc_threshold)\n",
    "\n",
    "            # Log transformation for large gradients\n",
    "            preprocessed[keep_grads, 2 * i] = (torch.log(torch.abs(gradient[keep_grads]) + 1e-8) / self.preproc_factor)\n",
    "            preprocessed[keep_grads, 2 * i + 1] = torch.sign(gradient[keep_grads])\n",
    "\n",
    "            # Direct scaling for small gradients\n",
    "            preprocessed[~keep_grads, 2 * i] = -1\n",
    "            preprocessed[~keep_grads, 2 * i + 1] = (float(torch.exp(self.preproc_factor)) * gradient[~keep_grads])\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_hidden_state(self, num_params):\n",
    "        h0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        return (h0, c0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_Post(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=200, discount=1, scheduler = None, writer=None):\n",
    "    \n",
    "    lstm_optimizer.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(meta_optimizer, factor=1.0, total_iters=num_epochs)\n",
    "\n",
    "\n",
    "    with tqdm(range(num_epochs), desc=\"Training Progress\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizees = initializer.initialize()\n",
    "            optimizees[0].set_params()\n",
    "            params = optimizees[0].all_parameters().to(device)\n",
    "            hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "            cumulative_loss = None\n",
    "            for t in range(time_horizon):\n",
    "                gradients = []\n",
    "                for i in range(len(optimizees)):\n",
    "                    optimizee = optimizees[i]\n",
    "                    loss, grad_params = optimizee.compute_loss(params, return_grad=True)\n",
    "                    if i == 0 and discount: cumulative_loss = loss*discount**(time_horizon-1) if cumulative_loss is None else cumulative_loss + loss*discount**(time_horizon-t-1)\n",
    "                    elif i==0: cumulative_loss = loss\n",
    "                    gradients.append(grad_params.squeeze().to(device))\n",
    "                    # if writer and i==0 and epoch==1: writer.add_scalar(\"Grad\", grad_params.squeeze().mean(), t)\n",
    "\n",
    "                grad_params = torch.stack(gradients).T\n",
    "                # print(grad_params.shape, len(optimizees))\n",
    "                output, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "                alpha = output.mean(dim=0)\n",
    "                lambda_ = nn.functional.softmax(alpha, dim=0)\n",
    "                update = grad_params @ lambda_\n",
    "                params = params + lstm_optimizer.lr * update.unsqueeze(-1)\n",
    "                # if writer and epoch==1: writer.add_scalar(\"Update\", update.mean(), t)\n",
    "                optimizees[0].set_params(params)\n",
    "\n",
    "\n",
    "            # Backpropagation through time (BPTT)\n",
    "            if writer: writer.add_scalar(\"Loss\", cumulative_loss, epoch)\n",
    "            meta_optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(lstm_optimizer.parameters(), 1)\n",
    "            meta_optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=cumulative_loss.item())\n",
    "\n",
    "            num_prints = num_epochs // 10\n",
    "            if (epoch + 1) % num_prints == 0:\n",
    "                current_lr = meta_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Cumulative Loss: {cumulative_loss.item():.4f}, LR: {current_lr:.3e}\")\n",
    "                print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "                print(f\"Lambdas: {(lambda_.detach().cpu().numpy().T)[:10]}...\")\n",
    "\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return lstm_optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_LSTM_Post(lstm_optimizer, initializer, time_horizon=200, writer=None):\n",
    "    lstm_optimizer.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizees = initializer.initialize()\n",
    "    optimizees[0].set_params()\n",
    "    params = optimizees[0].all_parameters().to(device)\n",
    "    hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "\n",
    "    lambdas = []\n",
    "    for t in range(time_horizon):\n",
    "        gradients = []\n",
    "        for i in range(len(optimizees)):\n",
    "            optimizee = optimizees[i]\n",
    "            loss, grad_params = optimizee.compute_loss(params)\n",
    "            if writer and i==0: writer.add_scalar(\"Loss\", loss, t)\n",
    "            gradients.append(grad_params.squeeze().to(device))\n",
    "\n",
    "        grad_params = torch.stack(gradients).T\n",
    "        # if len(grad_params.shape)==1: grad_params = grad_params.unsqueeze(-1)\n",
    "\n",
    "        output, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "        alpha = output.sum(dim=0)\n",
    "        lambda_ = nn.functional.softmax(alpha, dim=0)\n",
    "        lambdas.append(lambda_)        \n",
    "        update = grad_params @ lambda_\n",
    "        params = params + lstm_optimizer.lr * update.unsqueeze(-1)\n",
    "        optimizees[0].set_params(params)\n",
    "\n",
    "    final_loss = optimizees[0].compute_loss(params, return_grad=False)\n",
    "    print(f\"Final Loss: {final_loss}\")\n",
    "    print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "    lambdas = torch.stack(lambdas).T\n",
    "    return params, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f613dd1fab4d7d9e247a304c560367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 538.5627, LR: 1.000e-03\n",
      "Final parameters: [[-0.87955356  0.52410847 -2.448278   -1.2886281  -1.0808189  -1.8231136\n",
      "   0.982601    0.5773259  -0.6236282   0.6687098 ]]...\n",
      "Lambdas: [0.2319119  0.16179897 0.18114139 0.17833005 0.24681768]...\n",
      "Epoch [20/100], Cumulative Loss: 207.1111, LR: 1.000e-03\n",
      "Final parameters: [[ 0.9308614   0.15551014 -0.13941224  0.11179635 -0.98587835 -1.4441396\n",
      "  -1.2371871  -0.83737427  0.82849985 -0.49477306]]...\n",
      "Lambdas: [0.2274417  0.16046692 0.18279006 0.18195894 0.24734229]...\n",
      "Epoch [30/100], Cumulative Loss: 451.1354, LR: 1.000e-03\n",
      "Final parameters: [[-1.6675334   0.8975835  -1.1228527  -1.4673119  -1.4349031  -1.6759653\n",
      "  -1.1139376   1.4055      0.14894584  2.3297148 ]]...\n",
      "Lambdas: [0.22810116 0.1586107  0.18177626 0.18184382 0.24966803]...\n",
      "Epoch [40/100], Cumulative Loss: 713.3618, LR: 1.000e-03\n",
      "Final parameters: [[-3.3408432  -0.9995748  -1.4017767  -0.05888959 -2.854169   -0.38491982\n",
      "   0.39384905 -0.51047796 -1.4998513   0.18541528]]...\n",
      "Lambdas: [0.22782037 0.15597491 0.17723316 0.18075459 0.25821698]...\n",
      "Epoch [50/100], Cumulative Loss: 1344.0227, LR: 1.000e-03\n",
      "Final parameters: [[-2.9226172  -0.8460137  -3.4049685  -0.45692617 -5.6072803  -1.9744143\n",
      "   1.5469359  -1.679071   -2.2334547   0.9753606 ]]...\n",
      "Lambdas: [0.2269354  0.15082242 0.17435203 0.17758797 0.27030218]...\n",
      "Epoch [60/100], Cumulative Loss: 1724.4312, LR: 1.000e-03\n",
      "Final parameters: [[-5.008464    2.8254774  -3.921664   -0.6045452  -1.3431795  -2.4419055\n",
      "   0.48885062  0.6978798  -1.8330346  -1.7108963 ]]...\n",
      "Lambdas: [0.22396086 0.14929223 0.17194374 0.18182757 0.27297556]...\n",
      "Epoch [70/100], Cumulative Loss: 641.1694, LR: 1.000e-03\n",
      "Final parameters: [[-2.1427014  -2.6454537   0.4673745  -1.7045645   0.6205437  -1.786325\n",
      "  -1.0152757  -3.74676    -0.58311075  0.39235777]]...\n",
      "Lambdas: [0.22413823 0.14895661 0.16953889 0.18311433 0.274252  ]...\n",
      "Epoch [80/100], Cumulative Loss: 195.5643, LR: 1.000e-03\n",
      "Final parameters: [[ 0.24271424 -1.8211244  -0.64523214 -1.660269    0.5032504   0.30395424\n",
      "   0.05118518  1.1434537  -1.7296582   1.9974768 ]]...\n",
      "Lambdas: [0.22270793 0.14786404 0.1698138  0.18473694 0.27487734]...\n",
      "Epoch [90/100], Cumulative Loss: 455.0304, LR: 1.000e-03\n",
      "Final parameters: [[-1.2323554   0.28142077 -0.75769633 -0.5980069  -0.6298226  -0.43576872\n",
      "  -0.80038506 -1.1086769  -2.3972101  -1.7769965 ]]...\n",
      "Lambdas: [0.22167954 0.1441396  0.17001049 0.18459837 0.2795719 ]...\n",
      "Epoch [100/100], Cumulative Loss: 659.2509, LR: 1.000e-03\n",
      "Final parameters: [[-3.6997204  -1.61172    -2.3895633   0.8668708  -0.54786026 -1.347853\n",
      "  -0.40322417 -0.99586993  0.40844616 -0.7832664 ]]...\n",
      "Lambdas: [0.224756   0.1457211  0.17049047 0.18268104 0.2763514 ]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 241364.328125\n",
      "Final parameters: [[ 29.171186 -18.274126 -92.62797  -26.14445   -9.610684 -33.249897\n",
      "  -22.050903 -32.85314  -10.079305  -1.928413]]...\n",
      "tensor([0.1105, 0.0014, 0.0071, 0.0137, 0.8674], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.05, 0.1, 0.15, 0.2]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent_Post(num_optims=initializer.get_num_optims(), preproc=True, learning_rate=1e-4)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "lstm_optimizer = train_LSTM_Post(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=200, discount=0)\n",
    "params, lambdas = test_LSTM_Post(lstm_optimizer, initializer, time_horizon=1000)\n",
    "\n",
    "print(lambdas[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 - Input Convex Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMConcurrent_Pre(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based optimizer as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_optims, hidden_size=20, preproc=True, preproc_factor=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preproc = preproc\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preproc_factor = torch.tensor(preproc_factor, device=self.device)\n",
    "        self.preproc_threshold = float(torch.exp(-self.preproc_factor))\n",
    "        \n",
    "        self.input_layer = nn.Linear(num_optims, 1, bias=False).to(self.device)\n",
    "        self.input_size = 2 if preproc else 1\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, 2, batch_first=True).to(self.device)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        x: (num_params, 1, input_size)\n",
    "        hidden_state: tuple of (h, c) with shape (num_layers, num_params, hidden_size)\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        alphas = nn.utils.parameters_to_vector(self.input_layer.parameters()).to(self.device)\n",
    "        lambdas_ = nn.functional.softmax(alphas, dim=0)\n",
    "        x = x @ lambdas_.unsqueeze(-1)  # (num_params, 1, input_size) @ (input_size, 1) = (num_params, 1, 1)\n",
    "                \n",
    "        if self.preproc:\n",
    "            x = self.preprocess_gradients(x)  # shape: (num_params, input_size)\n",
    "        \n",
    "        x = x.unsqueeze(1)  # (num_params, 1, input_size) to match LSTM's (batch, seq_len, input_size)\n",
    "\n",
    "        out, new_hidden_state = self.lstm(x, hidden_state)  # Efficient batch LSTM call\n",
    "        out = self.output_layer(out).squeeze(1)  # (num_params, 1, 1) → (num_params, 1)\n",
    "        return out, new_hidden_state\n",
    "\n",
    "\n",
    "    def preprocess_gradients(self, gradients):\n",
    "        \"\"\" Applies log transformation & sign extraction to gradients. \"\"\"\n",
    "        gradients = gradients.data.to(self.device)\n",
    "        if len(gradients.size()) == 1:\n",
    "            gradients = gradients.unsqueeze(-1)\n",
    "\n",
    "        param_size = gradients.size(0)\n",
    "        num_optims = gradients.size(1)\n",
    "\n",
    "        preprocessed = torch.zeros(param_size, 2 * num_optims, device=self.device)\n",
    "\n",
    "        for i in range(num_optims):\n",
    "            gradient = gradients[:, i]\n",
    "            keep_grads = (torch.abs(gradient) >= self.preproc_threshold)\n",
    "\n",
    "            # Log transformation for large gradients\n",
    "            preprocessed[keep_grads, 2 * i] = (torch.log(torch.abs(gradient[keep_grads]) + 1e-8) / self.preproc_factor)\n",
    "            preprocessed[keep_grads, 2 * i + 1] = torch.sign(gradient[keep_grads])\n",
    "\n",
    "            # Direct scaling for small gradients\n",
    "            preprocessed[~keep_grads, 2 * i] = -1\n",
    "            preprocessed[~keep_grads, 2 * i + 1] = (float(torch.exp(self.preproc_factor)) * gradient[~keep_grads])\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "\n",
    "    def initialize_hidden_state(self, num_params):\n",
    "        h0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        return (h0, c0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_Pre(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=200, discount=1, scheduler = None, writer=None):\n",
    "    \n",
    "    lstm_optimizer.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(meta_optimizer, factor=1.0, total_iters=num_epochs)\n",
    "\n",
    "    lambdas_ = torch.zeros((initializer.get_num_optims(), num_epochs), device=device)\n",
    "    with tqdm(range(num_epochs), desc=\"Training Progress\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizees = initializer.initialize()\n",
    "            optimizees[0].set_params()\n",
    "            params = optimizees[0].all_parameters().to(device)\n",
    "            hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "            cumulative_loss = None\n",
    "            for t in range(time_horizon):\n",
    "                gradients = []\n",
    "                for i in range(len(optimizees)):\n",
    "                    optimizee = optimizees[i]\n",
    "                    loss, grad_params = optimizee.compute_loss(params, return_grad=True)\n",
    "                    if i == 0 and discount: cumulative_loss = loss*discount**(time_horizon-1) if cumulative_loss is None else cumulative_loss + loss*discount**(time_horizon-t-1)\n",
    "                    elif i==0: cumulative_loss = loss\n",
    "                    gradients.append(grad_params.squeeze().to(device))\n",
    "                    # if writer and i==0 and epoch==1: writer.add_scalar(\"Grad\", grad_params.squeeze().mean(), t)\n",
    "\n",
    "                grad_params = torch.stack(gradients).T\n",
    "                # print(grad_params.shape, len(optimizees))\n",
    "                update, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "                params = params + update\n",
    "                # if writer and epoch==1: writer.add_scalar(\"Update\", update.mean(), t)\n",
    "                optimizees[0].set_params(params)\n",
    "\n",
    "\n",
    "            # Backpropagation through time (BPTT)\n",
    "            if writer: writer.add_scalar(\"Loss\", cumulative_loss, epoch)\n",
    "            meta_optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(lstm_optimizer.parameters(), 1)\n",
    "            meta_optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=cumulative_loss.item())\n",
    "\n",
    "            num_prints = num_epochs // 10\n",
    "            if (epoch + 1) % num_prints == 0:\n",
    "                current_lr = meta_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Cumulative Loss: {cumulative_loss.item():.4f}, LR: {current_lr:.3e}\")\n",
    "                print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "                print(f\"Input Weights: {(nn.functional.softmax(nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters()), dim=0))[:10]}...\")\n",
    "\n",
    "            lam = nn.functional.softmax(nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters()), dim=0)\n",
    "            lambdas_[:, epoch] = lam\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return lstm_optimizer, lambdas_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_LSTM_Pre(lstm_optimizer, initializer, time_horizon=200, writer=None):\n",
    "    lstm_optimizer.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizees = initializer.initialize()\n",
    "    optimizees[0].set_params()\n",
    "    params = optimizees[0].all_parameters().to(device)\n",
    "    hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "\n",
    "    for t in range(time_horizon):\n",
    "        gradients = []\n",
    "        for i in range(len(optimizees)):\n",
    "            optimizee = optimizees[i]\n",
    "            loss, grad_params = optimizee.compute_loss(params)\n",
    "            if writer and i==0: writer.add_scalar(\"Loss\", loss, t)\n",
    "            gradients.append(grad_params.squeeze().to(device))\n",
    "\n",
    "        grad_params = torch.stack(gradients).T\n",
    "        # if len(grad_params.shape)==1: grad_params = grad_params.unsqueeze(-1)\n",
    "\n",
    "        update, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "        params = params + update\n",
    "        optimizees[0].set_params(params)\n",
    "\n",
    "    final_loss = optimizees[0].compute_loss(params, return_grad=False)\n",
    "    print(f\"Final Loss: {final_loss}\")\n",
    "    print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ca2464533c48849622c4627c03e76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.W = torch.tensor(W, dtype=torch.float32)\n",
      "c:\\Users\\miche\\Documents\\GitHub\\Learning-to-Optimize\\src\\optimizee.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.theta0 = torch.tensor(theta0, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Cumulative Loss: 71.1911, LR: 1.000e-02\n",
      "Final parameters: [[1.8952897  1.258449   1.4465712  1.4452436  0.8949873  1.2395346\n",
      "  0.27024692 1.2885857  0.31817612 1.3181561 ]]...\n",
      "Input Weights: tensor([0.3378, 0.2999, 0.1484, 0.0960, 0.1179], grad_fn=<SliceBackward0>)...\n",
      "Epoch [200/1000], Cumulative Loss: 5.5919, LR: 1.000e-02\n",
      "Final parameters: [[0.81382895 1.6417365  0.7952496  0.34929067 1.259179   1.3180219\n",
      "  1.268008   0.7288328  2.1270819  0.7227667 ]]...\n",
      "Input Weights: tensor([0.3834, 0.2884, 0.1392, 0.0855, 0.1035], grad_fn=<SliceBackward0>)...\n",
      "Epoch [300/1000], Cumulative Loss: 5.4380, LR: 1.000e-02\n",
      "Final parameters: [[0.8893719  1.6021168  0.84854627 0.5145622  1.2484499  1.2609316\n",
      "  1.2271467  0.75881106 1.8907614  0.8247846 ]]...\n",
      "Input Weights: tensor([0.4698, 0.2583, 0.1185, 0.0717, 0.0816], grad_fn=<SliceBackward0>)...\n",
      "Epoch [400/1000], Cumulative Loss: 6.6139, LR: 1.000e-02\n",
      "Final parameters: [[0.96914804 0.8650543  0.9885403  0.95239615 0.9966526  1.0049834\n",
      "  1.0673919  0.9799667  1.0655487  0.80797005]]...\n",
      "Input Weights: tensor([0.5719, 0.2127, 0.0944, 0.0580, 0.0630], grad_fn=<SliceBackward0>)...\n",
      "Epoch [500/1000], Cumulative Loss: 11.4408, LR: 1.000e-02\n",
      "Final parameters: [[0.9990228  1.0512427  0.9816679  1.0866212  0.98014486 1.0052316\n",
      "  0.9918708  0.9953385  0.9099772  1.0707891 ]]...\n",
      "Input Weights: tensor([0.6633, 0.1693, 0.0745, 0.0455, 0.0475], grad_fn=<SliceBackward0>)...\n",
      "Epoch [600/1000], Cumulative Loss: 5.5506, LR: 1.000e-02\n",
      "Final parameters: [[1.1439154  0.53335387 1.1660272  1.4458344  0.75756896 0.7543804\n",
      "  0.7670789  1.2195538  0.13363911 1.2486502 ]]...\n",
      "Input Weights: tensor([0.7308, 0.1346, 0.0596, 0.0372, 0.0377], grad_fn=<SliceBackward0>)...\n",
      "Epoch [700/1000], Cumulative Loss: 2.3463, LR: 1.000e-02\n",
      "Final parameters: [[1.0397882  1.0448968  1.020774   1.058066   0.96223706 0.97932243\n",
      "  0.97328854 1.009154   0.8642005  1.0740767 ]]...\n",
      "Input Weights: tensor([0.7832, 0.1075, 0.0481, 0.0305, 0.0307], grad_fn=<SliceBackward0>)...\n",
      "Epoch [800/1000], Cumulative Loss: 3.6042, LR: 1.000e-02\n",
      "Final parameters: [[0.9502933 1.21234   0.9310522 0.7788965 1.1097678 1.107452  1.084366\n",
      "  0.9290986 1.3675497 0.9355789]]...\n",
      "Input Weights: tensor([0.8165, 0.0913, 0.0405, 0.0259, 0.0259], grad_fn=<SliceBackward0>)...\n",
      "Epoch [900/1000], Cumulative Loss: 3.0954, LR: 1.000e-02\n",
      "Final parameters: [[0.959375   1.1552336  0.95201176 0.8408373  1.0803413  1.0808648\n",
      "  1.0581323  0.93712974 1.2768544  0.9239674 ]]...\n",
      "Input Weights: tensor([0.8393, 0.0796, 0.0356, 0.0229, 0.0226], grad_fn=<SliceBackward0>)...\n",
      "Epoch [1000/1000], Cumulative Loss: 4.2635, LR: 1.000e-02\n",
      "Final parameters: [[1.0497031  0.75563484 1.058384   1.2414     0.8961162  0.90815145\n",
      "  0.9265303  1.0820944  0.63031507 1.0479609 ]]...\n",
      "Input Weights: tensor([0.8565, 0.0708, 0.0319, 0.0206, 0.0201], grad_fn=<SliceBackward0>)...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.015073861926794052\n",
      "Final parameters: [[1.1296906  0.6235871  1.1311792  1.4536742  0.7970426  0.77610475\n",
      "  0.84784013 1.1753976  0.22388615 1.1893595 ]]...\n",
      "tensor([0.8565, 0.0708, 0.0319, 0.0206, 0.0201], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.1, 0.2, 0.3, 0.4]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent_Pre(num_optims=initializer.get_num_optims(), preproc=False)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "lstm_optimizer, lambdas_ = train_LSTM_Pre(lstm_optimizer, meta_optimizer, initializer, num_epochs=1000, time_horizon=50, discount=0.9)\n",
    "params = test_LSTM_Pre(lstm_optimizer, initializer, time_horizon=50)\n",
    "\n",
    "v = nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters())\n",
    "print(nn.functional.softmax(v, dim=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
