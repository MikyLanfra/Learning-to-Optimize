{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # Import tqdm for Jupyter Notebook\n",
    "\n",
    "from src.optimizee import *\n",
    "from src.initializer import *\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 - Output Convex Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMConcurrent_Post(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based optimizer as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_optims, hidden_size=20, preproc=True, preproc_factor=10, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preproc = preproc\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preproc_factor = torch.tensor(preproc_factor, device=self.device)\n",
    "        self.preproc_threshold = float(torch.exp(-self.preproc_factor))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        self.input_size = 2 * num_optims if preproc else 1 * num_optims\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, 2, batch_first=True).to(self.device)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_optims).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        x: (num_params, 1, input_size)\n",
    "        hidden_state: tuple of (h, c) with shape (num_layers, num_params, hidden_size)\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        if self.preproc:\n",
    "            x = self.preprocess_gradients(x)  # shape: (num_params, input_size)\n",
    "        \n",
    "        x = x.unsqueeze(1)  # (num_params, 1, input_size) to match LSTM's (batch, seq_len, input_size)\n",
    "\n",
    "        out, new_hidden_state = self.lstm(x, hidden_state)  # Efficient batch LSTM call\n",
    "        out = self.output_layer(out).squeeze(1)  # (num_params, 1, 1) → (num_params, 1)\n",
    "        return out, new_hidden_state\n",
    "\n",
    "\n",
    "    def preprocess_gradients(self, gradients):\n",
    "        \"\"\" Applies log transformation & sign extraction to gradients. \"\"\"\n",
    "        gradients = gradients.data.to(self.device)\n",
    "        if len(gradients.size()) == 1:\n",
    "            gradients = gradients.unsqueeze(-1)\n",
    "\n",
    "        param_size = gradients.size(0)\n",
    "        num_optims = gradients.size(1)\n",
    "\n",
    "        preprocessed = torch.zeros(param_size, 2 * num_optims, device=self.device)\n",
    "\n",
    "        for i in range(num_optims):\n",
    "            gradient = gradients[:, i]\n",
    "            keep_grads = (torch.abs(gradient) >= self.preproc_threshold)\n",
    "\n",
    "            # Log transformation for large gradients\n",
    "            preprocessed[keep_grads, 2 * i] = (torch.log(torch.abs(gradient[keep_grads]) + 1e-8) / self.preproc_factor)\n",
    "            preprocessed[keep_grads, 2 * i + 1] = torch.sign(gradient[keep_grads])\n",
    "\n",
    "            # Direct scaling for small gradients\n",
    "            preprocessed[~keep_grads, 2 * i] = -1\n",
    "            preprocessed[~keep_grads, 2 * i + 1] = (float(torch.exp(self.preproc_factor)) * gradient[~keep_grads])\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_hidden_state(self, num_params):\n",
    "        h0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        return (h0, c0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_Post(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=200, discount=1, scheduler = None, writer=None):\n",
    "    \n",
    "    lstm_optimizer.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(meta_optimizer, factor=1.0, total_iters=num_epochs)\n",
    "\n",
    "\n",
    "    with tqdm(range(num_epochs), desc=\"Training Progress\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizees = initializer.initialize()\n",
    "            optimizees[0].set_params()\n",
    "            params = optimizees[0].all_parameters().to(device)\n",
    "            hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "            cumulative_loss = None\n",
    "            for t in range(time_horizon):\n",
    "                gradients = []\n",
    "                for i in range(len(optimizees)):\n",
    "                    optimizee = optimizees[i]\n",
    "                    loss, grad_params = optimizee.compute_loss(params, return_grad=True)\n",
    "                    if i == 0 and discount: cumulative_loss = loss*discount**(time_horizon-1) if cumulative_loss is None else cumulative_loss + loss*discount**(time_horizon-t-1)\n",
    "                    elif i==0: cumulative_loss = loss\n",
    "                    gradients.append(grad_params.squeeze().to(device))\n",
    "                    # if writer and i==0 and epoch==1: writer.add_scalar(\"Grad\", grad_params.squeeze().mean(), t)\n",
    "\n",
    "                grad_params = torch.stack(gradients).T\n",
    "                # print(grad_params.shape, len(optimizees))\n",
    "                output, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "                alpha = output.mean(dim=0)\n",
    "                lambda_ = nn.functional.softmax(alpha, dim=0)\n",
    "                update = grad_params @ lambda_\n",
    "                params = params + lstm_optimizer.lr * update.unsqueeze(-1)\n",
    "                # if writer and epoch==1: writer.add_scalar(\"Update\", update.mean(), t)\n",
    "                optimizees[0].set_params(params)\n",
    "\n",
    "\n",
    "            # Backpropagation through time (BPTT)\n",
    "            if writer: writer.add_scalar(\"Loss\", cumulative_loss, epoch)\n",
    "            meta_optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(lstm_optimizer.parameters(), 1)\n",
    "            meta_optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=cumulative_loss.item())\n",
    "\n",
    "            num_prints = num_epochs // 10\n",
    "            if (epoch + 1) % num_prints == 0:\n",
    "                current_lr = meta_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Cumulative Loss: {cumulative_loss.item():.4f}, LR: {current_lr:.3e}\")\n",
    "                print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "                print(f\"Lambdas: {(lambda_.detach().cpu().numpy().T)[:10]}...\")\n",
    "\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return lstm_optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_LSTM_Post(lstm_optimizer, initializer, time_horizon=200, writer=None):\n",
    "    lstm_optimizer.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizees = initializer.initialize()\n",
    "    optimizees[0].set_params()\n",
    "    params = optimizees[0].all_parameters().to(device)\n",
    "    hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "\n",
    "    lambdas = []\n",
    "    for t in range(time_horizon):\n",
    "        gradients = []\n",
    "        for i in range(len(optimizees)):\n",
    "            optimizee = optimizees[i]\n",
    "            loss, grad_params = optimizee.compute_loss(params)\n",
    "            if writer and i==0: writer.add_scalar(\"Loss\", loss, t)\n",
    "            gradients.append(grad_params.squeeze().to(device))\n",
    "\n",
    "        grad_params = torch.stack(gradients).T\n",
    "        # if len(grad_params.shape)==1: grad_params = grad_params.unsqueeze(-1)\n",
    "\n",
    "        output, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "        alpha = output.sum(dim=0)\n",
    "        lambda_ = nn.functional.softmax(alpha, dim=0)\n",
    "        lambdas.append(lambda_)        \n",
    "        update = grad_params @ lambda_\n",
    "        params = params + lstm_optimizer.lr * update.unsqueeze(-1)\n",
    "        optimizees[0].set_params(params)\n",
    "\n",
    "    final_loss = optimizees[0].compute_loss(params, return_grad=False)\n",
    "    print(f\"Final Loss: {final_loss}\")\n",
    "    print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "    lambdas = torch.stack(lambdas).T\n",
    "    return params, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f613dd1fab4d7d9e247a304c560367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Cumulative Loss: 538.5627, LR: 1.000e-03\n",
      "Final parameters: [[-0.87955356  0.52410847 -2.448278   -1.2886281  -1.0808189  -1.8231136\n",
      "   0.982601    0.5773259  -0.6236282   0.6687098 ]]...\n",
      "Lambdas: [0.2319119  0.16179897 0.18114139 0.17833005 0.24681768]...\n",
      "Epoch [20/100], Cumulative Loss: 207.1111, LR: 1.000e-03\n",
      "Final parameters: [[ 0.9308614   0.15551014 -0.13941224  0.11179635 -0.98587835 -1.4441396\n",
      "  -1.2371871  -0.83737427  0.82849985 -0.49477306]]...\n",
      "Lambdas: [0.2274417  0.16046692 0.18279006 0.18195894 0.24734229]...\n",
      "Epoch [30/100], Cumulative Loss: 451.1354, LR: 1.000e-03\n",
      "Final parameters: [[-1.6675334   0.8975835  -1.1228527  -1.4673119  -1.4349031  -1.6759653\n",
      "  -1.1139376   1.4055      0.14894584  2.3297148 ]]...\n",
      "Lambdas: [0.22810116 0.1586107  0.18177626 0.18184382 0.24966803]...\n",
      "Epoch [40/100], Cumulative Loss: 713.3618, LR: 1.000e-03\n",
      "Final parameters: [[-3.3408432  -0.9995748  -1.4017767  -0.05888959 -2.854169   -0.38491982\n",
      "   0.39384905 -0.51047796 -1.4998513   0.18541528]]...\n",
      "Lambdas: [0.22782037 0.15597491 0.17723316 0.18075459 0.25821698]...\n",
      "Epoch [50/100], Cumulative Loss: 1344.0227, LR: 1.000e-03\n",
      "Final parameters: [[-2.9226172  -0.8460137  -3.4049685  -0.45692617 -5.6072803  -1.9744143\n",
      "   1.5469359  -1.679071   -2.2334547   0.9753606 ]]...\n",
      "Lambdas: [0.2269354  0.15082242 0.17435203 0.17758797 0.27030218]...\n",
      "Epoch [60/100], Cumulative Loss: 1724.4312, LR: 1.000e-03\n",
      "Final parameters: [[-5.008464    2.8254774  -3.921664   -0.6045452  -1.3431795  -2.4419055\n",
      "   0.48885062  0.6978798  -1.8330346  -1.7108963 ]]...\n",
      "Lambdas: [0.22396086 0.14929223 0.17194374 0.18182757 0.27297556]...\n",
      "Epoch [70/100], Cumulative Loss: 641.1694, LR: 1.000e-03\n",
      "Final parameters: [[-2.1427014  -2.6454537   0.4673745  -1.7045645   0.6205437  -1.786325\n",
      "  -1.0152757  -3.74676    -0.58311075  0.39235777]]...\n",
      "Lambdas: [0.22413823 0.14895661 0.16953889 0.18311433 0.274252  ]...\n",
      "Epoch [80/100], Cumulative Loss: 195.5643, LR: 1.000e-03\n",
      "Final parameters: [[ 0.24271424 -1.8211244  -0.64523214 -1.660269    0.5032504   0.30395424\n",
      "   0.05118518  1.1434537  -1.7296582   1.9974768 ]]...\n",
      "Lambdas: [0.22270793 0.14786404 0.1698138  0.18473694 0.27487734]...\n",
      "Epoch [90/100], Cumulative Loss: 455.0304, LR: 1.000e-03\n",
      "Final parameters: [[-1.2323554   0.28142077 -0.75769633 -0.5980069  -0.6298226  -0.43576872\n",
      "  -0.80038506 -1.1086769  -2.3972101  -1.7769965 ]]...\n",
      "Lambdas: [0.22167954 0.1441396  0.17001049 0.18459837 0.2795719 ]...\n",
      "Epoch [100/100], Cumulative Loss: 659.2509, LR: 1.000e-03\n",
      "Final parameters: [[-3.6997204  -1.61172    -2.3895633   0.8668708  -0.54786026 -1.347853\n",
      "  -0.40322417 -0.99586993  0.40844616 -0.7832664 ]]...\n",
      "Lambdas: [0.224756   0.1457211  0.17049047 0.18268104 0.2763514 ]...\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 241364.328125\n",
      "Final parameters: [[ 29.171186 -18.274126 -92.62797  -26.14445   -9.610684 -33.249897\n",
      "  -22.050903 -32.85314  -10.079305  -1.928413]]...\n",
      "tensor([0.1105, 0.0014, 0.0071, 0.0137, 0.8674], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.05, 0.1, 0.15, 0.2]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent_Post(num_optims=initializer.get_num_optims(), preproc=True, learning_rate=1e-4)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.001)\n",
    "\n",
    "lstm_optimizer = train_LSTM_Post(lstm_optimizer, meta_optimizer, initializer, num_epochs=100, time_horizon=200, discount=0)\n",
    "params, lambdas = test_LSTM_Post(lstm_optimizer, initializer, time_horizon=1000)\n",
    "\n",
    "print(lambdas[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 - Input Convex Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMConcurrent_Pre(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based optimizer as described in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_optims, hidden_size=20, preproc=True, preproc_factor=10):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.preproc = preproc\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preproc_factor = torch.tensor(preproc_factor, device=self.device)\n",
    "        self.preproc_threshold = float(torch.exp(-self.preproc_factor))\n",
    "        \n",
    "        self.input_layer = nn.Linear(num_optims, 1, bias=False).to(self.device)\n",
    "        self.input_size = 2 if preproc else 1\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, 2, batch_first=True).to(self.device)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        x: (num_params, 1, input_size)\n",
    "        hidden_state: tuple of (h, c) with shape (num_layers, num_params, hidden_size)\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        alphas = nn.utils.parameters_to_vector(self.input_layer.parameters()).to(self.device)\n",
    "        lambdas_ = nn.functional.softmax(alphas, dim=0)\n",
    "        x = x @ lambdas_.unsqueeze(-1)  # (num_params, 1, input_size) @ (input_size, 1) = (num_params, 1, 1)\n",
    "                \n",
    "        if self.preproc:\n",
    "            x = self.preprocess_gradients(x)  # shape: (num_params, input_size)\n",
    "        \n",
    "        x = x.unsqueeze(1)  # (num_params, 1, input_size) to match LSTM's (batch, seq_len, input_size)\n",
    "\n",
    "        out, new_hidden_state = self.lstm(x, hidden_state)  # Efficient batch LSTM call\n",
    "        out = self.output_layer(out).squeeze(1)  # (num_params, 1, 1) → (num_params, 1)\n",
    "        return out, new_hidden_state\n",
    "\n",
    "\n",
    "    def preprocess_gradients(self, gradients):\n",
    "        \"\"\" Applies log transformation & sign extraction to gradients. \"\"\"\n",
    "        gradients = gradients.data.to(self.device)\n",
    "        if len(gradients.size()) == 1:\n",
    "            gradients = gradients.unsqueeze(-1)\n",
    "\n",
    "        param_size = gradients.size(0)\n",
    "        num_optims = gradients.size(1)\n",
    "\n",
    "        preprocessed = torch.zeros(param_size, 2 * num_optims, device=self.device)\n",
    "\n",
    "        for i in range(num_optims):\n",
    "            gradient = gradients[:, i]\n",
    "            keep_grads = (torch.abs(gradient) >= self.preproc_threshold)\n",
    "\n",
    "            # Log transformation for large gradients\n",
    "            preprocessed[keep_grads, 2 * i] = (torch.log(torch.abs(gradient[keep_grads]) + 1e-8) / self.preproc_factor)\n",
    "            preprocessed[keep_grads, 2 * i + 1] = torch.sign(gradient[keep_grads])\n",
    "\n",
    "            # Direct scaling for small gradients\n",
    "            preprocessed[~keep_grads, 2 * i] = -1\n",
    "            preprocessed[~keep_grads, 2 * i + 1] = (float(torch.exp(self.preproc_factor)) * gradient[~keep_grads])\n",
    "\n",
    "        return preprocessed\n",
    "\n",
    "\n",
    "    def initialize_hidden_state(self, num_params):\n",
    "        h0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        c0 = torch.zeros(2, num_params, self.hidden_size, device=self.device)\n",
    "        return (h0, c0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_Pre(lstm_optimizer, meta_optimizer, initializer, num_epochs=500, time_horizon=200, discount=1, scheduler = None, writer=None):\n",
    "    \n",
    "    lstm_optimizer.train()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if scheduler is None:\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(meta_optimizer, factor=1.0, total_iters=num_epochs)\n",
    "\n",
    "    lambdas_ = torch.zeros((initializer.get_num_optims(), num_epochs), device=device)\n",
    "    with tqdm(range(num_epochs), desc=\"Training Progress\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizees = initializer.initialize()\n",
    "            optimizees[0].set_params()\n",
    "            params = optimizees[0].all_parameters().to(device)\n",
    "            hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "            cumulative_loss = None\n",
    "            for t in range(time_horizon):\n",
    "                gradients = []\n",
    "                for i in range(len(optimizees)):\n",
    "                    optimizee = optimizees[i]\n",
    "                    loss, grad_params = optimizee.compute_loss(params, return_grad=True)\n",
    "                    if i == 0 and discount: cumulative_loss = loss*discount**(time_horizon-1) if cumulative_loss is None else cumulative_loss + loss*discount**(time_horizon-t-1)\n",
    "                    elif i==0: cumulative_loss = loss\n",
    "                    gradients.append(grad_params.squeeze().to(device))\n",
    "                    # if writer and i==0 and epoch==1: writer.add_scalar(\"Grad\", grad_params.squeeze().mean(), t)\n",
    "\n",
    "                grad_params = torch.stack(gradients).T\n",
    "                # print(grad_params.shape, len(optimizees))\n",
    "                update, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "                params = params + update\n",
    "                # if writer and epoch==1: writer.add_scalar(\"Update\", update.mean(), t)\n",
    "                optimizees[0].set_params(params)\n",
    "\n",
    "\n",
    "            # Backpropagation through time (BPTT)\n",
    "            if writer: writer.add_scalar(\"Loss\", cumulative_loss, epoch)\n",
    "            meta_optimizer.zero_grad()\n",
    "            cumulative_loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(lstm_optimizer.parameters(), 1)\n",
    "            meta_optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(loss=cumulative_loss.item())\n",
    "\n",
    "            num_prints = num_epochs // 10\n",
    "            if (epoch + 1) % num_prints == 0:\n",
    "                current_lr = meta_optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Cumulative Loss: {cumulative_loss.item():.4f}, LR: {current_lr:.3e}\")\n",
    "                print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "                print(f\"Input Weights: {(nn.functional.softmax(nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters()), dim=0))[:10]}...\")\n",
    "\n",
    "            lam = nn.functional.softmax(nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters()), dim=0)\n",
    "            if lam[0]>0.99: \n",
    "                print(\"Stopping at epoch\", epoch+1, \"due to convergence.\")\n",
    "                break\n",
    "            lambdas_[:, epoch] = lam\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return lstm_optimizer, lambdas_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_LSTM_Pre(lstm_optimizer, initializer, time_horizon=200, writer=None):\n",
    "    lstm_optimizer.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizees = initializer.initialize()\n",
    "    optimizees[0].set_params()\n",
    "    params = optimizees[0].all_parameters().to(device)\n",
    "    hidden_state = lstm_optimizer.initialize_hidden_state(params.size(0))\n",
    "\n",
    "    for t in range(time_horizon):\n",
    "        gradients = []\n",
    "        for i in range(len(optimizees)):\n",
    "            optimizee = optimizees[i]\n",
    "            loss, grad_params = optimizee.compute_loss(params)\n",
    "            if writer and i==0: writer.add_scalar(\"Loss\", loss, t)\n",
    "            gradients.append(grad_params.squeeze().to(device))\n",
    "\n",
    "        grad_params = torch.stack(gradients).T\n",
    "        # if len(grad_params.shape)==1: grad_params = grad_params.unsqueeze(-1)\n",
    "\n",
    "        update, hidden_state = lstm_optimizer(grad_params, hidden_state)\n",
    "        params = params + update\n",
    "        optimizees[0].set_params(params)\n",
    "\n",
    "    final_loss = optimizees[0].compute_loss(params, return_grad=False)\n",
    "    print(f\"Final Loss: {final_loss}\")\n",
    "    print(f\"Final parameters: {(params.detach().cpu().numpy().T)[:10]}...\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f94e4436694f1ba90750b9a3417842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/10000], Cumulative Loss: 4.2635, LR: 1.000e-02\n",
      "Final parameters: [[1.0497031  0.75563484 1.058384   1.2414     0.8961162  0.90815145\n",
      "  0.9265303  1.0820944  0.63031507 1.0479609 ]]...\n",
      "Input Weights: tensor([0.8565, 0.0708, 0.0319, 0.0206, 0.0201], grad_fn=<SliceBackward0>)...\n",
      "Epoch [2000/10000], Cumulative Loss: 1.6800, LR: 1.000e-02\n",
      "Final parameters: [[0.8665524  1.368811   0.8716192  0.5881127  1.2063721  1.2046357\n",
      "  1.1790186  0.8200624  1.7120963  0.79581356]]...\n",
      "Input Weights: tensor([0.9394, 0.0295, 0.0135, 0.0090, 0.0085], grad_fn=<SliceBackward0>)...\n",
      "Epoch [3000/10000], Cumulative Loss: 2.0430, LR: 1.000e-02\n",
      "Final parameters: [[1.0513903  0.8425856  1.0537324  1.1599139  0.9206704  0.92245924\n",
      "  0.92868984 1.0703173  0.69660276 1.0521542 ]]...\n",
      "Input Weights: tensor([0.9643, 0.0171, 0.0080, 0.0055, 0.0050], grad_fn=<SliceBackward0>)...\n",
      "Epoch [4000/10000], Cumulative Loss: 2.0883, LR: 1.000e-02\n",
      "Final parameters: [[0.91533995 1.3589557  0.9013477  0.6310213  1.1695914  1.1779194\n",
      "  1.1354332  0.8664016  1.6204888  0.8633855 ]]...\n",
      "Input Weights: tensor([0.9775, 0.0107, 0.0051, 0.0035, 0.0032], grad_fn=<SliceBackward0>)...\n",
      "Epoch [5000/10000], Cumulative Loss: 1.7157, LR: 1.000e-02\n",
      "Final parameters: [[1.1660005  0.5033178  1.1713479  1.5340359  0.7311176  0.7360204\n",
      "  0.77783346 1.2232666  0.07541613 1.2419124 ]]...\n",
      "Input Weights: tensor([0.9829, 0.0080, 0.0038, 0.0027, 0.0026], grad_fn=<SliceBackward0>)...\n",
      "Epoch [6000/10000], Cumulative Loss: 1.4608, LR: 1.000e-02\n",
      "Final parameters: [[0.96000034 1.1336731  0.95885825 0.8647509  1.0639901  1.0728638\n",
      "  1.0566458  0.9395793  1.2362278  0.9339076 ]]...\n",
      "Input Weights: tensor([0.9880, 0.0056, 0.0027, 0.0019, 0.0019], grad_fn=<SliceBackward0>)...\n",
      "Stopping at epoch 6532 due to convergence.\n",
      "\n",
      "Training complete!\n",
      "Final Loss: 0.0002693661372177303\n",
      "Final parameters: [[0.99416035 1.0327859  0.99378717 0.97128993 1.0031755  1.006482\n",
      "  1.0021769  0.9884368  1.0364027  0.9964203 ]]...\n",
      "tensor([0.9900, 0.0046, 0.0022, 0.0016, 0.0015], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Set random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "n=10\n",
    "W = torch.randn(n, n)  # Random weights for the linear model\n",
    "theta0 = torch.ones(n,1)  # Random theta for the linear model\n",
    "\n",
    "kwargs = {\"W\": [W], \"theta0\": [theta0], \"noise_std\": [0.01, 0.1, 0.2, 0.3, 0.4]}\n",
    "\n",
    "initializer = Param_Initializer(QuadraticOptimizee, kwargs)\n",
    "\n",
    "lstm_optimizer = LSTMConcurrent_Pre(num_optims=initializer.get_num_optims(), preproc=False)\n",
    "meta_optimizer = optim.Adam(lstm_optimizer.parameters(), lr=0.01)\n",
    "\n",
    "lstm_optimizer, lambdas_ = train_LSTM_Pre(lstm_optimizer, meta_optimizer, initializer, num_epochs=10000, time_horizon=50, discount=0.9)\n",
    "params = test_LSTM_Pre(lstm_optimizer, initializer, time_horizon=50)\n",
    "\n",
    "v = nn.utils.parameters_to_vector(lstm_optimizer.input_layer.parameters())\n",
    "print(nn.functional.softmax(v, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"lambdas_.npy\", lambdas_.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
